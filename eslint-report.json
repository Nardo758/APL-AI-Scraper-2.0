[{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\browser-extension\\background.js","messages":[{"ruleId":"no-unused-vars","severity":1,"message":"'backgroundService' is assigned a value but never used.","line":239,"column":7,"nodeType":"Identifier","messageId":"unusedVar","endLine":239,"endColumn":24}],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":0,"source":"∩╗┐// APL AI Scraper 2.0 - Browser Extension Background Script\nclass BackgroundService {\n  constructor() {\n    this.init();\n  }\n\n  init() {\n    this.setupInstallListener();\n    this.setupTabListeners();\n    this.setupMessageListener();\n    this.setupContextMenu();\n    console.log('├░┼╕ΓÇ¥┬º Background service initialized');\n  }\n\n  setupInstallListener() {\n    chrome.runtime.onInstalled.addListener((details) => {\n      console.log('├░┼╕┼╜ΓÇ░ APL AI Scraper Recorder installed');\n      \n      // Set default settings\n      chrome.storage.sync.set({\n        serverUrl: 'http://localhost:3000',\n        projectId: '',\n        autoScreenshot: false,\n        recordingEnabled: true\n      });\n\n      // Show welcome notification\n      if (details.reason === 'install') {\n        this.showWelcomeNotification();\n      }\n    });\n  }\n\n  setupTabListeners() {\n    // Listen for tab updates to inject content script if needed\n    chrome.tabs.onUpdated.addListener((tabId, changeInfo, tab) => {\n      if (changeInfo.status === 'complete' && tab.url && \n          (tab.url.startsWith('http://') || tab.url.startsWith('https://'))) {\n        \n        // Inject content script into new pages\n        this.injectContentScript(tabId);\n      }\n    });\n  }\n\n  setupMessageListener() {\n    chrome.runtime.onMessage.addListener((request, sender, sendResponse) => {\n      switch (request.action) {\n      case 'saveRecording':\n        this.saveRecording(request.data)\n          .then(() => sendResponse({ success: true }))\n          .catch(error => sendResponse({ success: false, error: error.message }));\n        return true; // Indicates async response\n\n      case 'getSettings':\n        chrome.storage.sync.get(['serverUrl', 'projectId'], (result) => {\n          sendResponse(result);\n        });\n        return true;\n\n      case 'updateSettings':\n        chrome.storage.sync.set(request.settings, () => {\n          sendResponse({ success: true });\n        });\n        return true;\n\n      case 'showNotification':\n        this.showNotification(request.title, request.message, request.type);\n        sendResponse({ success: true });\n        break;\n\n      default:\n        console.log('Unknown message action:', request.action);\n      }\n    });\n  }\n\n  setupContextMenu() {\n    // Add context menu items for easy access\n    chrome.contextMenus.create({\n      id: 'apl-start-recording',\n      title: 'Start APL Recording',\n      contexts: ['page']\n    });\n\n    chrome.contextMenus.create({\n      id: 'apl-take-screenshot',\n      title: 'Take Screenshot',\n      contexts: ['page']\n    });\n\n    chrome.contextMenus.onClicked.addListener((info, tab) => {\n      switch (info.menuItemId) {\n      case 'apl-start-recording':\n        this.sendMessageToTab(tab.id, { action: 'startRecording' });\n        break;\n      case 'apl-take-screenshot':\n        this.sendMessageToTab(tab.id, { action: 'takeScreenshot' });\n        break;\n      }\n    });\n  }\n\n  async injectContentScript(tabId) {\n    try {\n      // Check if content script is already injected\n      const response = await chrome.tabs.sendMessage(tabId, { action: 'ping' });\n      if (response && response.pong) {\n        return; // Already injected\n      }\n    } catch (error) {\n      // Content script not injected, proceed with injection\n    }\n\n    try {\n      await chrome.scripting.executeScript({\n        target: { tabId: tabId },\n        files: ['libs/html2canvas.min.js', 'content-script.js']\n      });\n\n      await chrome.scripting.insertCSS({\n        target: { tabId: tabId },\n        files: ['recorder-ui.css']\n      });\n\n      console.log(`├ó┼ôΓÇª Content script injected into tab ${tabId}`);\n    } catch (error) {\n      console.error(`├ó┬¥┼Æ Failed to inject content script into tab ${tabId}:`, error);\n    }\n  }\n\n  async sendMessageToTab(tabId, message) {\n    try {\n      await chrome.tabs.sendMessage(tabId, message);\n    } catch (error) {\n      console.error('Failed to send message to tab:', error);\n      // Try injecting content script first\n      await this.injectContentScript(tabId);\n      \n      // Wait a moment and try again\n      setTimeout(async () => {\n        try {\n          await chrome.tabs.sendMessage(tabId, message);\n        } catch (retryError) {\n          console.error('Failed to send message after injection:', retryError);\n        }\n      }, 1000);\n    }\n  }\n\n  async saveRecording(recordingData) {\n    try {\n      const settings = await chrome.storage.sync.get(['serverUrl']);\n      const serverUrl = settings.serverUrl || 'http://localhost:3000';\n\n      const response = await fetch(`${serverUrl}/api/training-sessions`, {\n        method: 'POST',\n        headers: {\n          'Content-Type': 'application/json'\n        },\n        body: JSON.stringify(recordingData)\n      });\n\n      if (!response.ok) {\n        throw new Error(`HTTP ${response.status}: ${response.statusText}`);\n      }\n\n      const result = await response.json();\n      \n      // Store session info locally\n      await chrome.storage.local.set({\n        lastSessionId: result.id,\n        lastSessionTime: Date.now()\n      });\n\n      this.showNotification(\n        'Recording Saved',\n        'Your browsing session has been saved successfully!',\n        'success'\n      );\n\n      return result;\n    } catch (error) {\n      console.error('Failed to save recording:', error);\n      \n      this.showNotification(\n        'Save Failed',\n        `Failed to save recording: ${error.message}`,\n        'error'\n      );\n      \n      throw error;\n    }\n  }\n\n  showWelcomeNotification() {\n    this.showNotification(\n      'APL AI Scraper Installed',\n      'Click the extension icon to start recording your browsing sessions!',\n      'info'\n    );\n  }\n\n  showNotification(title, message, type = 'info') {\n    // Create notification ID\n    const notificationId = `apl-${Date.now()}`;\n    \n    const iconUrl = this.getIconForType(type);\n\n    chrome.notifications.create(notificationId, {\n      type: 'basic',\n      iconUrl: iconUrl,\n      title: title,\n      message: message,\n      priority: 1\n    });\n\n    // Auto-clear notification after 5 seconds\n    setTimeout(() => {\n      chrome.notifications.clear(notificationId);\n    }, 5000);\n  }\n\n  getIconForType(type) {\n    switch (type) {\n    case 'success':\n      return 'icons/icon-success-48.png';\n    case 'error':\n      return 'icons/icon-error-48.png';\n    case 'warning':\n      return 'icons/icon-warning-48.png';\n    default:\n      return 'icons/icon-48.png';\n    }\n  }\n}\n\n// Initialize background service\nconst backgroundService = new BackgroundService();","usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\browser-extension\\content-script.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\browser-extension\\popup.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\core\\logger.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\core\\supabase.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\frontend\\src\\App.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"dot-location","replacedBy":[]},{"ruleId":"new-parens","replacedBy":[]},{"ruleId":"no-mixed-operators","replacedBy":[]},{"ruleId":"no-new-object","replacedBy":["no-object-constructor"]},{"ruleId":"no-whitespace-before-property","replacedBy":[]},{"ruleId":"rest-spread-spacing","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\frontend\\src\\components\\Dashboard.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"dot-location","replacedBy":[]},{"ruleId":"new-parens","replacedBy":[]},{"ruleId":"no-mixed-operators","replacedBy":[]},{"ruleId":"no-new-object","replacedBy":["no-object-constructor"]},{"ruleId":"no-whitespace-before-property","replacedBy":[]},{"ruleId":"rest-spread-spacing","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\frontend\\src\\components\\Dashboard\\AlertPanel.js","messages":[{"ruleId":"no-unused-vars","severity":1,"message":"'Filter' is defined but never used.","line":10,"column":3,"nodeType":"Identifier","messageId":"unusedVar","endLine":10,"endColumn":9}],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":0,"source":"import React, { useEffect, useState } from 'react';\nimport { \n  AlertTriangle, \n  AlertCircle,\n  XCircle,\n  CheckCircle,\n  X,\n  Bell,\n  BellOff,\n  Filter,\n  RefreshCw\n} from 'lucide-react';\n\nconst AlertPanel = ({ supabase, notifications }) => {\n  const [alerts, setAlerts] = useState([]);\n  const [filter, setFilter] = useState('all');\n  const [isLoading, setIsLoading] = useState(true);\n  const [showDismissed, setShowDismissed] = useState(false);\n\n  useEffect(() => {\n    if (!supabase) return;\n\n    const fetchAlerts = async () => {\n      try {\n        // Fetch system alerts from the database\n        const { data: systemAlerts } = await supabase\n          .from('system_alerts')\n          .select('*')\n          .order('created_at', { ascending: false });\n\n        // Fetch recent errors and issues\n        const { data: recentErrors } = await supabase\n          .from('scraping_executions')\n          .select('id, error_message, created_at, template_name')\n          .not('error_message', 'is', null)\n          .gte('created_at', new Date(Date.now() - 24 * 60 * 60 * 1000).toISOString())\n          .order('created_at', { ascending: false })\n          .limit(10);\n\n        // Check proxy health for alerts\n        const { data: proxies } = await supabase\n          .from('proxy_list')\n          .select('id, url, status, success_rate, last_used');\n\n        const failedProxies = proxies?.filter(p => \n          p.status === 'failed' || p.success_rate < 0.5\n        ) || [];\n\n        // Generate alerts from data\n        const generatedAlerts = [];\n\n        // Add system alerts\n        systemAlerts?.forEach(alert => {\n          generatedAlerts.push({\n            id: `system-${alert.id}`,\n            type: 'system',\n            severity: alert.severity || 'medium',\n            title: alert.title,\n            message: alert.message,\n            timestamp: alert.created_at,\n            dismissed: alert.dismissed || false,\n            source: 'System'\n          });\n        });\n\n        // Add error alerts\n        recentErrors?.forEach(error => {\n          generatedAlerts.push({\n            id: `error-${error.id}`,\n            type: 'error',\n            severity: 'high',\n            title: 'Scraping Job Failed',\n            message: `Template \"${error.template_name}\": ${error.error_message}`,\n            timestamp: error.created_at,\n            dismissed: false,\n            source: 'Scraper'\n          });\n        });\n\n        // Add proxy alerts\n        if (failedProxies.length > 0) {\n          generatedAlerts.push({\n            id: 'proxy-health',\n            type: 'warning',\n            severity: 'medium',\n            title: 'Proxy Health Issues',\n            message: `${failedProxies.length} proxies are failing or have low success rates`,\n            timestamp: new Date().toISOString(),\n            dismissed: false,\n            source: 'Proxy Manager',\n            details: failedProxies.map(p => `${p.url}: ${(p.success_rate * 100).toFixed(1)}%`).join(', ')\n          });\n        }\n\n        // Check for high queue size\n        const { data: queueStats } = await supabase\n          .from('job_queue_stats')\n          .select('pending_jobs, failed_jobs')\n          .single();\n\n        if (queueStats?.pending_jobs > 100) {\n          generatedAlerts.push({\n            id: 'queue-backlog',\n            type: 'warning',\n            severity: 'medium',\n            title: 'Job Queue Backlog',\n            message: `${queueStats.pending_jobs} jobs are pending execution`,\n            timestamp: new Date().toISOString(),\n            dismissed: false,\n            source: 'Job Queue'\n          });\n        }\n\n        if (queueStats?.failed_jobs > 10) {\n          generatedAlerts.push({\n            id: 'queue-failures',\n            type: 'error',\n            severity: 'high',\n            title: 'Multiple Job Failures',\n            message: `${queueStats.failed_jobs} jobs have failed recently`,\n            timestamp: new Date().toISOString(),\n            dismissed: false,\n            source: 'Job Queue'\n          });\n        }\n\n        setAlerts(generatedAlerts);\n\n      } catch (error) {\n        console.error('Error fetching alerts:', error);\n      } finally {\n        setIsLoading(false);\n      }\n    };\n\n    fetchAlerts();\n    const interval = setInterval(fetchAlerts, 60000); // Refresh every minute\n\n    return () => clearInterval(interval);\n  }, [supabase]);\n\n  const getSeverityIcon = (severity) => {\n    switch (severity) {\n      case 'critical':\n        return <XCircle className=\"severity-icon critical\" size={16} />;\n      case 'high':\n        return <AlertTriangle className=\"severity-icon high\" size={16} />;\n      case 'medium':\n        return <AlertCircle className=\"severity-icon medium\" size={16} />;\n      case 'low':\n        return <CheckCircle className=\"severity-icon low\" size={16} />;\n      default:\n        return <AlertCircle className=\"severity-icon medium\" size={16} />;\n    }\n  };\n\n  const getSeverityColor = (severity) => {\n    switch (severity) {\n      case 'critical': return 'critical';\n      case 'high': return 'high';\n      case 'medium': return 'medium';\n      case 'low': return 'low';\n      default: return 'medium';\n    }\n  };\n\n  const getTypeIcon = (type) => {\n    switch (type) {\n      case 'system':\n        return <AlertTriangle size={14} />;\n      case 'error':\n        return <XCircle size={14} />;\n      case 'warning':\n        return <AlertCircle size={14} />;\n      default:\n        return <Bell size={14} />;\n    }\n  };\n\n  const dismissAlert = async (alertId) => {\n    try {\n      // Update local state immediately\n      setAlerts(prev => prev.map(alert => \n        alert.id === alertId ? { ...alert, dismissed: true } : alert\n      ));\n\n      // Update in database if it's a system alert\n      if (alertId.startsWith('system-')) {\n        const systemAlertId = alertId.replace('system-', '');\n        await supabase\n          .from('system_alerts')\n          .update({ dismissed: true })\n          .eq('id', systemAlertId);\n      }\n    } catch (error) {\n      console.error('Error dismissing alert:', error);\n    }\n  };\n\n  const formatTimeAgo = (timestamp) => {\n    const now = new Date();\n    const date = new Date(timestamp);\n    const diffMs = now - date;\n    const diffMinutes = Math.floor(diffMs / (1000 * 60));\n    const diffHours = Math.floor(diffMinutes / 60);\n\n    if (diffHours > 0) return `${diffHours}h ago`;\n    if (diffMinutes > 0) return `${diffMinutes}m ago`;\n    return 'Just now';\n  };\n\n  const filteredAlerts = alerts.filter(alert => {\n    if (!showDismissed && alert.dismissed) return false;\n    if (filter === 'all') return true;\n    if (filter === 'active') return !alert.dismissed;\n    return alert.severity === filter;\n  });\n\n  const activeCriticalCount = alerts.filter(a => !a.dismissed && a.severity === 'critical').length;\n  const activeHighCount = alerts.filter(a => !a.dismissed && a.severity === 'high').length;\n  const activeMediumCount = alerts.filter(a => !a.dismissed && a.severity === 'medium').length;\n\n  if (isLoading) {\n    return (\n      <div className=\"alert-panel loading\">\n        <div className=\"section-header\">\n          <h2>System Alerts</h2>\n        </div>\n        <div className=\"loading-state\">\n          <Bell className=\"spinner\" size={24} />\n          <span>Loading alerts...</span>\n        </div>\n      </div>\n    );\n  }\n\n  return (\n    <div className=\"alert-panel\">\n      <div className=\"section-header\">\n        <h2>System Alerts</h2>\n        <div className=\"alert-controls\">\n          <div className=\"alert-summary\">\n            {activeCriticalCount > 0 && (\n              <span className=\"alert-count critical\">{activeCriticalCount} Critical</span>\n            )}\n            {activeHighCount > 0 && (\n              <span className=\"alert-count high\">{activeHighCount} High</span>\n            )}\n            {activeMediumCount > 0 && (\n              <span className=\"alert-count medium\">{activeMediumCount} Medium</span>\n            )}\n            {activeCriticalCount + activeHighCount + activeMediumCount === 0 && (\n              <span className=\"alert-count none\">No active alerts</span>\n            )}\n          </div>\n          <button \n            className=\"refresh-alerts\"\n            onClick={() => window.location.reload()}\n          >\n            <RefreshCw size={14} />\n          </button>\n        </div>\n      </div>\n\n      <div className=\"alert-filters\">\n        <button \n          className={filter === 'all' ? 'active' : ''}\n          onClick={() => setFilter('all')}\n        >\n          All ({alerts.length})\n        </button>\n        <button \n          className={filter === 'active' ? 'active' : ''}\n          onClick={() => setFilter('active')}\n        >\n          Active ({alerts.filter(a => !a.dismissed).length})\n        </button>\n        <button \n          className={filter === 'critical' ? 'active' : ''}\n          onClick={() => setFilter('critical')}\n        >\n          Critical ({activeCriticalCount})\n        </button>\n        <button \n          className={filter === 'high' ? 'active' : ''}\n          onClick={() => setFilter('high')}\n        >\n          High ({activeHighCount})\n        </button>\n        <button\n          className={`show-dismissed ${showDismissed ? 'active' : ''}`}\n          onClick={() => setShowDismissed(!showDismissed)}\n        >\n          {showDismissed ? <BellOff size={14} /> : <Bell size={14} />}\n          {showDismissed ? 'Hide' : 'Show'} Dismissed\n        </button>\n      </div>\n\n      <div className=\"alerts-list\">\n        {filteredAlerts.length === 0 ? (\n          <div className=\"empty-state\">\n            <CheckCircle size={48} />\n            <h3>No alerts to show</h3>\n            <p>\n              {filter === 'all' \n                ? 'All systems are running normally'\n                : `No ${filter} alerts found`\n              }\n            </p>\n          </div>\n        ) : (\n          filteredAlerts.map((alert) => (\n            <div \n              key={alert.id} \n              className={`alert-item severity-${getSeverityColor(alert.severity)} ${alert.dismissed ? 'dismissed' : ''}`}\n            >\n              <div className=\"alert-content\">\n                <div className=\"alert-header\">\n                  <div className=\"alert-severity\">\n                    {getSeverityIcon(alert.severity)}\n                  </div>\n                  <div className=\"alert-info\">\n                    <div className=\"alert-title\">{alert.title}</div>\n                    <div className=\"alert-meta\">\n                      <span className=\"alert-source\">{alert.source}</span>\n                      <span className=\"alert-time\">{formatTimeAgo(alert.timestamp)}</span>\n                    </div>\n                  </div>\n                  <div className=\"alert-actions\">\n                    {!alert.dismissed && (\n                      <button \n                        className=\"dismiss-btn\"\n                        onClick={() => dismissAlert(alert.id)}\n                        title=\"Dismiss alert\"\n                      >\n                        <X size={14} />\n                      </button>\n                    )}\n                  </div>\n                </div>\n                \n                <div className=\"alert-message\">\n                  {alert.message}\n                </div>\n\n                {alert.details && (\n                  <div className=\"alert-details\">\n                    <strong>Details:</strong> {alert.details}\n                  </div>\n                )}\n\n                <div className=\"alert-footer\">\n                  <div className=\"alert-type\">\n                    {getTypeIcon(alert.type)}\n                    <span>{alert.type}</span>\n                  </div>\n                  {alert.dismissed && (\n                    <span className=\"dismissed-indicator\">Dismissed</span>\n                  )}\n                </div>\n              </div>\n            </div>\n          ))\n        )}\n      </div>\n    </div>\n  );\n};\n\nexport default AlertPanel;","usedDeprecatedRules":[{"ruleId":"dot-location","replacedBy":[]},{"ruleId":"new-parens","replacedBy":[]},{"ruleId":"no-mixed-operators","replacedBy":[]},{"ruleId":"no-new-object","replacedBy":["no-object-constructor"]},{"ruleId":"no-whitespace-before-property","replacedBy":[]},{"ruleId":"rest-spread-spacing","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\frontend\\src\\components\\Dashboard\\Dashboard.js","messages":[{"ruleId":"react-hooks/exhaustive-deps","severity":1,"message":"React Hook useEffect has missing dependencies: 'loadDashboardData' and 'setupRealtimeSubscriptions'. Either include them or remove the dependency array.","line":25,"column":6,"nodeType":"ArrayExpression","endLine":25,"endColumn":8,"suggestions":[{"desc":"Update the dependencies array to be: [loadDashboardData, setupRealtimeSubscriptions]","fix":{"range":[823,825],"text":"[loadDashboardData, setupRealtimeSubscriptions]"}}]}],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":0,"source":"import React, { useState, useEffect } from 'react';\nimport { useSupabase } from '../../contexts/SupabaseContext';\nimport { useNotifications } from '../../contexts/NotificationContext';\nimport MetricsOverview from './MetricsOverview';\nimport RealTimeJobs from './RealTimeJobs';\nimport SystemHealth from './SystemHealth';\nimport RecentActivity from './RecentActivity';\nimport AlertPanel from './AlertPanel';\nimport './Dashboard.css';\n\nconst Dashboard = () => {\n  const { supabase } = useSupabase();\n  const { addNotification } = useNotifications();\n  const [dashboardData, setDashboardData] = useState({\n    metrics: {},\n    activeJobs: [],\n    systemHealth: {},\n    recentActivity: []\n  });\n  const [loading, setLoading] = useState(true);\n\n  useEffect(() => {\n    loadDashboardData();\n    setupRealtimeSubscriptions();\n  }, []);\n\n  const loadDashboardData = async () => {\n    try {\n      // Load dashboard metrics from API\n      const response = await fetch('/api/dashboard/metrics');\n      const metrics = await response.json();\n\n      // Load active jobs\n      const { data: jobs } = await supabase\n        .from('scraping_executions')\n        .select(`\n          *,\n          scraper_templates(name, version)\n        `)\n        .eq('status', 'running')\n        .order('created_at', { ascending: false })\n        .limit(10);\n\n      // Load system health\n      const healthResponse = await fetch('/api/system/status');\n      const systemHealth = await healthResponse.json();\n\n      // Load recent activity\n      const { data: activity } = await supabase\n        .from('scraping_executions')\n        .select(`\n          *,\n          scraper_templates(name)\n        `)\n        .order('created_at', { ascending: false })\n        .limit(20);\n\n      setDashboardData({\n        metrics: metrics || {},\n        activeJobs: jobs || [],\n        systemHealth: systemHealth || {},\n        recentActivity: activity || []\n      });\n    } catch (error) {\n      console.error('Dashboard data load error:', error);\n      addNotification('error', 'Failed to load dashboard data');\n    } finally {\n      setLoading(false);\n    }\n  };\n\n  const setupRealtimeSubscriptions = () => {\n    // Real-time job updates\n    const jobSubscription = supabase\n      .channel('jobs-channel')\n      .on('postgres_changes', \n        { event: '*', schema: 'public', table: 'scraping_executions' },\n        (payload) => {\n          handleJobUpdate(payload);\n        }\n      )\n      .subscribe();\n\n    // System health updates\n    const healthSubscription = supabase\n      .channel('health-channel')\n      .on('postgres_changes',\n        { event: 'UPDATE', schema: 'public', table: 'system_health' },\n        (payload) => {\n          setDashboardData(prev => ({\n            ...prev,\n            systemHealth: payload.new\n          }));\n        }\n      )\n      .subscribe();\n\n    return () => {\n      jobSubscription.unsubscribe();\n      healthSubscription.unsubscribe();\n    };\n  };\n\n  const handleJobUpdate = (payload) => {\n    if (payload.eventType === 'INSERT' && payload.new.status === 'running') {\n      // New job started\n      setDashboardData(prev => ({\n        ...prev,\n        activeJobs: [payload.new, ...prev.activeJobs.slice(0, 9)],\n        recentActivity: [payload.new, ...prev.recentActivity.slice(0, 19)]\n      }));\n    } else if (payload.eventType === 'UPDATE') {\n      // Job status changed\n      setDashboardData(prev => ({\n        ...prev,\n        activeJobs: prev.activeJobs.map(job => \n          job.id === payload.new.id ? payload.new : job\n        ).filter(job => job.status === 'running'),\n        recentActivity: [payload.new, ...prev.recentActivity.slice(0, 19)]\n      }));\n\n      // Show notification for completed/failed jobs\n      if (payload.new.status === 'completed') {\n        addNotification('success', `Job completed: ${payload.new.url}`);\n      } else if (payload.new.status === 'failed') {\n        addNotification('error', `Job failed: ${payload.new.url}`);\n      }\n    }\n  };\n\n  const handleNewScrapingJob = () => {\n    // Navigate to scrapers page or show job creation modal\n    window.location.href = '/scrapers';\n  };\n\n  const handleHealthCheck = async () => {\n    try {\n      const response = await fetch('/api/system/status');\n      const health = await response.json();\n      \n      if (health.services) {\n        const failedServices = Object.entries(health.services)\n          .filter(([name, status]) => !status)\n          .map(([name]) => name);\n          \n        if (failedServices.length > 0) {\n          addNotification('warning', `Services need attention: ${failedServices.join(', ')}`);\n        } else {\n          addNotification('success', 'All systems operational');\n        }\n      }\n    } catch (error) {\n      addNotification('error', 'Health check failed');\n    }\n  };\n\n  if (loading) {\n    return (\n      <div className=\"dashboard-loading\">\n        <div className=\"loading-spinner\"></div>\n        <p>Loading dashboard...</p>\n      </div>\n    );\n  }\n\n  return (\n    <div className=\"dashboard\">\n      <div className=\"dashboard-header\">\n        <div className=\"header-content\">\n          <h1>Command Center</h1>\n          <p>Monitor and manage your AI scraping operations</p>\n        </div>\n        <div className=\"dashboard-actions\">\n          <button className=\"btn btn-primary\" onClick={handleNewScrapingJob}>\n            New Scraping Job\n          </button>\n          <button className=\"btn btn-secondary\" onClick={handleHealthCheck}>\n            Run Health Check\n          </button>\n        </div>\n      </div>\n\n      <AlertPanel />\n\n      <div className=\"dashboard-grid\">\n        <div className=\"grid-column main-column\">\n          <MetricsOverview data={dashboardData.metrics} />\n          <RealTimeJobs jobs={dashboardData.activeJobs} />\n        </div>\n        \n        <div className=\"grid-column side-column\">\n          <SystemHealth data={dashboardData.systemHealth} />\n          <RecentActivity activities={dashboardData.recentActivity} />\n        </div>\n      </div>\n    </div>\n  );\n};\n\nexport default Dashboard;","usedDeprecatedRules":[{"ruleId":"dot-location","replacedBy":[]},{"ruleId":"new-parens","replacedBy":[]},{"ruleId":"no-mixed-operators","replacedBy":[]},{"ruleId":"no-new-object","replacedBy":["no-object-constructor"]},{"ruleId":"no-whitespace-before-property","replacedBy":[]},{"ruleId":"rest-spread-spacing","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\frontend\\src\\components\\Dashboard\\MetricsOverview.js","messages":[{"ruleId":"no-unused-vars","severity":1,"message":"'XCircle' is defined but never used.","line":7,"column":3,"nodeType":"Identifier","messageId":"unusedVar","endLine":7,"endColumn":10}],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":0,"source":"import React from 'react';\nimport { \n  TrendingUp, \n  TrendingDown, \n  Clock, \n  CheckCircle, \n  XCircle,\n  Database,\n  Zap \n} from 'lucide-react';\n\nconst MetricsOverview = ({ data }) => {\n  const metrics = [\n    {\n      title: 'Total Jobs',\n      value: data.total_jobs || 0,\n      change: data.jobs_change || 0,\n      icon: <Database className=\"metric-icon\" />,\n      color: 'blue',\n      format: 'number'\n    },\n    {\n      title: 'Success Rate',\n      value: (data.success_rate || 0) * 100,\n      change: (data.success_rate_change || 0) * 100,\n      icon: <CheckCircle className=\"metric-icon\" />,\n      color: 'green',\n      format: 'percentage'\n    },\n    {\n      title: 'Avg Response Time',\n      value: data.avg_response_time || 0,\n      change: data.response_time_change || 0,\n      icon: <Clock className=\"metric-icon\" />,\n      color: 'orange',\n      format: 'time'\n    },\n    {\n      title: 'Jobs/Hour',\n      value: data.jobs_per_hour || 0,\n      change: data.jobs_per_hour_change || 0,\n      icon: <Zap className=\"metric-icon\" />,\n      color: 'purple',\n      format: 'number'\n    }\n  ];\n\n  const formatValue = (value, format) => {\n    switch (format) {\n      case 'percentage':\n        return `${value.toFixed(1)}%`;\n      case 'time':\n        return `${value.toFixed(2)}s`;\n      case 'number':\n        return value.toLocaleString();\n      default:\n        return value.toString();\n    }\n  };\n\n  const getChangeIcon = (change) => {\n    if (change > 0) {\n      return <TrendingUp className=\"change-icon positive\" size={14} />;\n    } else if (change < 0) {\n      return <TrendingDown className=\"change-icon negative\" size={14} />;\n    }\n    return null;\n  };\n\n  return (\n    <div className=\"metrics-overview\">\n      <div className=\"section-header\">\n        <h2>Performance Metrics</h2>\n        <span className=\"update-time\">\n          Last updated: {new Date().toLocaleTimeString()}\n        </span>\n      </div>\n      \n      <div className=\"metrics-grid\">\n        {metrics.map((metric, index) => (\n          <div key={index} className={`metric-card metric-${metric.color}`}>\n            <div className=\"metric-header\">\n              <div className=\"metric-icon-wrapper\">\n                {metric.icon}\n              </div>\n              <div className=\"metric-values\">\n                <div className=\"metric-value\">\n                  {formatValue(metric.value, metric.format)}\n                </div>\n                {metric.change !== 0 && (\n                  <div className=\"metric-change\">\n                    {getChangeIcon(metric.change)}\n                    <span className={metric.change > 0 ? 'positive' : 'negative'}>\n                      {Math.abs(metric.change).toFixed(1)}\n                      {metric.format === 'percentage' ? 'pp' : '%'}\n                    </span>\n                  </div>\n                )}\n              </div>\n            </div>\n            <div className=\"metric-title\">{metric.title}</div>\n            <div className=\"metric-sparkline\">\n              {/* Placeholder for mini chart - could be enhanced with actual data */}\n              <div className=\"sparkline-placeholder\"></div>\n            </div>\n          </div>\n        ))}\n      </div>\n\n      <div className=\"metrics-summary\">\n        <div className=\"summary-item\">\n          <span className=\"summary-label\">Active Templates:</span>\n          <span className=\"summary-value\">{data.active_templates || 0}</span>\n        </div>\n        <div className=\"summary-item\">\n          <span className=\"summary-label\">Total Proxies:</span>\n          <span className=\"summary-value\">{data.total_proxies || 0}</span>\n        </div>\n        <div className=\"summary-item\">\n          <span className=\"summary-label\">Queue Size:</span>\n          <span className=\"summary-value\">{data.queue_size || 0}</span>\n        </div>\n      </div>\n    </div>\n  );\n};\n\nexport default MetricsOverview;","usedDeprecatedRules":[{"ruleId":"dot-location","replacedBy":[]},{"ruleId":"new-parens","replacedBy":[]},{"ruleId":"no-mixed-operators","replacedBy":[]},{"ruleId":"no-new-object","replacedBy":["no-object-constructor"]},{"ruleId":"no-whitespace-before-property","replacedBy":[]},{"ruleId":"rest-spread-spacing","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\frontend\\src\\components\\Dashboard\\RealTimeJobs.js","messages":[{"ruleId":"no-unused-vars","severity":1,"message":"'Play' is defined but never used.","line":3,"column":3,"nodeType":"Identifier","messageId":"unusedVar","endLine":3,"endColumn":7}],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":0,"source":"import React, { useEffect, useState } from 'react';\nimport { \n  Play, \n  Pause, \n  Clock, \n  AlertCircle,\n  CheckCircle,\n  XCircle,\n  Loader,\n  Monitor,\n  Globe\n} from 'lucide-react';\n\nconst RealTimeJobs = ({ supabase }) => {\n  const [jobs, setJobs] = useState([]);\n  const [isLoading, setIsLoading] = useState(true);\n  const [filter, setFilter] = useState('all');\n\n  useEffect(() => {\n    if (!supabase) return;\n\n    const fetchJobs = async () => {\n      try {\n        const { data, error } = await supabase\n          .from('scraping_executions')\n          .select(`\n            id,\n            status,\n            created_at,\n            completed_at,\n            error_message,\n            records_scraped,\n            template_name,\n            proxy_used,\n            execution_time_ms\n          `)\n          .order('created_at', { ascending: false })\n          .limit(20);\n\n        if (error) throw error;\n        setJobs(data || []);\n      } catch (error) {\n        console.error('Error fetching jobs:', error);\n      } finally {\n        setIsLoading(false);\n      }\n    };\n\n    fetchJobs();\n\n    // Set up real-time subscription\n    const subscription = supabase\n      .channel('scraping_executions_changes')\n      .on(\n        'postgres_changes',\n        {\n          event: '*',\n          schema: 'public',\n          table: 'scraping_executions'\n        },\n        (payload) => {\n          console.log('Job update:', payload);\n          fetchJobs(); // Refresh jobs list\n        }\n      )\n      .subscribe();\n\n    return () => {\n      supabase.removeChannel(subscription);\n    };\n  }, [supabase]);\n\n  const getStatusIcon = (status) => {\n    switch (status) {\n      case 'running':\n        return <Loader className=\"status-icon running\" size={16} />;\n      case 'completed':\n        return <CheckCircle className=\"status-icon completed\" size={16} />;\n      case 'failed':\n        return <XCircle className=\"status-icon failed\" size={16} />;\n      case 'queued':\n        return <Clock className=\"status-icon queued\" size={16} />;\n      case 'paused':\n        return <Pause className=\"status-icon paused\" size={16} />;\n      default:\n        return <AlertCircle className=\"status-icon unknown\" size={16} />;\n    }\n  };\n\n  const getStatusColor = (status) => {\n    switch (status) {\n      case 'running': return 'blue';\n      case 'completed': return 'green';\n      case 'failed': return 'red';\n      case 'queued': return 'yellow';\n      case 'paused': return 'gray';\n      default: return 'gray';\n    }\n  };\n\n  const formatDuration = (ms) => {\n    if (!ms) return '-';\n    const seconds = Math.floor(ms / 1000);\n    const minutes = Math.floor(seconds / 60);\n    const hours = Math.floor(minutes / 60);\n    \n    if (hours > 0) return `${hours}h ${minutes % 60}m`;\n    if (minutes > 0) return `${minutes}m ${seconds % 60}s`;\n    return `${seconds}s`;\n  };\n\n  const formatTimeAgo = (dateString) => {\n    const now = new Date();\n    const date = new Date(dateString);\n    const diffMs = now - date;\n    const diffMinutes = Math.floor(diffMs / (1000 * 60));\n    const diffHours = Math.floor(diffMinutes / 60);\n    const diffDays = Math.floor(diffHours / 24);\n\n    if (diffDays > 0) return `${diffDays}d ago`;\n    if (diffHours > 0) return `${diffHours}h ago`;\n    if (diffMinutes > 0) return `${diffMinutes}m ago`;\n    return 'Just now';\n  };\n\n  const filteredJobs = jobs.filter(job => {\n    if (filter === 'all') return true;\n    return job.status === filter;\n  });\n\n  const statusCounts = jobs.reduce((acc, job) => {\n    acc[job.status] = (acc[job.status] || 0) + 1;\n    return acc;\n  }, {});\n\n  if (isLoading) {\n    return (\n      <div className=\"real-time-jobs loading\">\n        <div className=\"section-header\">\n          <h2>Real-Time Jobs</h2>\n        </div>\n        <div className=\"loading-state\">\n          <Loader className=\"spinner\" size={24} />\n          <span>Loading jobs...</span>\n        </div>\n      </div>\n    );\n  }\n\n  return (\n    <div className=\"real-time-jobs\">\n      <div className=\"section-header\">\n        <h2>Real-Time Jobs</h2>\n        <div className=\"job-filters\">\n          <button \n            className={filter === 'all' ? 'active' : ''}\n            onClick={() => setFilter('all')}\n          >\n            All ({jobs.length})\n          </button>\n          <button \n            className={filter === 'running' ? 'active' : ''}\n            onClick={() => setFilter('running')}\n          >\n            Running ({statusCounts.running || 0})\n          </button>\n          <button \n            className={filter === 'completed' ? 'active' : ''}\n            onClick={() => setFilter('completed')}\n          >\n            Completed ({statusCounts.completed || 0})\n          </button>\n          <button \n            className={filter === 'failed' ? 'active' : ''}\n            onClick={() => setFilter('failed')}\n          >\n            Failed ({statusCounts.failed || 0})\n          </button>\n        </div>\n      </div>\n\n      <div className=\"jobs-list\">\n        {filteredJobs.length === 0 ? (\n          <div className=\"empty-state\">\n            <Monitor size={48} />\n            <h3>No jobs found</h3>\n            <p>No scraping jobs match the current filter</p>\n          </div>\n        ) : (\n          filteredJobs.map((job) => (\n            <div key={job.id} className={`job-item status-${job.status}`}>\n              <div className=\"job-header\">\n                <div className=\"job-status\">\n                  {getStatusIcon(job.status)}\n                  <span className={`status-text ${getStatusColor(job.status)}`}>\n                    {job.status.charAt(0).toUpperCase() + job.status.slice(1)}\n                  </span>\n                </div>\n                <div className=\"job-time\">\n                  {formatTimeAgo(job.created_at)}\n                </div>\n              </div>\n\n              <div className=\"job-details\">\n                <div className=\"job-template\">\n                  <strong>{job.template_name || 'Unknown Template'}</strong>\n                </div>\n                \n                <div className=\"job-metrics\">\n                  <div className=\"metric\">\n                    <span className=\"metric-label\">Records:</span>\n                    <span className=\"metric-value\">{job.records_scraped || 0}</span>\n                  </div>\n                  <div className=\"metric\">\n                    <span className=\"metric-label\">Duration:</span>\n                    <span className=\"metric-value\">\n                      {formatDuration(job.execution_time_ms)}\n                    </span>\n                  </div>\n                  {job.proxy_used && (\n                    <div className=\"metric\">\n                      <Globe size={14} />\n                      <span className=\"metric-value\">{job.proxy_used}</span>\n                    </div>\n                  )}\n                </div>\n\n                {job.error_message && (\n                  <div className=\"job-error\">\n                    <AlertCircle size={14} />\n                    <span className=\"error-text\">{job.error_message}</span>\n                  </div>\n                )}\n              </div>\n\n              <div className=\"job-progress\">\n                <div className={`progress-bar status-${job.status}`}>\n                  <div \n                    className=\"progress-fill\"\n                    style={{\n                      width: job.status === 'completed' ? '100%' : \n                             job.status === 'running' ? '60%' : '0%'\n                    }}\n                  ></div>\n                </div>\n              </div>\n            </div>\n          ))\n        )}\n      </div>\n\n      {filteredJobs.length > 0 && (\n        <div className=\"jobs-footer\">\n          <span className=\"jobs-count\">\n            Showing {filteredJobs.length} of {jobs.length} jobs\n          </span>\n          <button className=\"refresh-btn\">\n            <Loader size={14} />\n            Auto-refresh active\n          </button>\n        </div>\n      )}\n    </div>\n  );\n};\n\nexport default RealTimeJobs;","usedDeprecatedRules":[{"ruleId":"dot-location","replacedBy":[]},{"ruleId":"new-parens","replacedBy":[]},{"ruleId":"no-mixed-operators","replacedBy":[]},{"ruleId":"no-new-object","replacedBy":["no-object-constructor"]},{"ruleId":"no-whitespace-before-property","replacedBy":[]},{"ruleId":"rest-spread-spacing","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\frontend\\src\\components\\Dashboard\\RecentActivity.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"dot-location","replacedBy":[]},{"ruleId":"new-parens","replacedBy":[]},{"ruleId":"no-mixed-operators","replacedBy":[]},{"ruleId":"no-new-object","replacedBy":["no-object-constructor"]},{"ruleId":"no-whitespace-before-property","replacedBy":[]},{"ruleId":"rest-spread-spacing","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\frontend\\src\\components\\Dashboard\\SystemHealth.js","messages":[{"ruleId":"no-unused-vars","severity":1,"message":"'dbTest' is assigned a value but never used.","line":33,"column":23,"nodeType":"Identifier","messageId":"unusedVar","endLine":33,"endColumn":29}],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":0,"source":"import React, { useEffect, useState } from 'react';\nimport { \n  Server, \n  Database, \n  Wifi, \n  WifiOff,\n  AlertTriangle,\n  CheckCircle,\n  Clock,\n  Activity,\n  HardDrive,\n  Cpu,\n  MemoryStick\n} from 'lucide-react';\n\nconst SystemHealth = ({ supabase }) => {\n  const [healthData, setHealthData] = useState({\n    database: { status: 'checking', latency: null, connections: null },\n    redis: { status: 'checking', latency: null, memory_usage: null },\n    proxies: { active: 0, total: 0, success_rate: 0 },\n    system: { cpu_usage: 0, memory_usage: 0, disk_usage: 0, uptime: 0 }\n  });\n  \n  const [isLoading, setIsLoading] = useState(true);\n\n  useEffect(() => {\n    if (!supabase) return;\n\n    const checkSystemHealth = async () => {\n      try {\n        // Check database health\n        const dbStart = Date.now();\n        const { data: dbTest, error: dbError } = await supabase\n          .from('system_health')\n          .select('*')\n          .limit(1);\n        const dbLatency = Date.now() - dbStart;\n\n        // Get proxy status\n        const { data: proxyData } = await supabase\n          .from('proxy_list')\n          .select('status, last_used, success_rate');\n\n        const activeProxies = proxyData?.filter(p => p.status === 'active').length || 0;\n        const totalProxies = proxyData?.length || 0;\n        const avgSuccessRate = proxyData?.reduce((acc, p) => acc + (p.success_rate || 0), 0) / totalProxies || 0;\n\n        // Simulate system metrics (in real implementation, these would come from system monitoring)\n        const mockSystemMetrics = {\n          cpu_usage: Math.random() * 100,\n          memory_usage: Math.random() * 100,\n          disk_usage: Math.random() * 100,\n          uptime: Date.now() - (24 * 60 * 60 * 1000) // 24 hours ago\n        };\n\n        setHealthData({\n          database: {\n            status: dbError ? 'error' : 'healthy',\n            latency: dbLatency,\n            connections: Math.floor(Math.random() * 10) + 5 // Mock connection count\n          },\n          redis: {\n            status: 'healthy', // Mock Redis status\n            latency: Math.floor(Math.random() * 5) + 1,\n            memory_usage: Math.random() * 100\n          },\n          proxies: {\n            active: activeProxies,\n            total: totalProxies,\n            success_rate: avgSuccessRate\n          },\n          system: mockSystemMetrics\n        });\n\n      } catch (error) {\n        console.error('Error checking system health:', error);\n        setHealthData(prev => ({\n          ...prev,\n          database: { status: 'error', latency: null, connections: null }\n        }));\n      } finally {\n        setIsLoading(false);\n      }\n    };\n\n    checkSystemHealth();\n    const interval = setInterval(checkSystemHealth, 30000); // Check every 30 seconds\n\n    return () => clearInterval(interval);\n  }, [supabase]);\n\n  const getStatusIcon = (status, size = 16) => {\n    switch (status) {\n      case 'healthy':\n        return <CheckCircle className=\"status-icon healthy\" size={size} />;\n      case 'warning':\n        return <AlertTriangle className=\"status-icon warning\" size={size} />;\n      case 'error':\n        return <WifiOff className=\"status-icon error\" size={size} />;\n      case 'checking':\n      default:\n        return <Clock className=\"status-icon checking\" size={size} />;\n    }\n  };\n\n  const formatUptime = (milliseconds) => {\n    const seconds = Math.floor(milliseconds / 1000);\n    const minutes = Math.floor(seconds / 60);\n    const hours = Math.floor(minutes / 60);\n    const days = Math.floor(hours / 24);\n\n    if (days > 0) return `${days}d ${hours % 24}h`;\n    if (hours > 0) return `${hours}h ${minutes % 60}m`;\n    return `${minutes}m`;\n  };\n\n  const getUsageColor = (percentage) => {\n    if (percentage > 90) return 'critical';\n    if (percentage > 75) return 'warning';\n    if (percentage > 50) return 'moderate';\n    return 'good';\n  };\n\n  if (isLoading) {\n    return (\n      <div className=\"system-health loading\">\n        <div className=\"section-header\">\n          <h2>System Health</h2>\n        </div>\n        <div className=\"loading-state\">\n          <Activity className=\"spinner\" size={24} />\n          <span>Checking system status...</span>\n        </div>\n      </div>\n    );\n  }\n\n  return (\n    <div className=\"system-health\">\n      <div className=\"section-header\">\n        <h2>System Health</h2>\n        <div className=\"health-status\">\n          {getStatusIcon('healthy', 20)}\n          <span className=\"status-text\">All systems operational</span>\n        </div>\n      </div>\n\n      <div className=\"health-grid\">\n        {/* Database Health */}\n        <div className=\"health-card\">\n          <div className=\"health-header\">\n            <Database className=\"service-icon\" size={20} />\n            <div className=\"service-info\">\n              <h3>Database</h3>\n              <div className=\"service-status\">\n                {getStatusIcon(healthData.database.status)}\n                <span className={`status-text ${healthData.database.status}`}>\n                  {healthData.database.status}\n                </span>\n              </div>\n            </div>\n          </div>\n          <div className=\"health-metrics\">\n            <div className=\"metric\">\n              <span className=\"metric-label\">Latency:</span>\n              <span className=\"metric-value\">\n                {healthData.database.latency ? `${healthData.database.latency}ms` : '-'}\n              </span>\n            </div>\n            <div className=\"metric\">\n              <span className=\"metric-label\">Connections:</span>\n              <span className=\"metric-value\">{healthData.database.connections || 0}</span>\n            </div>\n          </div>\n        </div>\n\n        {/* Redis Health */}\n        <div className=\"health-card\">\n          <div className=\"health-header\">\n            <Server className=\"service-icon\" size={20} />\n            <div className=\"service-info\">\n              <h3>Redis Queue</h3>\n              <div className=\"service-status\">\n                {getStatusIcon(healthData.redis.status)}\n                <span className={`status-text ${healthData.redis.status}`}>\n                  {healthData.redis.status}\n                </span>\n              </div>\n            </div>\n          </div>\n          <div className=\"health-metrics\">\n            <div className=\"metric\">\n              <span className=\"metric-label\">Latency:</span>\n              <span className=\"metric-value\">\n                {healthData.redis.latency ? `${healthData.redis.latency}ms` : '-'}\n              </span>\n            </div>\n            <div className=\"metric\">\n              <span className=\"metric-label\">Memory:</span>\n              <span className=\"metric-value\">\n                {healthData.redis.memory_usage ? `${healthData.redis.memory_usage.toFixed(1)}%` : '-'}\n              </span>\n            </div>\n          </div>\n        </div>\n\n        {/* Proxy Health */}\n        <div className=\"health-card\">\n          <div className=\"health-header\">\n            <Wifi className=\"service-icon\" size={20} />\n            <div className=\"service-info\">\n              <h3>Proxy Network</h3>\n              <div className=\"service-status\">\n                {getStatusIcon(healthData.proxies.active > 0 ? 'healthy' : 'warning')}\n                <span className={`status-text ${healthData.proxies.active > 0 ? 'healthy' : 'warning'}`}>\n                  {healthData.proxies.active > 0 ? 'Active' : 'Limited'}\n                </span>\n              </div>\n            </div>\n          </div>\n          <div className=\"health-metrics\">\n            <div className=\"metric\">\n              <span className=\"metric-label\">Active:</span>\n              <span className=\"metric-value\">\n                {healthData.proxies.active}/{healthData.proxies.total}\n              </span>\n            </div>\n            <div className=\"metric\">\n              <span className=\"metric-label\">Success Rate:</span>\n              <span className=\"metric-value\">\n                {(healthData.proxies.success_rate * 100).toFixed(1)}%\n              </span>\n            </div>\n          </div>\n        </div>\n\n        {/* System Resources */}\n        <div className=\"health-card system-resources\">\n          <div className=\"health-header\">\n            <Activity className=\"service-icon\" size={20} />\n            <div className=\"service-info\">\n              <h3>System Resources</h3>\n              <div className=\"service-status\">\n                <span className=\"status-text uptime\">\n                  Uptime: {formatUptime(healthData.system.uptime)}\n                </span>\n              </div>\n            </div>\n          </div>\n          \n          <div className=\"resource-metrics\">\n            <div className=\"resource-item\">\n              <div className=\"resource-header\">\n                <Cpu size={16} />\n                <span>CPU Usage</span>\n                <span className=\"resource-value\">\n                  {healthData.system.cpu_usage.toFixed(1)}%\n                </span>\n              </div>\n              <div className=\"resource-bar\">\n                <div \n                  className={`resource-fill ${getUsageColor(healthData.system.cpu_usage)}`}\n                  style={{ width: `${healthData.system.cpu_usage}%` }}\n                ></div>\n              </div>\n            </div>\n\n            <div className=\"resource-item\">\n              <div className=\"resource-header\">\n                <MemoryStick size={16} />\n                <span>Memory</span>\n                <span className=\"resource-value\">\n                  {healthData.system.memory_usage.toFixed(1)}%\n                </span>\n              </div>\n              <div className=\"resource-bar\">\n                <div \n                  className={`resource-fill ${getUsageColor(healthData.system.memory_usage)}`}\n                  style={{ width: `${healthData.system.memory_usage}%` }}\n                ></div>\n              </div>\n            </div>\n\n            <div className=\"resource-item\">\n              <div className=\"resource-header\">\n                <HardDrive size={16} />\n                <span>Disk Usage</span>\n                <span className=\"resource-value\">\n                  {healthData.system.disk_usage.toFixed(1)}%\n                </span>\n              </div>\n              <div className=\"resource-bar\">\n                <div \n                  className={`resource-fill ${getUsageColor(healthData.system.disk_usage)}`}\n                  style={{ width: `${healthData.system.disk_usage}%` }}\n                ></div>\n              </div>\n            </div>\n          </div>\n        </div>\n      </div>\n\n      <div className=\"health-footer\">\n        <span className=\"last-check\">\n          Last health check: {new Date().toLocaleTimeString()}\n        </span>\n        <button className=\"refresh-health\">\n          <Activity size={14} />\n          Auto-refresh: 30s\n        </button>\n      </div>\n    </div>\n  );\n};\n\nexport default SystemHealth;","usedDeprecatedRules":[{"ruleId":"dot-location","replacedBy":[]},{"ruleId":"new-parens","replacedBy":[]},{"ruleId":"no-mixed-operators","replacedBy":[]},{"ruleId":"no-new-object","replacedBy":["no-object-constructor"]},{"ruleId":"no-whitespace-before-property","replacedBy":[]},{"ruleId":"rest-spread-spacing","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\frontend\\src\\components\\Layout\\Navigation.js","messages":[{"ruleId":"no-unused-vars","severity":1,"message":"'location' is assigned a value but never used.","line":16,"column":9,"nodeType":"Identifier","messageId":"unusedVar","endLine":16,"endColumn":17}],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":0,"source":"import React from 'react';\nimport { NavLink, useLocation } from 'react-router-dom';\nimport { \n  Home, \n  FolderOpen, \n  Bot, \n  GraduationCap, \n  Database, \n  BarChart3, \n  Settings,\n  Activity\n} from 'lucide-react';\nimport './Navigation.css';\n\nconst Navigation = () => {\n  const location = useLocation();\n\n  const navItems = [\n    {\n      path: '/',\n      icon: <Home size={20} />,\n      label: 'Dashboard',\n      exact: true\n    },\n    {\n      path: '/projects',\n      icon: <FolderOpen size={20} />,\n      label: 'Projects'\n    },\n    {\n      path: '/scrapers',\n      icon: <Bot size={20} />,\n      label: 'Scrapers'\n    },\n    {\n      path: '/training',\n      icon: <GraduationCap size={20} />,\n      label: 'Training'\n    },\n    {\n      path: '/data',\n      icon: <Database size={20} />,\n      label: 'Data Explorer'\n    },\n    {\n      path: '/analytics',\n      icon: <BarChart3 size={20} />,\n      label: 'Analytics'\n    }\n  ];\n\n  return (\n    <nav className=\"navigation\">\n      <div className=\"nav-header\">\n        <div className=\"nav-logo\">\n          <Activity size={24} />\n          <span className=\"nav-title\">APL AI Scraper</span>\n        </div>\n        <div className=\"nav-version\">v2.0</div>\n      </div>\n\n      <div className=\"nav-menu\">\n        {navItems.map((item) => (\n          <NavLink\n            key={item.path}\n            to={item.path}\n            className={({ isActive }) => \n              `nav-item ${isActive ? 'active' : ''}`\n            }\n            end={item.exact}\n          >\n            <span className=\"nav-icon\">{item.icon}</span>\n            <span className=\"nav-label\">{item.label}</span>\n          </NavLink>\n        ))}\n      </div>\n\n      <div className=\"nav-footer\">\n        <div className=\"nav-item nav-settings\">\n          <span className=\"nav-icon\">\n            <Settings size={20} />\n          </span>\n          <span className=\"nav-label\">Settings</span>\n        </div>\n        \n        <div className=\"nav-status\">\n          <div className=\"status-indicator online\"></div>\n          <span className=\"status-text\">System Online</span>\n        </div>\n      </div>\n    </nav>\n  );\n};\n\nexport default Navigation;","usedDeprecatedRules":[{"ruleId":"dot-location","replacedBy":[]},{"ruleId":"new-parens","replacedBy":[]},{"ruleId":"no-mixed-operators","replacedBy":[]},{"ruleId":"no-new-object","replacedBy":["no-object-constructor"]},{"ruleId":"no-whitespace-before-property","replacedBy":[]},{"ruleId":"rest-spread-spacing","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\frontend\\src\\contexts\\NotificationContext.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"dot-location","replacedBy":[]},{"ruleId":"new-parens","replacedBy":[]},{"ruleId":"no-mixed-operators","replacedBy":[]},{"ruleId":"no-new-object","replacedBy":["no-object-constructor"]},{"ruleId":"no-whitespace-before-property","replacedBy":[]},{"ruleId":"rest-spread-spacing","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\frontend\\src\\contexts\\SupabaseContext.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"dot-location","replacedBy":[]},{"ruleId":"new-parens","replacedBy":[]},{"ruleId":"no-mixed-operators","replacedBy":[]},{"ruleId":"no-new-object","replacedBy":["no-object-constructor"]},{"ruleId":"no-whitespace-before-property","replacedBy":[]},{"ruleId":"rest-spread-spacing","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\healthcheck.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\jest.config.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\middleware\\security-middleware.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\models\\scraper-template.js","messages":[{"ruleId":"no-unused-vars","severity":1,"message":"'reason' is defined but never used.","line":112,"column":43,"nodeType":"Identifier","messageId":"unusedVar","endLine":112,"endColumn":49}],"suppressedMessages":[{"ruleId":"no-useless-escape","severity":2,"message":"Unnecessary escape character: \\/.","line":302,"column":34,"nodeType":"Literal","messageId":"unnecessaryEscape","endLine":302,"endColumn":35,"suggestions":[{"messageId":"removeEscape","fix":{"range":[9372,9373],"text":""},"desc":"Remove the `\\`. This maintains the current functionality."},{"messageId":"escapeBackslash","fix":{"range":[9372,9372],"text":"\\"},"desc":"Replace the `\\` with `\\\\` to include the actual backslash character."}],"suppressions":[{"kind":"directive","justification":""}]},{"ruleId":"no-useless-escape","severity":2,"message":"Unnecessary escape character: \\/.","line":302,"column":46,"nodeType":"Literal","messageId":"unnecessaryEscape","endLine":302,"endColumn":47,"suggestions":[{"messageId":"removeEscape","fix":{"range":[9384,9385],"text":""},"desc":"Remove the `\\`. This maintains the current functionality."},{"messageId":"escapeBackslash","fix":{"range":[9384,9384],"text":"\\"},"desc":"Replace the `\\` with `\\\\` to include the actual backslash character."}],"suppressions":[{"kind":"directive","justification":""}]},{"ruleId":"no-useless-escape","severity":2,"message":"Unnecessary escape character: \\/.","line":302,"column":64,"nodeType":"Literal","messageId":"unnecessaryEscape","endLine":302,"endColumn":65,"suggestions":[{"messageId":"removeEscape","fix":{"range":[9402,9403],"text":""},"desc":"Remove the `\\`. This maintains the current functionality."},{"messageId":"escapeBackslash","fix":{"range":[9402,9402],"text":"\\"},"desc":"Replace the `\\` with `\\\\` to include the actual backslash character."}],"suppressions":[{"kind":"directive","justification":""}]},{"ruleId":"no-useless-escape","severity":2,"message":"Unnecessary escape character: \\/.","line":302,"column":76,"nodeType":"Literal","messageId":"unnecessaryEscape","endLine":302,"endColumn":77,"suggestions":[{"messageId":"removeEscape","fix":{"range":[9414,9415],"text":""},"desc":"Remove the `\\`. This maintains the current functionality."},{"messageId":"escapeBackslash","fix":{"range":[9414,9414],"text":"\\"},"desc":"Replace the `\\` with `\\\\` to include the actual backslash character."}],"suppressions":[{"kind":"directive","justification":""}]},{"ruleId":"no-useless-escape","severity":2,"message":"Unnecessary escape character: \\/.","line":303,"column":34,"nodeType":"Literal","messageId":"unnecessaryEscape","endLine":303,"endColumn":35,"suggestions":[{"messageId":"removeEscape","fix":{"range":[9476,9477],"text":""},"desc":"Remove the `\\`. This maintains the current functionality."},{"messageId":"escapeBackslash","fix":{"range":[9476,9476],"text":"\\"},"desc":"Replace the `\\` with `\\\\` to include the actual backslash character."}],"suppressions":[{"kind":"directive","justification":""}]},{"ruleId":"no-useless-escape","severity":2,"message":"Unnecessary escape character: \\/.","line":303,"column":46,"nodeType":"Literal","messageId":"unnecessaryEscape","endLine":303,"endColumn":47,"suggestions":[{"messageId":"removeEscape","fix":{"range":[9488,9489],"text":""},"desc":"Remove the `\\`. This maintains the current functionality."},{"messageId":"escapeBackslash","fix":{"range":[9488,9488],"text":"\\"},"desc":"Replace the `\\` with `\\\\` to include the actual backslash character."}],"suppressions":[{"kind":"directive","justification":""}]},{"ruleId":"no-useless-escape","severity":2,"message":"Unnecessary escape character: \\/.","line":303,"column":64,"nodeType":"Literal","messageId":"unnecessaryEscape","endLine":303,"endColumn":65,"suggestions":[{"messageId":"removeEscape","fix":{"range":[9506,9507],"text":""},"desc":"Remove the `\\`. This maintains the current functionality."},{"messageId":"escapeBackslash","fix":{"range":[9506,9506],"text":"\\"},"desc":"Replace the `\\` with `\\\\` to include the actual backslash character."}],"suppressions":[{"kind":"directive","justification":""}]},{"ruleId":"no-useless-escape","severity":2,"message":"Unnecessary escape character: \\/.","line":303,"column":76,"nodeType":"Literal","messageId":"unnecessaryEscape","endLine":303,"endColumn":77,"suggestions":[{"messageId":"removeEscape","fix":{"range":[9518,9519],"text":""},"desc":"Remove the `\\`. This maintains the current functionality."},{"messageId":"escapeBackslash","fix":{"range":[9518,9518],"text":"\\"},"desc":"Replace the `\\` with `\\\\` to include the actual backslash character."}],"suppressions":[{"kind":"directive","justification":""}]}],"errorCount":0,"fatalErrorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":0,"source":"∩╗┐const { createClient } = require('@supabase/supabase-js');\n\nclass ScraperTemplate {\n  constructor(supabase) {\n    this.supabase = supabase || createClient(\n      process.env.SUPABASE_URL,\n      process.env.SUPABASE_SERVICE_KEY\n    );\n  }\n\n  async createTemplate(projectId, templateData) {\n    try {\n      console.log(`├░┼╕ΓÇ£┬¥ Creating scraper template: ${templateData.name}`);\n\n      const { data, error } = await this.supabase\n        .from('scraper_templates')\n        .insert([{\n          project_id: projectId,\n          name: templateData.name,\n          description: templateData.description,\n          code: templateData.code,\n          config: templateData.config || {},\n          version: '1.0.0',\n          status: 'active'\n        }])\n        .select()\n        .single();\n\n      if (error) throw error;\n\n      // Initialize metrics for new template\n      await this.initializeMetrics(data.id);\n\n      console.log(`├ó┼ôΓÇª Template created successfully: ${data.id}`);\n      return data;\n    } catch (error) {\n      console.error('Error creating template:', error);\n      throw error;\n    }\n  }\n\n  async updateTemplate(templateId, updates) {\n    try {\n      console.log(`├░┼╕ΓÇ¥ΓÇ₧ Updating template: ${templateId}`);\n\n      // Get current template data\n      const { data: current, error: fetchError } = await this.supabase\n        .from('scraper_templates')\n        .select('*')\n        .eq('id', templateId)\n        .single();\n\n      if (fetchError) throw fetchError;\n\n      // Archive current version if code is being updated\n      if (updates.code && updates.code !== current.code) {\n        await this.archiveVersion(templateId, current, updates.changeReason || 'Template update');\n      }\n\n      // Increment version if code changed\n      const newVersion = updates.code ? this.incrementVersion(current.version) : current.version;\n\n      const updateData = {\n        ...updates,\n        version: newVersion,\n        updated_at: new Date().toISOString()\n      };\n\n      const { data, error } = await this.supabase\n        .from('scraper_templates')\n        .update(updateData)\n        .eq('id', templateId)\n        .select()\n        .single();\n\n      if (error) throw error;\n\n      console.log(`├ó┼ôΓÇª Template updated successfully: ${templateId} (v${newVersion})`);\n      return data;\n    } catch (error) {\n      console.error('Error updating template:', error);\n      throw error;\n    }\n  }\n\n  async archiveVersion(templateId, currentTemplate, changeReason) {\n    try {\n      const { error } = await this.supabase\n        .from('scraper_template_versions')\n        .insert([{\n          template_id: templateId,\n          code: currentTemplate.code,\n          config: currentTemplate.config,\n          version: currentTemplate.version,\n          change_reason: changeReason\n        }]);\n\n      if (error) throw error;\n      console.log(`├░┼╕ΓÇ£┼í Archived version ${currentTemplate.version} for template ${templateId}`);\n    } catch (error) {\n      console.error('Error archiving version:', error);\n      throw error;\n    }\n  }\n\n  incrementVersion(version) {\n    const parts = version.split('.');\n    const patch = parseInt(parts[2]) + 1;\n    return `${parts[0]}.${parts[1]}.${patch}`;\n  }\n\n  async incrementMinorVersion(templateId, reason) {\n    try {\n      const { data: current } = await this.supabase\n        .from('scraper_templates')\n        .select('version')\n        .eq('id', templateId)\n        .single();\n\n      const parts = current.version.split('.');\n      const minor = parseInt(parts[1]) + 1;\n      const newVersion = `${parts[0]}.${minor}.0`;\n\n      await this.supabase\n        .from('scraper_templates')\n        .update({ version: newVersion })\n        .eq('id', templateId);\n\n      return newVersion;\n    } catch (error) {\n      console.error('Error incrementing minor version:', error);\n      throw error;\n    }\n  }\n\n  async detectChanges(templateId, currentResults) {\n    try {\n      console.log(`├░┼╕ΓÇ¥┬ì Analyzing changes for template: ${templateId}`);\n\n      // Get previous successful results for comparison\n      const { data: previousExecutions } = await this.supabase\n        .from('scraping_executions')\n        .select('processed_result, raw_result, created_at')\n        .eq('template_id', templateId)\n        .eq('status', 'completed')\n        .not('processed_result', 'is', null)\n        .order('created_at', { ascending: false })\n        .limit(10);\n\n      if (!previousExecutions || previousExecutions.length === 0) {\n        console.log('No previous results to compare against');\n        return { noComparisonData: true };\n      }\n\n      const changes = this.analyzeDataChanges(previousExecutions, currentResults);\n      \n      if (changes.significantChange) {\n        await this.flagTemplateForReview(templateId, changes);\n        console.log(`├░┼╕┼í┬¿ Significant changes detected for template ${templateId}`);\n      }\n\n      return changes;\n    } catch (error) {\n      console.error('Error detecting changes:', error);\n      throw error;\n    }\n  }\n\n  analyzeDataChanges(previousExecutions, currentResults) {\n    const changes = {\n      structuralChanges: [],\n      contentChanges: [],\n      selectorFailures: [],\n      significantChange: false,\n      confidence: 0\n    };\n\n    try {\n      // Analyze structural changes\n      const previousKeys = this.extractDataKeys(previousExecutions);\n      const currentKeys = this.extractDataKeys([{ processed_result: currentResults }]);\n\n      const addedKeys = currentKeys.filter(k => !previousKeys.includes(k));\n      const removedKeys = previousKeys.filter(k => !currentKeys.includes(k));\n\n      if (addedKeys.length > 0) {\n        changes.structuralChanges.push({\n          type: 'added_fields',\n          fields: addedKeys,\n          impact: 'medium'\n        });\n      }\n\n      if (removedKeys.length > 0) {\n        changes.structuralChanges.push({\n          type: 'removed_fields',\n          fields: removedKeys,\n          impact: 'high'\n        });\n      }\n\n      // Analyze content patterns\n      const contentAnalysis = this.analyzeContentPatterns(previousExecutions, currentResults);\n      changes.contentChanges = contentAnalysis;\n\n      // Check for selector failures (empty/null values where there were values before)\n      const selectorAnalysis = this.analyzeSelectorReliability(previousExecutions, currentResults);\n      changes.selectorFailures = selectorAnalysis;\n\n      // Determine significance\n      const structuralWeight = changes.structuralChanges.length * 0.4;\n      const contentWeight = changes.contentChanges.length * 0.3;\n      const selectorWeight = changes.selectorFailures.length * 0.6;\n\n      changes.confidence = Math.min((structuralWeight + contentWeight + selectorWeight) / 3, 1.0);\n      changes.significantChange = changes.confidence > 0.3 || removedKeys.length > 0;\n\n      return changes;\n    } catch (error) {\n      console.error('Error analyzing data changes:', error);\n      return { error: error.message, significantChange: false };\n    }\n  }\n\n  extractDataKeys(executions) {\n    const allKeys = new Set();\n    \n    executions.forEach(execution => {\n      const result = execution.processed_result || execution.raw_result;\n      if (result && typeof result === 'object') {\n        this.extractKeysRecursive(result, '', allKeys);\n      }\n    });\n\n    return Array.from(allKeys);\n  }\n\n  extractKeysRecursive(obj, prefix, keySet) {\n    Object.keys(obj).forEach(key => {\n      const fullKey = prefix ? `${prefix}.${key}` : key;\n      keySet.add(fullKey);\n      \n      if (obj[key] && typeof obj[key] === 'object' && !Array.isArray(obj[key])) {\n        this.extractKeysRecursive(obj[key], fullKey, keySet);\n      }\n    });\n  }\n\n  analyzeContentPatterns(previousExecutions, currentResults) {\n    const patterns = [];\n    \n    try {\n      // Sample analysis - compare data types, lengths, formats\n      const previousSample = previousExecutions[0]?.processed_result || {};\n      const currentSample = currentResults || {};\n\n      Object.keys(currentSample).forEach(key => {\n        if (previousSample[key] !== undefined) {\n          const prevType = typeof previousSample[key];\n          const currType = typeof currentSample[key];\n          \n          if (prevType !== currType) {\n            patterns.push({\n              type: 'type_change',\n              field: key,\n              previous: prevType,\n              current: currType,\n              impact: 'medium'\n            });\n          }\n\n          // Check for format changes in strings\n          if (prevType === 'string' && currType === 'string') {\n            const formatChange = this.detectFormatChange(previousSample[key], currentSample[key]);\n            if (formatChange) {\n              patterns.push({\n                type: 'format_change',\n                field: key,\n                change: formatChange,\n                impact: 'low'\n              });\n            }\n          }\n        }\n      });\n\n      return patterns;\n    } catch (error) {\n      console.error('Error analyzing content patterns:', error);\n      return [];\n    }\n  }\n\n  detectFormatChange(prevValue, currValue) {\n    if (!prevValue || !currValue) return null;\n\n    // Detect common format changes\n    /* eslint-disable no-useless-escape */\n    const prevIsPrice = /[$Γé¼┬ú┬Ñ]\\s*[\\d,.]+|[\\d,.]+\\s*[$Γé¼┬ú┬Ñ]/i.test(prevValue);\n    const currIsPrice = /[$Γé¼┬ú┬Ñ]\\s*[\\d,.]+|[\\d,.]+\\s*[$Γé¼┬ú┬Ñ]/i.test(currValue);\n\n    const prevIsDate = /\\d{1,2}[-\\/]\\d{1,2}[-\\/]\\d{2,4}|\\d{4}[-\\/]\\d{1,2}[-\\/]\\d{1,2}/.test(prevValue);\n    const currIsDate = /\\d{1,2}[-\\/]\\d{1,2}[-\\/]\\d{2,4}|\\d{4}[-\\/]\\d{1,2}[-\\/]\\d{1,2}/.test(currValue);\n    /* eslint-enable no-useless-escape */\n\n    if (prevIsPrice !== currIsPrice) return 'price_format_change';\n    if (prevIsDate !== currIsDate) return 'date_format_change';\n\n    return null;\n  }\n\n  analyzeSelectorReliability(previousExecutions, currentResults) {\n    const failures = [];\n    \n    try {\n      const previousSample = previousExecutions[0]?.processed_result || {};\n      const currentSample = currentResults || {};\n\n      Object.keys(previousSample).forEach(key => {\n        const prevValue = previousSample[key];\n        const currValue = currentSample[key];\n\n        // Check if previously successful field is now empty/null\n        if (prevValue && (currValue === null || currValue === undefined || currValue === '')) {\n          failures.push({\n            type: 'selector_failure',\n            field: key,\n            previousValue: this.sanitizeValue(prevValue),\n            impact: 'high',\n            suggestion: 'Update selector or add fallback strategy'\n          });\n        }\n      });\n\n      return failures;\n    } catch (error) {\n      console.error('Error analyzing selector reliability:', error);\n      return [];\n    }\n  }\n\n  sanitizeValue(value) {\n    if (typeof value === 'string' && value.length > 100) {\n      return value.substring(0, 100) + '...';\n    }\n    return value;\n  }\n\n  async flagTemplateForReview(templateId, changes) {\n    try {\n      await this.supabase\n        .from('scraper_templates')\n        .update({\n          status: 'needs_review',\n          last_change_detected: new Date().toISOString(),\n          change_details: changes\n        })\n        .eq('id', templateId);\n\n      // Log change history\n      await this.supabase\n        .from('template_change_history')\n        .insert([{\n          template_id: templateId,\n          change_type: changes.significantChange ? 'significant_change' : 'minor_change',\n          change_description: this.generateChangeDescription(changes),\n          confidence_score: changes.confidence,\n          sample_data: changes,\n          suggested_fix: this.generateSuggestedFix(changes)\n        }]);\n\n      console.log(`├░┼╕ΓÇ¥ΓÇ¥ Template ${templateId} flagged for review due to significant changes`);\n    } catch (error) {\n      console.error('Error flagging template for review:', error);\n      throw error;\n    }\n  }\n\n  generateChangeDescription(changes) {\n    const descriptions = [];\n    \n    if (changes.structuralChanges.length > 0) {\n      descriptions.push(`Structural changes: ${changes.structuralChanges.length} detected`);\n    }\n    \n    if (changes.selectorFailures.length > 0) {\n      descriptions.push(`Selector failures: ${changes.selectorFailures.length} fields affected`);\n    }\n    \n    if (changes.contentChanges.length > 0) {\n      descriptions.push(`Content pattern changes: ${changes.contentChanges.length} detected`);\n    }\n\n    return descriptions.join('; ') || 'Unknown changes detected';\n  }\n\n  generateSuggestedFix(changes) {\n    const suggestions = [];\n    \n    if (changes.selectorFailures.length > 0) {\n      suggestions.push('Review and update CSS selectors for failed fields');\n    }\n    \n    if (changes.structuralChanges.some(c => c.type === 'removed_fields')) {\n      suggestions.push('Check if removed fields moved to different locations');\n    }\n    \n    if (changes.contentChanges.some(c => c.type === 'format_change')) {\n      suggestions.push('Update data normalization rules for format changes');\n    }\n\n    return suggestions.join('; ') || 'Manual review recommended';\n  }\n\n  async initializeMetrics(templateId) {\n    try {\n      const { error } = await this.supabase\n        .from('template_metrics')\n        .insert([{\n          template_id: templateId,\n          total_runs: 0,\n          successful_runs: 0,\n          failed_runs: 0,\n          success_rate: 0,\n          average_duration: 0\n        }]);\n\n      if (error) throw error;\n    } catch (error) {\n      console.error('Error initializing metrics:', error);\n      // Don't throw here as it's not critical for template creation\n    }\n  }\n\n  async getTemplate(templateId) {\n    try {\n      const { data, error } = await this.supabase\n        .from('scraper_templates')\n        .select(`\n          *,\n          template_metrics(*),\n          scraper_template_versions(*)\n        `)\n        .eq('id', templateId)\n        .single();\n\n      if (error) throw error;\n      return data;\n    } catch (error) {\n      console.error('Error fetching template:', error);\n      throw error;\n    }\n  }\n\n  async listTemplates(projectId, options = {}) {\n    try {\n      let query = this.supabase\n        .from('scraper_templates')\n        .select(`\n          *,\n          template_metrics(success_rate, total_runs, last_run)\n        `)\n        .order('created_at', { ascending: false });\n\n      if (projectId) {\n        query = query.eq('project_id', projectId);\n      }\n\n      if (options.status) {\n        query = query.eq('status', options.status);\n      }\n\n      if (options.limit) {\n        query = query.limit(options.limit);\n      }\n\n      const { data, error } = await query;\n      if (error) throw error;\n\n      return data;\n    } catch (error) {\n      console.error('Error listing templates:', error);\n      throw error;\n    }\n  }\n\n  async deleteTemplate(templateId) {\n    try {\n      const { error } = await this.supabase\n        .from('scraper_templates')\n        .delete()\n        .eq('id', templateId);\n\n      if (error) throw error;\n      console.log(`├░┼╕ΓÇöΓÇÿ├»┬╕┬Å Template ${templateId} deleted successfully`);\n    } catch (error) {\n      console.error('Error deleting template:', error);\n      throw error;\n    }\n  }\n\n  async cloneTemplate(templateId, newName, projectId) {\n    try {\n      const template = await this.getTemplate(templateId);\n      \n      const clonedTemplate = await this.createTemplate(projectId || template.project_id, {\n        name: newName,\n        description: `Cloned from ${template.name}`,\n        code: template.code,\n        config: template.config\n      });\n\n      console.log(`├░┼╕ΓÇ£ΓÇ╣ Template cloned: ${templateId} -> ${clonedTemplate.id}`);\n      return clonedTemplate;\n    } catch (error) {\n      console.error('Error cloning template:', error);\n      throw error;\n    }\n  }\n}\n\nmodule.exports = { ScraperTemplate };","usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\routes\\admin.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\routes\\auth.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\routes\\privacy.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\routes\\public.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\scrapers\\playwright-scraper.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\scripts\\apply-create-missing-tables.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\scripts\\db-schema-check-direct.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\scripts\\find_unmatched_braces.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\scripts\\migrate.js","messages":[{"ruleId":"no-unused-vars","severity":1,"message":"'data' is assigned a value but never used.","line":56,"column":15,"nodeType":"Identifier","messageId":"unusedVar","endLine":56,"endColumn":19}],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":0,"source":"∩╗┐// APL AI Scraper 2.0 - Database Migration Script\nconst { createClient } = require('@supabase/supabase-js');\nconst fs = require('fs').promises;\nconst path = require('path');\n\nrequire('dotenv').config();\n\nclass MigrationRunner {\n  constructor() {\n    this.supabase = createClient(\n      process.env.SUPABASE_URL,\n      process.env.SUPABASE_SERVICE_ROLE_KEY || process.env.SUPABASE_ANON_KEY\n    );\n  }\n\n  async runMigrations() {\n    try {\n      console.log('├░┼╕┼íΓé¼ Starting database migrations...');\n\n      const migrationsDir = path.join(__dirname, '..', 'migrations');\n      const migrationFiles = await fs.readdir(migrationsDir);\n\n      // Sort migration files by name (assuming naming convention: 001_name.sql)\n      const sortedMigrations = migrationFiles\n        .filter(file => file.endsWith('.sql'))\n        .sort();\n\n      for (const migrationFile of sortedMigrations) {\n        console.log(`├░┼╕ΓÇ£ΓÇ₧ Running migration: ${migrationFile}`);\n        \n        const migrationPath = path.join(migrationsDir, migrationFile);\n        const migrationSql = await fs.readFile(migrationPath, 'utf8');\n\n        // Execute migration\n        const { error } = await this.supabase.rpc('exec_sql', {\n          sql_query: migrationSql\n        });\n\n        if (error) {\n          console.error(`├ó┬¥┼Æ Migration ${migrationFile} failed:`, error);\n          throw error;\n        }\n\n        console.log(`├ó┼ôΓÇª Migration ${migrationFile} completed`);\n      }\n\n      console.log('├░┼╕┼╜ΓÇ░ All migrations completed successfully!');\n    } catch (error) {\n      console.error('├░┼╕ΓÇÖ┬Ñ Migration failed:', error);\n      process.exit(1);\n    }\n  }\n\n  async checkConnection() {\n    try {\n      const { data, error } = await this.supabase\n        .from('users')\n        .select('count(*)')\n        .limit(1);\n\n      if (error) {\n        console.error('├ó┬¥┼Æ Database connection failed:', error.message);\n        return false;\n      }\n\n      console.log('├ó┼ôΓÇª Database connection successful');\n      return true;\n    } catch (error) {\n      console.error('├ó┬¥┼Æ Database connection error:', error.message);\n      return false;\n    }\n  }\n}\n\n// Run migrations if called directly\nif (require.main === module) {\n  const runner = new MigrationRunner();\n  \n  runner.checkConnection().then(connected => {\n    if (connected) {\n      runner.runMigrations();\n    } else {\n      console.error('├░┼╕ΓÇÖ┬Ñ Cannot run migrations without database connection');\n      process.exit(1);\n    }\n  });\n}\n\nmodule.exports = { MigrationRunner };","usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\scripts\\schema-diff-report.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\scripts\\simple-db-verify.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\scripts\\test-pg-connection.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\scripts\\verify-database.js","messages":[{"ruleId":"no-unused-vars","severity":1,"message":"'path' is assigned a value but never used.","line":2,"column":7,"nodeType":"Identifier","messageId":"unusedVar","endLine":2,"endColumn":11},{"ruleId":"no-unused-vars","severity":1,"message":"'data' is assigned a value but never used.","line":23,"column":15,"nodeType":"Identifier","messageId":"unusedVar","endLine":23,"endColumn":19}],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":2,"fixableErrorCount":0,"fixableWarningCount":0,"source":"const { createClient } = require('@supabase/supabase-js');\r\nconst path = require('path');\r\n\r\nclass DatabaseVerifier {\r\n  constructor() {\r\n    if (!process.env.SUPABASE_URL || (!process.env.SUPABASE_ANON_KEY && !process.env.SUPABASE_SERVICE_ROLE_KEY)) {\r\n      console.log('Γ¥î Supabase environment variables not set');\r\n      console.log('   Set SUPABASE_URL and SUPABASE_ANON_KEY or SUPABASE_SERVICE_ROLE_KEY');\r\n      process.exit(1);\r\n    }\r\n\r\n    const keyToUse = process.env.SUPABASE_SERVICE_ROLE_KEY || process.env.SUPABASE_ANON_KEY;\r\n    this.supabase = createClient(\r\n      process.env.SUPABASE_URL,\r\n      keyToUse\r\n    );\r\n  }\r\n\r\n  async testConnection() {\r\n    try {\r\n      console.log('Testing Supabase connection...');\r\n\r\n      const { data, error } = await this.supabase\r\n        .from('projects')\r\n        .select('id')\r\n        .limit(1);\r\n\r\n      if (error) {\r\n        throw new Error(`Connection failed: ${error.message}`);\r\n      }\r\n\r\n      console.log('Γ£à Supabase connection successful');\r\n      return true;\r\n    } catch (error) {\r\n      console.log(`Γ¥î Supabase connection failed: ${error.message}`);\r\n      return false;\r\n    }\r\n  }\r\n\r\n  async verifySchema() {\r\n    const expectedTables = [\r\n      'projects',\r\n      'scraper_templates',\r\n      'scraping_jobs',\r\n      'scraped_data',\r\n      'user_profiles',\r\n      'login_attempts',\r\n      'api_keys'\r\n    ];\r\n\r\n    console.log('Verifying database schema...');\r\n\r\n    const results = {\r\n      connected: false,\r\n      tables: {},\r\n      missingTables: [],\r\n      schemaMatches: false\r\n    };\r\n\r\n    results.connected = await this.testConnection();\r\n    if (!results.connected) return results;\r\n\r\n    for (const tableName of expectedTables) {\r\n      try {\r\n        const { data, error } = await this.supabase\r\n          .from(tableName)\r\n          .select('*')\r\n          .limit(1);\r\n\r\n        if (error) {\r\n          // Supabase exposes Postgres errors differently; check message\r\n          const msg = error.message || '';\r\n          if (msg.includes('does not exist') || msg.includes('relation') || msg.includes('42P01')) {\r\n            results.tables[tableName] = { exists: false };\r\n            results.missingTables.push(tableName);\r\n          } else {\r\n            results.tables[tableName] = { exists: true, error: msg };\r\n          }\r\n        } else {\r\n          results.tables[tableName] = { exists: true, sampleCount: (data && data.length) || 0 };\r\n        }\r\n      } catch (err) {\r\n        results.tables[tableName] = { exists: false, error: err.message };\r\n        results.missingTables.push(tableName);\r\n      }\r\n    }\r\n\r\n    results.schemaMatches = results.missingTables.length === 0;\r\n    return results;\r\n  }\r\n\r\n  async checkTableStructure() {\r\n    const tableDefinitions = {\r\n      projects: ['id', 'name', 'user_id', 'created_at'],\r\n      scraper_templates: ['id', 'project_id', 'name', 'code', 'config', 'status'],\r\n      scraping_jobs: ['id', 'project_id', 'template_id', 'status', 'url', 'config'],\r\n      scraped_data: ['id', 'job_id', 'data', 'url', 'created_at']\r\n    };\r\n\r\n    const structureResults = {};\r\n\r\n    for (const [tableName, expectedColumns] of Object.entries(tableDefinitions)) {\r\n      try {\r\n        const { data, error } = await this.supabase\r\n          .from(tableName)\r\n          .select('*')\r\n          .limit(1);\r\n\r\n        if (error) {\r\n          structureResults[tableName] = { error: error.message };\r\n          continue;\r\n        }\r\n\r\n        if (data && data.length > 0) {\r\n          const actualColumns = Object.keys(data[0]);\r\n          const missingColumns = expectedColumns.filter(col => !actualColumns.includes(col));\r\n          const extraColumns = actualColumns.filter(col => !expectedColumns.includes(col));\r\n\r\n          structureResults[tableName] = {\r\n            exists: true,\r\n            columns: actualColumns,\r\n            missingColumns,\r\n            extraColumns,\r\n            matches: missingColumns.length === 0\r\n          };\r\n        } else {\r\n          structureResults[tableName] = {\r\n            exists: true,\r\n            empty: true,\r\n            matches: true // Can't verify columns on empty table\r\n          };\r\n        }\r\n      } catch (error) {\r\n        structureResults[tableName] = { error: error.message };\r\n      }\r\n    }\r\n\r\n    return structureResults;\r\n  }\r\n\r\n  async generateSchemaReport() {\r\n    console.log('Generating database schema report...');\r\n\r\n    const connectionTest = await this.testConnection();\r\n    if (!connectionTest) {\r\n      return { success: false, error: 'Database connection failed' };\r\n    }\r\n\r\n    const schemaVerification = await this.verifySchema();\r\n    const tableStructure = await this.checkTableStructure();\r\n\r\n    const recordCounts = {};\r\n    const mainTables = ['projects', 'scraper_templates', 'scraping_jobs', 'scraped_data'];\r\n\r\n    for (const table of mainTables) {\r\n      if (schemaVerification.tables[table] && schemaVerification.tables[table].exists) {\r\n        try {\r\n          const { count, error } = await this.supabase\r\n            .from(table)\r\n            .select('*', { count: 'exact', head: true });\r\n\r\n          if (!error) recordCounts[table] = count;\r\n        } catch (e) {\r\n          // ignore\r\n        }\r\n      }\r\n    }\r\n\r\n    const report = {\r\n      timestamp: new Date().toISOString(),\r\n      connection: {\r\n        url: process.env.SUPABASE_URL ? 'Γ£à Set' : 'Γ¥î Missing',\r\n        key: process.env.SUPABASE_ANON_KEY ? 'Γ£à Set' : 'Γ¥î Missing',\r\n        connected: connectionTest\r\n      },\r\n      schema: schemaVerification,\r\n      tableStructure,\r\n      recordCounts,\r\n      recommendations: []\r\n    };\r\n\r\n    if (schemaVerification.missingTables.length > 0) {\r\n      report.recommendations.push({\r\n        type: 'missing_tables',\r\n        severity: 'high',\r\n        message: `Create missing tables: ${schemaVerification.missingTables.join(', ')}`,\r\n        action: 'Run schema setup scripts'\r\n      });\r\n    }\r\n\r\n    // Check for RLS policies by attempting a select; if no error, RLS may not be active\r\n    const { error: rlsError } = await this.supabase\r\n      .from('projects')\r\n      .select('*')\r\n      .limit(1);\r\n\r\n    if (!rlsError) {\r\n      report.recommendations.push({\r\n        type: 'rls_missing',\r\n        severity: 'high',\r\n        message: 'Row Level Security may not be enabled',\r\n        action: 'Enable RLS and create policies'\r\n      });\r\n    }\r\n\r\n    return report;\r\n  }\r\n}\r\n\r\nmodule.exports = DatabaseVerifier;\r\n\r\n// If the script is run directly, execute a verification and print a report\r\nif (require.main === module) {\r\n  (async () => {\r\n    try {\r\n      const verifier = new DatabaseVerifier();\r\n      const report = await verifier.generateSchemaReport();\r\n\r\n      console.log('\\n≡ƒôè DATABASE SCHEMA REPORT');\r\n      console.log('========================\\n');\r\n\r\n      console.log('≡ƒöî Connection:');\r\n      console.log('   URL:', report.connection.url);\r\n      console.log('   Key:', report.connection.key);\r\n      console.log('   Connected:', report.connection.connected ? 'Yes' : 'No');\r\n      console.log('');\r\n\r\n      console.log('≡ƒÅù  Schema:');\r\n      console.log('   Matches expected:', report.schema.schemaMatches ? 'Yes' : 'No');\r\n      if (report.schema.missingTables && report.schema.missingTables.length > 0) {\r\n        console.log('   Missing tables:', report.schema.missingTables.join(', '));\r\n      }\r\n      console.log('');\r\n\r\n      console.log('≡ƒôê Record Counts:');\r\n      Object.entries(report.recordCounts || {}).forEach(([table, count]) => {\r\n        console.log('   ' + table + ': ' + count);\r\n      });\r\n      console.log('');\r\n\r\n      if (report.recommendations && report.recommendations.length > 0) {\r\n        console.log('≡ƒÆí Recommendations:');\r\n        report.recommendations.forEach(rec => {\r\n          console.log('   -', rec.message, '->', rec.action);\r\n        });\r\n      } else {\r\n        console.log('Γ£à No immediate recommendations');\r\n      }\r\n\r\n      process.exit(0);\r\n    } catch (err) {\r\n      console.error('Verification failed:', err && err.message ? err.message : err);\r\n      process.exit(2);\r\n    }\r\n  })();\r\n}\r\n","usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\server.js","messages":[{"ruleId":"no-unused-vars","severity":1,"message":"'cors' is assigned a value but never used.","line":3,"column":7,"nodeType":"Identifier","messageId":"unusedVar","endLine":3,"endColumn":11},{"ruleId":"no-unused-vars","severity":1,"message":"'helmet' is assigned a value but never used.","line":4,"column":7,"nodeType":"Identifier","messageId":"unusedVar","endLine":4,"endColumn":13},{"ruleId":"no-unused-vars","severity":1,"message":"'PlaywrightScraper' is assigned a value but never used.","line":6,"column":9,"nodeType":"Identifier","messageId":"unusedVar","endLine":6,"endColumn":26},{"ruleId":"no-unused-vars","severity":1,"message":"'recordingData' is defined but never used.","line":17,"column":35,"nodeType":"Identifier","messageId":"unusedVar","endLine":17,"endColumn":48},{"ruleId":"no-unused-vars","severity":1,"message":"'authorizeRole' is assigned a value but never used.","line":41,"column":75,"nodeType":"Identifier","messageId":"unusedVar","endLine":41,"endColumn":88},{"ruleId":"no-unused-vars","severity":1,"message":"'logger' is assigned a value but never used.","line":45,"column":7,"nodeType":"Identifier","messageId":"unusedVar","endLine":45,"endColumn":13},{"ruleId":"no-unused-vars","severity":1,"message":"'next' is defined but never used.","line":565,"column":25,"nodeType":"Identifier","messageId":"unusedVar","endLine":565,"endColumn":29}],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":7,"fixableErrorCount":0,"fixableWarningCount":0,"source":"∩╗┐// APL AI Scraper 2.0 - Main Server\nconst express = require('express');\nconst cors = require('cors');\nconst helmet = require('helmet');\nconst { createClient } = require('@supabase/supabase-js');\nconst { PlaywrightScraper } = require('./scrapers/playwright-scraper');\nconst { AIService } = require('./services/ai-service');\nconst { JobQueue } = require('./services/job-queue');\nlet VisualAnalysisEngine;\ntry {\n  // attempt to load the visual analysis engine (may depend on native libs like sharp)\n  VisualAnalysisEngine = require('./services/visual-analysis-engine').VisualAnalysisEngine;\n} catch (err) {\n  // Provide a lightweight stub so tests and environments without native deps won't fail\n  VisualAnalysisEngine = class {\n    constructor() { this.isInitialized = false; }\n    async analyzeRecordingSession(recordingData) {\n      // return a minimal analysis shape expected by downstream code\n      return {\n        interactiveElements: [],\n        dataFields: [],\n        patterns: [],\n        confidence: 0\n      };\n    }\n  };\n  console.warn('VisualAnalysisEngine not available; using stub. Error:', err.message);\n}\nconst { CodeGenerator } = require('./services/code-generator');\nconst { ScraperTemplate } = require('./models/scraper-template');\nconst { DistributedOrchestrator } = require('./services/distributed-orchestrator');\nconst { ProxyManager } = require('./services/proxy-manager');\nconst { CaptchaHandler } = require('./services/captcha-handler');\nconst { DataProcessor } = require('./services/data-processor');\n\nrequire('dotenv').config();\n\nconst app = express();\n\n// Security middleware (helmet/CORS/rate-limiting provided by our middleware module)\nconst { securityHeaders, corsMiddleware, createRateLimiter, authenticate, authorizeRole } = require('./middleware/security-middleware');\nconst { AuthService } = require('./services/auth/auth-service');\nconst { ComplianceManager } = require('./services/compliance-manager');\nconst { PrivacyManager } = require('./services/privacy-manager');\nconst logger = require('./services/core/logger');\n\napp.use(securityHeaders());\napp.use(corsMiddleware());\napp.use(createRateLimiter());\napp.use(express.json({ limit: '10mb' }));\napp.use(express.urlencoded({ extended: true, limit: '10mb' }));\n\n// Initialize services\nlet supabase;\n// Initialize Supabase client defensively. If SUPABASE_URL is missing, empty,\n// a placeholder, or createClient throws, fall back to the in-repo test stub so\n// integration tests remain hermetic on local environments.\ntry {\n  const url = process.env.SUPABASE_URL;\n  if (!url || url === 'your_supabase_url_here' || url.trim() === '') {\n    supabase = require('./services/core/supabase').supabase;\n  } else {\n    try {\n      supabase = createClient(url, process.env.SUPABASE_ANON_KEY);\n    } catch (e) {\n      console.warn('Supabase client initialization failed, using local stub. Error:', e.message);\n      supabase = require('./services/core/supabase').supabase;\n    }\n  }\n} catch (e) {\n  // Extremely defensive: ensure tests don't fail when loading server module\n  console.warn('Error during supabase initialization, using local stub:', e && e.message);\n  supabase = require('./services/core/supabase').supabase;\n}\n\nconst aiService = new AIService();\nconst jobQueue = new JobQueue(supabase);\nconst visualAnalysisEngine = new VisualAnalysisEngine();\nconst codeGenerator = new CodeGenerator();\nconst scraperTemplate = new ScraperTemplate(supabase);\nconst distributedOrchestrator = new DistributedOrchestrator();\nconst proxyManager = new ProxyManager();\nconst captchaHandler = new CaptchaHandler();\nconst dataProcessor = new DataProcessor();\n\n// Security/Compliance services\nconst authService = new AuthService();\nconst complianceManager = new ComplianceManager();\nconst privacyManager = new PrivacyManager();\n\n// Health check endpoint\napp.get('/health', (req, res) => {\n  res.json({ \n    status: 'healthy', \n    timestamp: new Date().toISOString(),\n    version: '1.0.0'\n  });\n});\n\n// Request context middleware - attach services for route handlers\napp.use((req, res, next) => {\n  req.services = {\n    supabase,\n    ai: aiService,\n    jobQueue,\n    visualAnalysisEngine,\n    codeGenerator,\n    orchestrator: distributedOrchestrator,\n    proxyManager,\n    captchaHandler,\n    dataProcessor,\n    auth: authService,\n    compliance: complianceManager,\n    privacy: privacyManager\n  };\n  next();\n});\n\n// Public routes (no authentication required)\napp.use('/api/auth', require('./routes/auth'));\napp.use('/api/public', require('./routes/public'));\n\n// Compliance check for all scraping requests\napp.use('/api/scrape', async (req, res, next) => {\n  try {\n    const url = (req.body && req.body.url) || (req.query && req.query.url);\n    const compliance = await complianceManager.checkCompliance(url, 'AI-Scraper-Service');\n    if (!compliance.allowed) {\n      await complianceManager.logComplianceEvent({ project_id: (req.body && req.body.projectId) || null, domain: new URL(url).hostname, url, allowed: false, reason: compliance.reason, timestamp: new Date().toISOString() });\n      return res.status(429).json({ error: 'Compliance violation', reason: compliance.reason, retryAfter: compliance.retryAfter });\n    }\n\n    if (compliance.crawlDelay) await complianceManager.respectCrawlDelay(new URL(url).hostname, compliance.crawlDelay);\n\n    next();\n  } catch (error) {\n    next(error);\n  }\n});\n\n// Projects endpoints\napp.post('/api/projects', authenticate, async (req, res) => {\n  try {\n    const { name, description, user_id } = req.body;\n    \n    if (!name || !user_id) {\n      return res.status(400).json({ error: 'Name and user_id are required' });\n    }\n\n    const { data, error } = await supabase\n      .from('projects')\n      .insert([{ name, description, user_id }])\n      .select()\n      .single();\n\n    if (error) throw error;\n    res.json(data);\n  } catch (error) {\n    console.error('Error creating project:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\napp.get('/api/projects', authenticate, async (req, res) => {\n  try {\n    const { user_id } = req.query;\n    \n    if (!user_id) {\n      return res.status(400).json({ error: 'user_id is required' });\n    }\n\n    const { data, error } = await supabase\n      .from('projects')\n      .select('*')\n      .eq('user_id', user_id)\n      .order('created_at', { ascending: false });\n\n    if (error) throw error;\n    res.json(data);\n  } catch (error) {\n    console.error('Error fetching projects:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\napp.get('/api/projects/:id', authenticate, async (req, res) => {\n  try {\n    const { id } = req.params;\n    const { data, error } = await supabase\n      .from('projects')\n      .select('*')\n      .eq('id', id)\n      .single();\n\n    if (error) throw error;\n    res.json(data);\n  } catch (error) {\n    console.error('Error fetching project:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\n// Scraping jobs endpoints\napp.post('/api/jobs', async (req, res) => {\n  try {\n    const { project_id, url, config } = req.body;\n    \n    if (!project_id || !url) {\n      return res.status(400).json({ error: 'project_id and url are required' });\n    }\n\n    const { data, error } = await supabase\n      .from('scraping_jobs')\n      .insert([{ project_id, url, config: config || {} }])\n      .select()\n      .single();\n\n    if (error) throw error;\n    \n    // Add to processing queue\n    await jobQueue.addJob(data.id);\n    \n    res.json(data);\n  } catch (error) {\n    console.error('Error creating job:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\napp.get('/api/jobs/:id', async (req, res) => {\n  try {\n    const { id } = req.params;\n    const { data, error } = await supabase\n      .from('scraping_jobs')\n      .select(`\n        *,\n        scraped_data(*)\n      `)\n      .eq('id', id)\n      .single();\n\n    if (error) throw error;\n    res.json(data);\n  } catch (error) {\n    console.error('Error fetching job:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\napp.get('/api/projects/:projectId/jobs', async (req, res) => {\n  try {\n    const { projectId } = req.params;\n    const { status, limit = 50, offset = 0 } = req.query;\n\n    let query = supabase\n      .from('scraping_jobs')\n      .select('*')\n      .eq('project_id', projectId)\n      .order('created_at', { ascending: false })\n      .range(offset, offset + limit - 1);\n\n    if (status) {\n      query = query.eq('status', status);\n    }\n\n    const { data, error } = await query;\n\n    if (error) throw error;\n    res.json(data);\n  } catch (error) {\n    console.error('Error fetching jobs:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\n// AI endpoints\napp.post('/api/ai/discover-sites', async (req, res) => {\n  try {\n    const { query } = req.body;\n    \n    if (!query) {\n      return res.status(400).json({ error: 'Query is required' });\n    }\n\n    const sites = await aiService.discoverSitesWithAI(query);\n    res.json(sites);\n  } catch (error) {\n    console.error('Error discovering sites:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\napp.post('/api/ai/analyze-screenshot', async (req, res) => {\n  try {\n    const { imageBase64, prompt } = req.body;\n    \n    if (!imageBase64 || !prompt) {\n      return res.status(400).json({ error: 'Image and prompt are required' });\n    }\n\n    const imageBuffer = Buffer.from(imageBase64, 'base64');\n    const analysis = await aiService.analyzeWithGPT4V(imageBuffer, prompt);\n    res.json({ analysis });\n  } catch (error) {\n    console.error('Error analyzing screenshot:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\n// Training sessions endpoints\napp.post('/api/training-sessions', async (req, res) => {\n  try {\n    const { project_id, recording_data, screenshots, metadata } = req.body;\n    \n    if (!recording_data || !recording_data.actions) {\n      return res.status(400).json({ error: 'Recording data with actions is required' });\n    }\n\n    const screenshotCount = recording_data.screenshots ? recording_data.screenshots.length : 0;\n    console.log(`├░┼╕ΓÇ£┬¥ Creating training session with ${recording_data.actions.length} actions and ${screenshotCount} screenshots`);\n\n    const { data: session, error } = await supabase\n      .from('training_sessions')\n      .insert([{ \n        project_id,\n        recording_data: { \n          actions: recording_data.actions,\n          screenshots: recording_data.screenshots || screenshots || [],\n          metadata: recording_data.metadata || metadata || {}\n        },\n        status: 'analyzing'\n      }])\n      .select()\n      .single();\n\n    if (error) throw error;\n\n    // Start background analysis\n    analyzeTrainingSession(session.id, recording_data);\n\n    res.json(session);\n  } catch (error) {\n    console.error('Error creating training session:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\napp.get('/api/training-sessions/:id', async (req, res) => {\n  try {\n    const { id } = req.params;\n    const { data, error } = await supabase\n      .from('training_sessions')\n      .select('*')\n      .eq('id', id)\n      .single();\n\n    if (error) throw error;\n    res.json(data);\n  } catch (error) {\n    console.error('Error fetching training session:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\napp.get('/api/training-sessions', async (req, res) => {\n  try {\n    const { project_id, status, limit = 50, offset = 0 } = req.query;\n\n    let query = supabase\n      .from('training_sessions')\n      .select('id, project_id, status, created_at, recording_data')\n      .order('created_at', { ascending: false })\n      .range(offset, offset + limit - 1);\n\n    if (project_id) {\n      query = query.eq('project_id', project_id);\n    }\n\n    if (status) {\n      query = query.eq('status', status);\n    }\n\n    const { data, error } = await query;\n\n    if (error) throw error;\n    \n    // Add summary statistics to each session\n    const sessionsWithStats = data.map(session => ({\n      ...session,\n      stats: {\n        actionCount: session.recording_data && session.recording_data.actions ? session.recording_data.actions.length : 0,\n        screenshotCount: session.recording_data && session.recording_data.screenshots ? session.recording_data.screenshots.length : 0,\n        duration: session.recording_data && session.recording_data.metadata ? session.recording_data.metadata.duration || 0 : 0\n      }\n    }));\n\n    res.json(sessionsWithStats);\n  } catch (error) {\n    console.error('Error fetching training sessions:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\napp.post('/api/training-sessions/:id/analyze', async (req, res) => {\n  try {\n    const { id } = req.params;\n    \n    // Get session data\n    const { data: session, error } = await supabase\n      .from('training_sessions')\n      .select('*')\n      .eq('id', id)\n      .single();\n\n    if (error) throw error;\n    \n    if (!session.recording_data) {\n      return res.status(400).json({ error: 'No recording data found' });\n    }\n\n    // Update status to analyzing\n    await supabase\n      .from('training_sessions')\n      .update({ status: 'analyzing' })\n      .eq('id', id);\n\n    // Start analysis\n    analyzeTrainingSession(id, session.recording_data);\n\n    res.json({ message: 'Analysis started', sessionId: id });\n  } catch (error) {\n    console.error('Error starting analysis:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\napp.post('/api/training-sessions/:id/generate-code', async (req, res) => {\n  try {\n    const { id } = req.params;\n    const { options = {} } = req.body;\n\n    // Get session data\n    const { data: session, error } = await supabase\n      .from('training_sessions')\n      .select('*')\n      .eq('id', id)\n      .single();\n\n    if (error) throw error;\n\n    if (session.status !== 'analyzed' && session.status !== 'completed') {\n      return res.status(400).json({ \n        error: 'Session must be analyzed before generating code',\n        currentStatus: session.status\n      });\n    }\n\n    const recordingData = session.recording_data;\n    const analysis = recordingData.analysis;\n\n    if (!analysis) {\n      return res.status(400).json({ error: 'No analysis data found' });\n    }\n\n    console.log(`├░┼╕┬ÅΓÇö├»┬╕┬Å Generating code for session ${id}`);\n\n    const codeResult = await codeGenerator.generateScrapingCode(\n      analysis, \n      recordingData.actions,\n      options\n    );\n\n    // Update session with generated code\n    await supabase\n      .from('training_sessions')\n      .update({ \n        generated_code: codeResult.code,\n        status: 'code_generated',\n        recording_data: {\n          ...recordingData,\n          codeGeneration: {\n            ...codeResult,\n            generatedAt: new Date().toISOString()\n          }\n        }\n      })\n      .eq('id', id);\n\n    res.json({\n      sessionId: id,\n      code: codeResult.code,\n      metadata: codeResult.metadata,\n      validation: codeResult.validation\n    });\n\n  } catch (error) {\n    console.error('Error generating code:', error);\n    \n    // Update session status to reflect error\n    await supabase\n      .from('training_sessions')\n      .update({ \n        status: 'code_generation_failed',\n        recording_data: {\n          ...req.body,\n          error: error.message\n        }\n      })\n      .eq('id', req.params.id);\n\n    res.status(500).json({ error: error.message });\n  }\n});\n\napp.get('/api/training-sessions/:id/code', async (req, res) => {\n  try {\n    const { id } = req.params;\n    const { data: session, error } = await supabase\n      .from('training_sessions')\n      .select('generated_code, status, recording_data')\n      .eq('id', id)\n      .single();\n\n    if (error) throw error;\n\n    if (!session.generated_code) {\n      return res.status(404).json({ error: 'No generated code found for this session' });\n    }\n\n    const codeGeneration = session.recording_data && session.recording_data.codeGeneration ? session.recording_data.codeGeneration : {};\n\n    res.json({\n      sessionId: id,\n      code: session.generated_code,\n      status: session.status,\n      metadata: codeGeneration.metadata,\n      validation: codeGeneration.validation,\n      generatedAt: codeGeneration.generatedAt\n    });\n\n  } catch (error) {\n    console.error('Error fetching generated code:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\napp.delete('/api/training-sessions/:id', async (req, res) => {\n  try {\n    const { id } = req.params;\n    const { error } = await supabase\n      .from('training_sessions')\n      .delete()\n      .eq('id', id);\n\n    if (error) throw error;\n\n    res.json({ message: 'Training session deleted successfully' });\n  } catch (error) {\n    console.error('Error deleting training session:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\n// Error handling middleware\napp.use((err, req, res, next) => {\n  console.error('Unhandled error:', err);\n  res.status(500).json({ error: 'Internal server error' });\n});\n\n// ================================\n// PHASE 3: SCRAPER MANAGEMENT & EXECUTION\n// ================================\n\n// Scraper Templates API\napp.post('/api/templates', async (req, res) => {\n  try {\n    const { project_id, name, description, code, config } = req.body;\n\n    if (!project_id || !name || !code) {\n      return res.status(400).json({ \n        error: 'project_id, name, and code are required' \n      });\n    }\n\n    const template = await scraperTemplate.createTemplate(project_id, {\n      name,\n      description,\n      code,\n      config: config || {}\n    });\n\n    res.json(template);\n  } catch (error) {\n    console.error('Error creating template:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\napp.get('/api/templates', async (req, res) => {\n  try {\n    const { project_id, status, limit } = req.query;\n    \n    const templates = await scraperTemplate.listTemplates(project_id, {\n      status,\n      limit: parseInt(limit) || undefined\n    });\n\n    res.json(templates);\n  } catch (error) {\n    console.error('Error listing templates:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\napp.get('/api/templates/:id', async (req, res) => {\n  try {\n    const { id } = req.params;\n    const template = await scraperTemplate.getTemplate(id);\n    \n    if (!template) {\n      return res.status(404).json({ error: 'Template not found' });\n    }\n\n    res.json(template);\n  } catch (error) {\n    console.error('Error fetching template:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\napp.put('/api/templates/:id', async (req, res) => {\n  try {\n    const { id } = req.params;\n    const updates = req.body;\n\n    const updatedTemplate = await scraperTemplate.updateTemplate(id, updates);\n    res.json(updatedTemplate);\n  } catch (error) {\n    console.error('Error updating template:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\napp.delete('/api/templates/:id', async (req, res) => {\n  try {\n    const { id } = req.params;\n    await scraperTemplate.deleteTemplate(id);\n    res.json({ message: 'Template deleted successfully' });\n  } catch (error) {\n    console.error('Error deleting template:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\napp.post('/api/templates/:id/clone', async (req, res) => {\n  try {\n    const { id } = req.params;\n    const { name, project_id } = req.body;\n\n    if (!name) {\n      return res.status(400).json({ error: 'name is required for cloning' });\n    }\n\n    const clonedTemplate = await scraperTemplate.cloneTemplate(id, name, project_id);\n    res.json(clonedTemplate);\n  } catch (error) {\n    console.error('Error cloning template:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\n// Distributed Execution API\napp.post('/api/execution/schedule', async (req, res) => {\n  try {\n    const { template_id, urls, options = {} } = req.body;\n\n    if (!template_id || !urls || !Array.isArray(urls)) {\n      return res.status(400).json({ \n        error: 'template_id and urls array are required' \n      });\n    }\n\n    // Get template\n    const template = await scraperTemplate.getTemplate(template_id);\n    if (!template) {\n      return res.status(404).json({ error: 'Template not found' });\n    }\n\n    // Initialize orchestrator if needed\n    if (!distributedOrchestrator.isInitialized) {\n      await distributedOrchestrator.initialize();\n    }\n\n    // Schedule jobs\n    const result = await distributedOrchestrator.scheduleJob(template, urls, options);\n    \n    res.json({\n      message: `Scheduled ${result.jobs.length} scraping jobs`,\n      batchId: result.batchId,\n      jobs: result.jobs,\n      queueName: result.queueName\n    });\n\n  } catch (error) {\n    console.error('Error scheduling execution:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\napp.get('/api/execution/queue/stats', async (req, res) => {\n  try {\n    const stats = await distributedOrchestrator.getQueueStats();\n    res.json(stats);\n  } catch (error) {\n    console.error('Error getting queue stats:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\napp.post('/api/execution/workers/start', async (req, res) => {\n  try {\n    const { configs } = req.body;\n    await distributedOrchestrator.startWorkers(configs);\n    res.json({ message: 'Workers started successfully' });\n  } catch (error) {\n    console.error('Error starting workers:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\napp.post('/api/execution/queue/:name/pause', async (req, res) => {\n  try {\n    const { name } = req.params;\n    await distributedOrchestrator.pauseQueue(name);\n    res.json({ message: `Queue ${name} paused` });\n  } catch (error) {\n    console.error('Error pausing queue:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\napp.post('/api/execution/queue/:name/resume', async (req, res) => {\n  try {\n    const { name } = req.params;\n    await distributedOrchestrator.resumeQueue(name);\n    res.json({ message: `Queue ${name} resumed` });\n  } catch (error) {\n    console.error('Error resuming queue:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\n// Scraping Executions API\napp.get('/api/executions', async (req, res) => {\n  try {\n    const { template_id, status, limit = 50, offset = 0 } = req.query;\n\n    let query = supabase\n      .from('scraping_executions')\n      .select(`\n        *,\n        scraper_templates(name, version)\n      `)\n      .order('created_at', { ascending: false })\n      .range(offset, offset + limit - 1);\n\n    if (template_id) {\n      query = query.eq('template_id', template_id);\n    }\n\n    if (status) {\n      query = query.eq('status', status);\n    }\n\n    const { data, error } = await query;\n\n    if (error) throw error;\n    res.json(data);\n  } catch (error) {\n    console.error('Error fetching executions:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\napp.get('/api/executions/:id', async (req, res) => {\n  try {\n    const { id } = req.params;\n    \n    const { data, error } = await supabase\n      .from('scraping_executions')\n      .select(`\n        *,\n        scraper_templates(name, version, config),\n        captcha_logs(*)\n      `)\n      .eq('id', id)\n      .single();\n\n    if (error) throw error;\n    \n    if (!data) {\n      return res.status(404).json({ error: 'Execution not found' });\n    }\n\n    res.json(data);\n  } catch (error) {\n    console.error('Error fetching execution:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\n// Proxy Management API\napp.get('/api/proxies', async (req, res) => {\n  try {\n    const { data, error } = await supabase\n      .from('proxy_list')\n      .select('*')\n      .order('success_rate', { ascending: false });\n\n    if (error) throw error;\n    res.json(data);\n  } catch (error) {\n    console.error('Error fetching proxies:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\napp.post('/api/proxies', async (req, res) => {\n  try {\n    const proxyData = req.body;\n    const proxy = await proxyManager.addProxy(proxyData);\n    res.json(proxy);\n  } catch (error) {\n    console.error('Error adding proxy:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\napp.delete('/api/proxies/:id', async (req, res) => {\n  try {\n    const { id } = req.params;\n    await proxyManager.removeProxy(id);\n    res.json({ message: 'Proxy removed successfully' });\n  } catch (error) {\n    console.error('Error removing proxy:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\napp.get('/api/proxies/stats', async (req, res) => {\n  try {\n    const stats = await proxyManager.getProxyStats();\n    res.json(stats);\n  } catch (error) {\n    console.error('Error getting proxy stats:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\napp.post('/api/proxies/refresh', async (req, res) => {\n  try {\n    const count = await proxyManager.refreshProxyList();\n    res.json({ message: `Refreshed ${count} proxies` });\n  } catch (error) {\n    console.error('Error refreshing proxies:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\n// Data Schemas API\napp.get('/api/schemas', async (req, res) => {\n  try {\n    const { project_id } = req.query;\n\n    let query = supabase\n      .from('data_schemas')\n      .select('*')\n      .order('created_at', { ascending: false });\n\n    if (project_id) {\n      query = query.eq('project_id', project_id);\n    }\n\n    const { data, error } = await query;\n\n    if (error) throw error;\n    res.json(data);\n  } catch (error) {\n    console.error('Error fetching schemas:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\napp.post('/api/schemas', async (req, res) => {\n  try {\n    const { project_id, name, schema_def, validation_rules, transformation_rules } = req.body;\n\n    if (!project_id || !name || !schema_def) {\n      return res.status(400).json({ \n        error: 'project_id, name, and schema_def are required' \n      });\n    }\n\n    const { data, error } = await supabase\n      .from('data_schemas')\n      .insert([{\n        project_id,\n        name,\n        schema_def,\n        validation_rules: validation_rules || {},\n        transformation_rules: transformation_rules || {}\n      }])\n      .select()\n      .single();\n\n    if (error) throw error;\n    res.json(data);\n  } catch (error) {\n    console.error('Error creating schema:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\napp.get('/api/schemas/:id', async (req, res) => {\n  try {\n    const { id } = req.params;\n    \n    const { data, error } = await supabase\n      .from('data_schemas')\n      .select('*')\n      .eq('id', id)\n      .single();\n\n    if (error) throw error;\n    \n    if (!data) {\n      return res.status(404).json({ error: 'Schema not found' });\n    }\n\n    res.json(data);\n  } catch (error) {\n    console.error('Error fetching schema:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\n// Data Processing API\napp.post('/api/data/process', async (req, res) => {\n  try {\n    const { raw_data, schema_id, options = {} } = req.body;\n\n    if (!raw_data || !schema_id) {\n      return res.status(400).json({ \n        error: 'raw_data and schema_id are required' \n      });\n    }\n\n    // Get schema\n    const { data: schema, error: schemaError } = await supabase\n      .from('data_schemas')\n      .select('*')\n      .eq('id', schema_id)\n      .single();\n\n    if (schemaError) throw schemaError;\n\n    if (!schema) {\n      return res.status(404).json({ error: 'Schema not found' });\n    }\n\n    // Process data\n    const result = await dataProcessor.processScrapedData(raw_data, schema.schema_def, options);\n\n    res.json({\n      processed_data: result.data,\n      quality_metrics: result.qualityMetrics,\n      errors: result.errors,\n      warnings: result.warnings,\n      transformations: result.transformations,\n      valid: result.valid,\n      processing_time: result.processingTime\n    });\n\n  } catch (error) {\n    console.error('Error processing data:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\n// CAPTCHA Analytics API\napp.get('/api/analytics/captcha', async (req, res) => {\n  try {\n    const { template_id, days = 7 } = req.query;\n    const stats = await captchaHandler.getCaptchaStats(template_id, parseInt(days));\n    res.json(stats);\n  } catch (error) {\n    console.error('Error getting CAPTCHA analytics:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\n// Template Analytics API\napp.get('/api/analytics/templates/:id', async (req, res) => {\n  try {\n    const { id } = req.params;\n    const { days = 30 } = req.query;\n\n    // Get template metrics\n    const { data: metrics, error: metricsError } = await supabase\n      .from('template_metrics')\n      .select('*')\n      .eq('template_id', id)\n      .single();\n\n    if (metricsError && metricsError.code !== 'PGRST116') {\n      throw metricsError;\n    }\n\n    // Get recent executions\n    const { data: executions, error: executionsError } = await supabase\n      .from('scraping_executions')\n      .select('status, execution_duration_ms, created_at')\n      .eq('template_id', id)\n      .gte('created_at', new Date(Date.now() - days * 24 * 60 * 60 * 1000).toISOString())\n      .order('created_at', { ascending: false });\n\n    if (executionsError) throw executionsError;\n\n    // Get quality metrics\n    const { data: qualityMetrics, error: qualityError } = await supabase\n      .from('data_quality_metrics')\n      .select('*')\n      .eq('template_id', id)\n      .gte('execution_date', new Date(Date.now() - days * 24 * 60 * 60 * 1000).toISOString().split('T')[0])\n      .order('execution_date', { ascending: false });\n\n    if (qualityError) throw qualityError;\n\n    res.json({\n      template_id: id,\n      metrics: metrics || {},\n      recent_executions: executions || [],\n      quality_metrics: qualityMetrics || [],\n      period_days: days\n    });\n\n  } catch (error) {\n    console.error('Error getting template analytics:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\n// Real-time activity feed\napp.get('/api/activity', async (req, res) => {\n  try {\n    const { limit = 50, offset = 0, type } = req.query;\n\n    // Create activity entries from multiple sources\n    const activities = [];\n\n    // Get recent executions\n    const { data: executions } = await supabase\n      .from('scraping_executions')\n      .select('id, status, created_at, template_name, records_scraped, error_message')\n      .order('created_at', { ascending: false })\n      .limit(20);\n\n    if (executions && executions.forEach) {\n      executions.forEach(exec => {\n        activities.push({\n          id: `exec-${exec.id}`,\n          type: 'execution',\n          title: `Scraping job ${exec.status}`,\n          description: `Template: ${exec.template_name} ΓÇó ${exec.records_scraped || 0} records`,\n          status: exec.status,\n          timestamp: exec.created_at,\n          metadata: {\n            execution_id: exec.id,\n            template_name: exec.template_name,\n            records_scraped: exec.records_scraped,\n            error_message: exec.error_message\n          }\n        });\n      });\n    }\n\n    // Get recent training sessions\n    const { data: trainingSessions } = await supabase\n      .from('training_sessions')\n      .select('id, status, created_at, recording_data')\n      .order('created_at', { ascending: false })\n      .limit(10);\n\n    if (trainingSessions && trainingSessions.forEach) {\n      trainingSessions.forEach(session => {\n        const sessionName = (session.recording_data && session.recording_data.metadata && session.recording_data.metadata.sessionName) || 'Training Session';\n        const interactionCount = session.recording_data && session.recording_data.actions ? session.recording_data.actions.length : 0;\n        \n        activities.push({\n          id: `training-${session.id}`,\n          type: 'training',\n          title: `Training session ${session.status}`,\n          description: `${sessionName} ├óΓé¼┬ó ${interactionCount} interactions`,\n          status: session.status,\n          timestamp: session.created_at,\n          metadata: {\n            session_id: session.id,\n            interaction_count: interactionCount\n          }\n        });\n      });\n    }\n\n    // Get recent template changes\n    const { data: templates } = await supabase\n      .from('scraper_templates')\n      .select('id, name, created_at, updated_at, status')\n      .order('updated_at', { ascending: false })\n      .limit(10);\n\n    if (templates && templates.forEach) {\n      templates.forEach(template => {\n        const isNew = new Date(template.created_at).getTime() === new Date(template.updated_at).getTime();\n        activities.push({\n          id: `template-${template.id}`,\n          type: 'template',\n          title: isNew ? 'New template created' : 'Template updated',\n          description: template.name,\n          status: template.status,\n          timestamp: template.updated_at,\n          metadata: {\n            template_id: template.id,\n            template_name: template.name,\n            is_new: isNew\n          }\n        });\n      });\n    }\n\n    // Sort all activities by timestamp\n    activities.sort((a, b) => new Date(b.timestamp) - new Date(a.timestamp));\n\n    // Apply filtering\n    let filteredActivities = activities;\n    if (type) {\n      filteredActivities = activities.filter(activity => activity.type === type);\n    }\n\n    // Apply pagination\n    const paginatedActivities = filteredActivities.slice(offset, offset + parseInt(limit));\n\n    res.json({\n      activities: paginatedActivities,\n      total: filteredActivities.length,\n      limit: parseInt(limit),\n      offset: parseInt(offset)\n    });\n\n  } catch (error) {\n    console.error('Error fetching activity feed:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\n// System alerts\napp.get('/api/alerts', async (req, res) => {\n  try {\n    const { resolved = false, severity, limit = 20, offset = 0 } = req.query;\n\n    // Get system alerts from database (if table exists)\n    let systemAlerts = [];\n    try {\n      const { data, error } = await supabase\n        .from('system_alerts')\n        .select('*')\n        .eq('resolved', resolved === 'true')\n        .order('created_at', { ascending: false })\n        .limit(parseInt(limit));\n\n      if (!error) {\n        systemAlerts = data || [];\n      }\n    } catch (dbError) {\n      console.log('System alerts table not found, generating synthetic alerts');\n    }\n\n    // Generate alerts from system analysis\n    const generatedAlerts = [];\n\n    // Check for recent job failures\n    const { data: recentFailures } = await supabase\n      .from('scraping_executions')\n      .select('id, error_message, created_at, template_name')\n      .eq('status', 'failed')\n      .gte('created_at', new Date(Date.now() - 24 * 60 * 60 * 1000).toISOString())\n      .limit(5);\n\n    recentFailures?.forEach(failure => {\n      generatedAlerts.push({\n        id: `failure-${failure.id}`,\n        type: 'error',\n        severity: 'high',\n        title: 'Scraping Job Failed',\n        message: `Template \"${failure.template_name}\": ${failure.error_message}`,\n        created_at: failure.created_at,\n        resolved: false,\n        source: 'Scraper',\n        metadata: {\n          execution_id: failure.id,\n          template_name: failure.template_name\n        }\n      });\n    });\n\n    // Check proxy health\n    const { data: proxies } = await supabase\n      .from('proxy_list')\n      .select('id, url, status, success_rate, last_used');\n\n    const failedProxies = proxies?.filter(p => \n      p.status === 'failed' || p.success_rate < 0.5\n    ) || [];\n\n    if (failedProxies.length > 0) {\n      generatedAlerts.push({\n        id: 'proxy-health',\n        type: 'warning',\n        severity: 'medium',\n        title: 'Proxy Health Issues',\n        message: `${failedProxies.length} proxies are failing or have low success rates`,\n        created_at: new Date().toISOString(),\n        resolved: false,\n        source: 'Proxy Manager',\n        metadata: {\n          failed_proxies: failedProxies.length,\n          proxy_details: failedProxies.map(p => ({\n            url: p.url,\n            success_rate: p.success_rate\n          }))\n        }\n      });\n    }\n\n    // Check queue backlog\n    try {\n      const queueStats = await distributedOrchestrator.getQueueStats();\n      const totalPending = Object.values(queueStats).reduce((acc, queue) => acc + (queue.waiting || 0), 0);\n      \n      if (totalPending > 100) {\n        generatedAlerts.push({\n          id: 'queue-backlog',\n          type: 'warning',\n          severity: 'medium',\n          title: 'Job Queue Backlog',\n          message: `${totalPending} jobs are pending execution`,\n          created_at: new Date().toISOString(),\n          resolved: false,\n          source: 'Job Queue',\n          metadata: {\n            pending_jobs: totalPending,\n            queue_stats: queueStats\n          }\n        });\n      }\n    } catch (queueError) {\n      console.log('Could not get queue stats for alerts:', queueError.message);\n    }\n\n    // Combine system and generated alerts\n    const allAlerts = [...systemAlerts, ...generatedAlerts];\n\n    // Apply severity filter\n    let filteredAlerts = allAlerts;\n    if (severity) {\n      filteredAlerts = allAlerts.filter(alert => alert.severity === severity);\n    }\n\n    // Sort by creation date\n    filteredAlerts.sort((a, b) => new Date(b.created_at) - new Date(a.created_at));\n\n    // Apply pagination\n    const paginatedAlerts = filteredAlerts.slice(offset, offset + parseInt(limit));\n\n    res.json({\n      alerts: paginatedAlerts,\n      total: filteredAlerts.length,\n      limit: parseInt(limit),\n      offset: parseInt(offset),\n      summary: {\n        critical: filteredAlerts.filter(a => a.severity === 'critical').length,\n        high: filteredAlerts.filter(a => a.severity === 'high').length,\n        medium: filteredAlerts.filter(a => a.severity === 'medium').length,\n        low: filteredAlerts.filter(a => a.severity === 'low').length\n      }\n    });\n\n  } catch (error) {\n    console.error('Error fetching alerts:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\n// Dashboard metrics endpoint\napp.get('/api/dashboard/metrics', async (req, res) => {\n  try {\n    const { days = 1 } = req.query;\n    const startDate = new Date(Date.now() - days * 24 * 60 * 60 * 1000).toISOString();\n\n    // Get execution metrics\n    const { data: executions } = await supabase\n      .from('scraping_executions')\n      .select('status, execution_time_ms, records_scraped, created_at')\n      .gte('created_at', startDate);\n\n    const totalJobs = executions ? executions.length : 0;\n    const completedJobs = executions ? (executions.filter(e => e.status === 'completed').length) : 0;\n    const failedJobs = executions ? (executions.filter(e => e.status === 'failed').length) : 0;\n    const runningJobs = executions ? (executions.filter(e => e.status === 'running').length) : 0;\n\n    const successRate = totalJobs > 0 ? completedJobs / totalJobs : 0;\n    \n    const avgResponseTime = executions?.length > 0 \n      ? executions.reduce((acc, e) => acc + (e.execution_time_ms || 0), 0) / executions.length / 1000\n      : 0;\n\n    const jobsPerHour = totalJobs / (days * 24);\n    const totalRecords = executions?.reduce((acc, e) => acc + (e.records_scraped || 0), 0) || 0;\n\n    // Get template and proxy counts\n    const { data: templates } = await supabase\n      .from('scraper_templates')\n      .select('status');\n\n    const { data: proxies } = await supabase\n      .from('proxy_list')\n      .select('status');\n\n    const activeTemplates = templates?.filter(t => t.status === 'active').length || 0;\n    const totalProxies = proxies?.length || 0;\n    const activeProxies = proxies?.filter(p => p.status === 'active').length || 0;\n\n    res.json({\n      period: {\n        days: parseInt(days),\n        start_date: startDate,\n        end_date: new Date().toISOString()\n      },\n      execution_metrics: {\n        total_jobs: totalJobs,\n        completed_jobs: completedJobs,\n        failed_jobs: failedJobs,\n        running_jobs: runningJobs,\n        success_rate: successRate,\n        avg_response_time: avgResponseTime,\n        jobs_per_hour: jobsPerHour,\n        total_records: totalRecords\n      },\n      system_metrics: {\n        active_templates: activeTemplates,\n        total_templates: templates?.length || 0,\n        total_proxies: totalProxies,\n        active_proxies: activeProxies,\n        proxy_success_rate: totalProxies > 0 ? activeProxies / totalProxies : 0\n      },\n      timestamp: new Date().toISOString()\n    });\n\n  } catch (error) {\n    console.error('Error fetching dashboard metrics:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\n// System Status API\napp.get('/api/system/status', async (req, res) => {\n  try {\n    const queueStats = await distributedOrchestrator.getQueueStats();\n    const proxyStats = await proxyManager.getProxyStats();\n    \n    // Get recent execution stats\n    const { data: recentExecutions } = await supabase\n      .from('scraping_executions')\n      .select('status')\n      .gte('created_at', new Date(Date.now() - 24 * 60 * 60 * 1000).toISOString());\n\n    const executionStats = {\n      total: recentExecutions?.length || 0,\n      completed: recentExecutions?.filter(e => e.status === 'completed').length || 0,\n      failed: recentExecutions?.filter(e => e.status === 'failed').length || 0,\n      running: recentExecutions?.filter(e => e.status === 'running').length || 0\n    };\n\n    res.json({\n      timestamp: new Date().toISOString(),\n      queues: queueStats,\n      proxies: proxyStats,\n      executions_24h: executionStats,\n      services: {\n        orchestrator: distributedOrchestrator.isInitialized,\n        proxy_manager: proxyManager.isInitialized,\n        captcha_handler: true,\n        data_processor: true\n      }\n    });\n\n  } catch (error) {\n    console.error('Error getting system status:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\n// Background analysis function\nasync function analyzeTrainingSession(sessionId, recordingData) {\n  try {\n    console.log(`├░┼╕ΓÇ¥┬ì Starting analysis for session ${sessionId}`);\n\n    // Analyze with visual analysis engine\n    const analysis = await visualAnalysisEngine.analyzeRecordingSession(recordingData);\n\n    // Update session with analysis results\n    await supabase\n      .from('training_sessions')\n      .update({\n        status: 'analyzed',\n        recording_data: {\n          ...recordingData,\n          analysis: analysis,\n          analyzedAt: new Date().toISOString()\n        }\n      })\n      .eq('id', sessionId);\n\n    console.log(`├ó┼ôΓÇª Analysis completed for session ${sessionId}`);\n\n    // Log analysis summary\n    const summary = {\n      interactiveElements: analysis.interactiveElements?.length || 0,\n      dataFields: analysis.dataFields?.length || 0,\n      patterns: analysis.patterns?.length || 0,\n      confidence: analysis.confidence\n    };\n\n    console.log(`├░┼╕ΓÇ£┼á Analysis summary for session ${sessionId}:`, summary);\n\n  } catch (error) {\n    console.error(`├ó┬¥┼Æ Analysis failed for session ${sessionId}:`, error);\n\n    // Update session to reflect analysis failure\n    await supabase\n      .from('training_sessions')\n      .update({\n        status: 'analysis_failed',\n        recording_data: {\n          ...recordingData,\n          analysisError: error.message,\n          failedAt: new Date().toISOString()\n        }\n      })\n      .eq('id', sessionId);\n  }\n}\n\n// 404 handler\napp.use('*', (req, res) => {\n  res.status(404).json({ error: 'Route not found' });\n});\n\n// Start server only when run directly; export app for tests\nif (require.main === module) {\n  const PORT = process.env.PORT || 3000;\n  app.listen(PORT, () => {\n    console.log(`├░┼╕┼íΓé¼ APL AI Scraper 2.0 Server running on port ${PORT}`);\n    console.log(`├░┼╕ΓÇ£┼á Health check: http://localhost:${PORT}/health`);\n  });\n}\n\nmodule.exports = app;","usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\services\\advanced\\api-gateway.js","messages":[{"ruleId":"no-unused-vars","severity":1,"message":"'next' is defined but never used.","line":703,"column":33,"nodeType":"Identifier","messageId":"unusedVar","endLine":703,"endColumn":37},{"ruleId":"no-unused-vars","severity":1,"message":"'filters' is defined but never used.","line":714,"column":27,"nodeType":"Identifier","messageId":"unusedVar","endLine":714,"endColumn":34},{"ruleId":"no-unused-vars","severity":1,"message":"'exportJob' is defined but never used.","line":719,"column":28,"nodeType":"Identifier","messageId":"unusedVar","endLine":719,"endColumn":37},{"ruleId":"no-unused-vars","severity":1,"message":"'data' is defined but never used.","line":724,"column":28,"nodeType":"Identifier","messageId":"unusedVar","endLine":724,"endColumn":32},{"ruleId":"no-unused-vars","severity":1,"message":"'format' is defined but never used.","line":724,"column":34,"nodeType":"Identifier","messageId":"unusedVar","endLine":724,"endColumn":40},{"ruleId":"no-unused-vars","severity":1,"message":"'filename' is defined but never used.","line":724,"column":42,"nodeType":"Identifier","messageId":"unusedVar","endLine":724,"endColumn":50}],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":6,"fixableErrorCount":0,"fixableWarningCount":0,"source":"∩╗┐// Third-Party API Access Gateway\nconst express = require('express');\nconst rateLimit = require('express-rate-limit');\nconst jwt = require('jsonwebtoken');\n\nclass ApiGateway {\n  constructor() {\n    this.router = express.Router();\n    this.supabase = null; // Will be injected\n    this.jobQueue = null; // Will be injected\n    this.webhookManager = null; // Will be injected\n    \n    this.setupMiddleware();\n    this.setupRoutes();\n  }\n\n  setDependencies(supabase, jobQueue, webhookManager) {\n    this.supabase = supabase;\n    this.jobQueue = jobQueue;\n    this.webhookManager = webhookManager;\n  }\n\n  setupMiddleware() {\n    // Rate limiting with different tiers\n    const createRateLimiter = (windowMs, max, message) => rateLimit({\n      windowMs,\n      max,\n      message: { error: message },\n      standardHeaders: true,\n      legacyHeaders: false,\n      handler: (req, res) => {\n        res.status(429).json({\n          error: message,\n          reset_time: new Date(Date.now() + windowMs),\n          limit: max\n        });\n      }\n    });\n\n    // Default rate limit\n    this.router.use('/v1/', createRateLimiter(15 * 60 * 1000, 100, 'Rate limit exceeded'));\n    \n    // Stricter limits for resource-intensive operations\n    this.router.use('/v1/scrape', createRateLimiter(60 * 1000, 10, 'Scraping rate limit exceeded'));\n    this.router.use('/v1/export', createRateLimiter(5 * 60 * 1000, 3, 'Export rate limit exceeded'));\n\n    // Authentication middleware\n    this.router.use(this.authenticate.bind(this));\n\n    // Request logging and metrics\n    this.router.use(this.logRequest.bind(this));\n\n    // Request validation\n    this.router.use(this.validateRequest.bind(this));\n  }\n\n  async authenticate(req, res, next) {\n    // Skip auth for health check\n    if (req.path === '/health') {\n      return next();\n    }\n\n    const authHeader = req.headers.authorization;\n    \n    if (!authHeader || !authHeader.startsWith('Bearer ')) {\n      return res.status(401).json({ \n        error: 'Authentication required',\n        documentation: '/docs/authentication'\n      });\n    }\n\n    const token = authHeader.substring(7);\n\n    try {\n      // Verify JWT token\n      const decoded = jwt.verify(token, process.env.JWT_SECRET);\n      \n      // Check if API key is still active\n      const { data: apiKey } = await this.supabase\n        .from('api_keys')\n        .select(`\n          *,\n          users (id, email, subscription_tier)\n        `)\n        .eq('key', decoded.apiKeyId)\n        .eq('active', true)\n        .single();\n\n      if (!apiKey) {\n        return res.status(401).json({ \n          error: 'Invalid API key',\n          documentation: '/docs/authentication'\n        });\n      }\n\n      // Check if API key has expired\n      if (apiKey.expires_at && new Date(apiKey.expires_at) < new Date()) {\n        return res.status(401).json({ \n          error: 'API key has expired',\n          expired_at: apiKey.expires_at\n        });\n      }\n\n      // Update last used timestamp\n      await this.supabase\n        .from('api_keys')\n        .update({ last_used: new Date().toISOString() })\n        .eq('id', apiKey.id);\n\n      req.apiKey = apiKey;\n      req.user = apiKey.users;\n      next();\n    } catch (error) {\n      if (error.name === 'TokenExpiredError') {\n        return res.status(401).json({ \n          error: 'Token has expired',\n          documentation: '/docs/authentication'\n        });\n      }\n      \n      return res.status(401).json({ \n        error: 'Invalid token',\n        documentation: '/docs/authentication'\n      });\n    }\n  }\n\n  logRequest(req, res, next) {\n    const startTime = Date.now();\n    \n    res.on('finish', async () => {\n      try {\n        const duration = Date.now() - startTime;\n        \n        await this.supabase\n          .from('api_requests')\n          .insert([{\n            api_key_id: req.apiKey?.id,\n            method: req.method,\n            path: req.path,\n            query_params: req.query,\n            status_code: res.statusCode,\n            user_agent: req.get('User-Agent'),\n            ip_address: req.ip,\n            duration_ms: duration,\n            request_size: req.get('Content-Length') || 0,\n            response_size: res.get('Content-Length') || 0,\n            timestamp: new Date().toISOString()\n          }]);\n      } catch (error) {\n        console.error('Error logging API request:', error);\n      }\n    });\n\n    next();\n  }\n\n  validateRequest(req, res, next) {\n    // Add request ID for tracking\n    req.requestId = crypto.randomUUID();\n    res.setHeader('X-Request-ID', req.requestId);\n    \n    // Add API version header\n    res.setHeader('X-API-Version', '1.0');\n    \n    next();\n  }\n\n  setupRoutes() {\n    // Health check (no auth required)\n    this.router.get('/health', this.healthCheck.bind(this));\n    \n    // API Info\n    this.router.get('/v1/info', this.getApiInfo.bind(this));\n    \n    // Scraping endpoints\n    this.router.post('/v1/scrape', this.handleScrapeRequest.bind(this));\n    this.router.get('/v1/jobs/:id', this.getJobStatus.bind(this));\n    this.router.get('/v1/jobs', this.listJobs.bind(this));\n    this.router.post('/v1/jobs/:id/cancel', this.cancelJob.bind(this));\n    \n    // Data endpoints\n    this.router.get('/v1/data', this.queryData.bind(this));\n    this.router.post('/v1/export', this.exportData.bind(this));\n    this.router.get('/v1/exports/:id', this.getExportStatus.bind(this));\n    \n    // Template endpoints\n    this.router.get('/v1/templates', this.listTemplates.bind(this));\n    this.router.get('/v1/templates/:id', this.getTemplate.bind(this));\n    this.router.post('/v1/templates', this.createTemplate.bind(this));\n    this.router.put('/v1/templates/:id', this.updateTemplate.bind(this));\n    \n    // Webhook management\n    this.router.get('/v1/webhooks', this.listWebhooks.bind(this));\n    this.router.post('/v1/webhooks', this.createWebhook.bind(this));\n    this.router.put('/v1/webhooks/:id', this.updateWebhook.bind(this));\n    this.router.delete('/v1/webhooks/:id', this.deleteWebhook.bind(this));\n    this.router.post('/v1/webhooks/:id/test', this.testWebhook.bind(this));\n    \n    // Analytics endpoints\n    this.router.get('/v1/analytics/usage', this.getUsageAnalytics.bind(this));\n    this.router.get('/v1/analytics/performance', this.getPerformanceAnalytics.bind(this));\n    \n    // Error handler\n    this.router.use(this.errorHandler.bind(this));\n  }\n\n  healthCheck(req, res) {\n    res.json({\n      status: 'healthy',\n      timestamp: new Date().toISOString(),\n      version: '1.0.0',\n      uptime: process.uptime()\n    });\n  }\n\n  getApiInfo(req, res) {\n    res.json({\n      name: 'APL AI Scraper API',\n      version: '1.0.0',\n      documentation: '/docs',\n      rate_limits: {\n        default: '100 requests per 15 minutes',\n        scraping: '10 requests per minute',\n        exports: '3 requests per 5 minutes'\n      },\n      user: {\n        id: req.user.id,\n        email: req.user.email,\n        tier: req.user.subscription_tier\n      },\n      api_key: {\n        name: req.apiKey.name,\n        scopes: req.apiKey.scopes,\n        rate_limit: req.apiKey.rate_limit,\n        last_used: req.apiKey.last_used\n      }\n    });\n  }\n\n  async handleScrapeRequest(req, res) {\n    try {\n      const { template_id, urls, options = {} } = req.body;\n\n      // Validation\n      if (!template_id || !urls || !Array.isArray(urls) || urls.length === 0) {\n        return res.status(400).json({ \n          error: 'template_id and urls array are required',\n          documentation: '/docs/scraping'\n        });\n      }\n\n      if (urls.length > 100) {\n        return res.status(400).json({ \n          error: 'Maximum 100 URLs per request',\n          received: urls.length\n        });\n      }\n\n      // Validate template access\n      const template = await this.validateTemplateAccess(template_id, req.apiKey.id);\n      if (!template) {\n        return res.status(404).json({ \n          error: 'Template not found or access denied',\n          template_id: template_id\n        });\n      }\n\n      // Check user limits\n      const usageLimits = await this.checkUsageLimits(req.user, urls.length);\n      if (!usageLimits.allowed) {\n        return res.status(429).json({\n          error: 'Usage limit exceeded',\n          limits: usageLimits.limits,\n          current_usage: usageLimits.current_usage\n        });\n      }\n\n      // Create scraping jobs\n      const jobs = [];\n      for (const url of urls) {\n        const { data: job } = await this.supabase\n          .from('scraping_jobs')\n          .insert([{\n            project_id: template.project_id,\n            template_id: template_id,\n            url: url,\n            config: {\n              ...template.config,\n              ...options,\n              api_request: true,\n              api_key_id: req.apiKey.id,\n              request_id: req.requestId\n            },\n            status: 'pending',\n            created_by: 'api',\n            api_key_id: req.apiKey.id\n          }])\n          .select()\n          .single();\n\n        jobs.push(job);\n      }\n\n      // Add to job queue\n      const queuePromises = jobs.map(job => this.jobQueue.addJob(job.id, {\n        priority: req.user.subscription_tier === 'premium' ? 'high' : 'normal'\n      }));\n      \n      await Promise.all(queuePromises);\n\n      res.status(201).json({\n        success: true,\n        message: `${jobs.length} jobs queued for processing`,\n        jobs: jobs.map(job => ({\n          job_id: job.id,\n          url: job.url,\n          status: job.status,\n          created_at: job.created_at\n        })),\n        estimated_completion: this.estimateCompletionTime(jobs.length),\n        documentation: '/docs/job-status'\n      });\n\n    } catch (error) {\n      console.error('Scrape request error:', error);\n      res.status(500).json({ \n        error: 'Internal server error',\n        request_id: req.requestId\n      });\n    }\n  }\n\n  async getJobStatus(req, res) {\n    try {\n      const { id } = req.params;\n\n      const { data: job, error } = await this.supabase\n        .from('scraping_jobs')\n        .select(`\n          *,\n          scraped_data (\n            id,\n            data,\n            created_at\n          )\n        `)\n        .eq('id', id)\n        .eq('api_key_id', req.apiKey.id)\n        .single();\n\n      if (error || !job) {\n        return res.status(404).json({ \n          error: 'Job not found',\n          job_id: id\n        });\n      }\n\n      res.json({\n        job_id: job.id,\n        status: job.status,\n        url: job.url,\n        template_id: job.template_id,\n        created_at: job.created_at,\n        started_at: job.started_at,\n        completed_at: job.completed_at,\n        execution_time: job.execution_duration_ms,\n        records_scraped: job.records_scraped,\n        error_message: job.error_message,\n        data: job.scraped_data?.data,\n        progress: this.calculateJobProgress(job)\n      });\n\n    } catch (error) {\n      console.error('Get job status error:', error);\n      res.status(500).json({ \n        error: 'Internal server error',\n        request_id: req.requestId\n      });\n    }\n  }\n\n  async listJobs(req, res) {\n    try {\n      const { \n        status, \n        template_id,\n        limit = 50, \n        offset = 0,\n        start_date,\n        end_date\n      } = req.query;\n\n      let query = this.supabase\n        .from('scraping_jobs')\n        .select('id, status, url, template_id, created_at, completed_at, records_scraped, error_message')\n        .eq('api_key_id', req.apiKey.id)\n        .order('created_at', { ascending: false })\n        .range(offset, offset + parseInt(limit) - 1);\n\n      if (status) query = query.eq('status', status);\n      if (template_id) query = query.eq('template_id', template_id);\n      if (start_date) query = query.gte('created_at', start_date);\n      if (end_date) query = query.lte('created_at', end_date);\n\n      const { data: jobs, error, count } = await query;\n\n      if (error) throw error;\n\n      res.json({\n        jobs: jobs || [],\n        pagination: {\n          limit: parseInt(limit),\n          offset: parseInt(offset),\n          total: count\n        }\n      });\n\n    } catch (error) {\n      console.error('List jobs error:', error);\n      res.status(500).json({ \n        error: 'Internal server error',\n        request_id: req.requestId\n      });\n    }\n  }\n\n  async queryData(req, res) {\n    try {\n      const { \n        template_id, \n        fields, \n        filters, \n        limit = 100, \n        offset = 0,\n        format = 'json'\n      } = req.query;\n\n      const maxLimit = req.user.subscription_tier === 'premium' ? 1000 : 100;\n      const actualLimit = Math.min(parseInt(limit), maxLimit);\n\n      let query = this.supabase\n        .from('scraped_data')\n        .select(`\n          *,\n          scraping_jobs!inner (\n            url,\n            template_id,\n            created_at,\n            api_key_id\n          )\n        `)\n        .eq('scraping_jobs.api_key_id', req.apiKey.id)\n        .range(offset, offset + actualLimit - 1);\n\n      if (template_id) {\n        query = query.eq('scraping_jobs.template_id', template_id);\n      }\n\n      if (filters) {\n        try {\n          const filterObj = JSON.parse(filters);\n          query = this.applyDataFilters(query, filterObj);\n        } catch (filterError) {\n          return res.status(400).json({\n            error: 'Invalid filters format',\n            expected: 'JSON object',\n            received: filters\n          });\n        }\n      }\n\n      const { data, error, count } = await query;\n      if (error) throw error;\n\n      // Transform data based on requested fields\n      let transformedData = data || [];\n      if (fields) {\n        const fieldList = fields.split(',').map(f => f.trim());\n        transformedData = transformedData.map(item => {\n          const filteredData = {};\n          \n          for (const field of fieldList) {\n            if (field in item.data) {\n              filteredData[field] = item.data[field];\n            }\n          }\n\n          return {\n            id: item.id,\n            job_id: item.job_id,\n            created_at: item.created_at,\n            url: item.scraping_jobs.url,\n            data: filteredData\n          };\n        });\n      }\n\n      res.json({\n        data: transformedData,\n        pagination: {\n          limit: actualLimit,\n          offset: parseInt(offset),\n          total: count\n        },\n        meta: {\n          format: format,\n          fields: fields ? fields.split(',') : null,\n          filters_applied: !!filters\n        }\n      });\n\n    } catch (error) {\n      console.error('Query data error:', error);\n      res.status(500).json({ \n        error: 'Internal server error',\n        request_id: req.requestId\n      });\n    }\n  }\n\n  async exportData(req, res) {\n    try {\n      const { format = 'json', filters, template_id, filename } = req.body;\n\n      const supportedFormats = ['json', 'csv', 'xlsx'];\n      if (!supportedFormats.includes(format)) {\n        return res.status(400).json({\n          error: 'Unsupported export format',\n          supported_formats: supportedFormats\n        });\n      }\n\n      // Create export job\n      const { data: exportJob } = await this.supabase\n        .from('export_jobs')\n        .insert([{\n          format: format,\n          filters: filters || {},\n          template_id: template_id,\n          filename: filename,\n          status: 'processing',\n          api_key_id: req.apiKey.id,\n          user_id: req.user.id,\n          request_id: req.requestId\n        }])\n        .select()\n        .single();\n\n      // Process export in background\n      this.processExport(exportJob);\n\n      res.status(202).json({\n        success: true,\n        export_id: exportJob.id,\n        status: 'processing',\n        message: 'Export job started',\n        check_status_url: `/v1/exports/${exportJob.id}`,\n        estimated_completion: this.estimateExportTime(format)\n      });\n\n    } catch (error) {\n      console.error('Export data error:', error);\n      res.status(500).json({ \n        error: 'Internal server error',\n        request_id: req.requestId\n      });\n    }\n  }\n\n  async processExport(exportJob) {\n    try {\n      console.log(`├░┼╕ΓÇ£┬ñ Processing export job ${exportJob.id}`);\n\n      // Update status to processing\n      await this.supabase\n        .from('export_jobs')\n        .update({ status: 'processing', started_at: new Date().toISOString() })\n        .eq('id', exportJob.id);\n\n      // Query data based on filters\n      const data = await this.fetchDataForExport(exportJob);\n      \n      // Generate export file\n      const exportResult = await this.generateExportFile(data, exportJob.format, exportJob.filename);\n      \n      // Update export job status\n      await this.supabase\n        .from('export_jobs')\n        .update({\n          status: 'completed',\n          record_count: data.length,\n          file_url: exportResult.fileUrl,\n          file_size: exportResult.fileSize,\n          completed_at: new Date().toISOString()\n        })\n        .eq('id', exportJob.id);\n\n      // Trigger webhook if configured\n      if (this.webhookManager) {\n        await this.webhookManager.onDataExported(exportJob, data.length);\n      }\n\n      console.log(`├ó┼ôΓÇª Export job ${exportJob.id} completed: ${data.length} records`);\n\n    } catch (error) {\n      console.error(`├ó┬¥┼Æ Export job ${exportJob.id} failed:`, error);\n      \n      await this.supabase\n        .from('export_jobs')\n        .update({\n          status: 'failed',\n          error_message: error.message,\n          completed_at: new Date().toISOString()\n        })\n        .eq('id', exportJob.id);\n    }\n  }\n\n  async validateTemplateAccess(templateId, apiKeyId) {\n    const { data: template } = await this.supabase\n      .from('scraper_templates')\n      .select(`\n        *,\n        projects!inner (\n          id,\n          api_access_enabled,\n          user_id\n        )\n      `)\n      .eq('id', templateId)\n      .eq('projects.api_access_enabled', true)\n      .single();\n\n    if (!template) return null;\n\n    // Check if API key belongs to template owner or has explicit access\n    const { data: access } = await this.supabase\n      .from('template_api_access')\n      .select('*')\n      .eq('template_id', templateId)\n      .eq('api_key_id', apiKeyId)\n      .single();\n\n    return access || (template.projects.user_id === apiKeyId) ? template : null;\n  }\n\n  async checkUsageLimits(user, requestCount) {\n    const limits = this.getUserLimits(user.subscription_tier);\n    \n    // Get current usage for this month\n    const startOfMonth = new Date();\n    startOfMonth.setDate(1);\n    startOfMonth.setHours(0, 0, 0, 0);\n\n    const { data: usage } = await this.supabase\n      .from('api_requests')\n      .select('*')\n      .eq('api_key_id', user.id)\n      .gte('timestamp', startOfMonth.toISOString());\n\n    const currentUsage = {\n      requests: usage?.length || 0,\n      scrapes: usage?.filter(r => r.path.includes('/scrape')).length || 0\n    };\n\n    return {\n      allowed: currentUsage.scrapes + requestCount <= limits.scrapes_per_month,\n      limits: limits,\n      current_usage: currentUsage\n    };\n  }\n\n  getUserLimits(tier) {\n    const limits = {\n      free: { requests_per_hour: 100, scrapes_per_month: 1000 },\n      basic: { requests_per_hour: 500, scrapes_per_month: 10000 },\n      premium: { requests_per_hour: 2000, scrapes_per_month: 100000 }\n    };\n\n    return limits[tier] || limits.free;\n  }\n\n  calculateJobProgress(job) {\n    if (job.status === 'completed') return 100;\n    if (job.status === 'failed') return 0;\n    if (job.status === 'running') return 50;\n    return 0;\n  }\n\n  estimateCompletionTime(jobCount) {\n    // Rough estimate: 30 seconds per job\n    const seconds = jobCount * 30;\n    return new Date(Date.now() + seconds * 1000).toISOString();\n  }\n\n  estimateExportTime(format) {\n    const estimates = { json: 30, csv: 60, xlsx: 120 };\n    const seconds = estimates[format] || 60;\n    return new Date(Date.now() + seconds * 1000).toISOString();\n  }\n\n  errorHandler(error, req, res, next) {\n    console.error('API Gateway error:', error);\n\n    res.status(500).json({\n      error: 'Internal server error',\n      request_id: req.requestId,\n      timestamp: new Date().toISOString()\n    });\n  }\n\n  // Additional helper methods would be implemented here...\n  applyDataFilters(query, filters) {\n    // Implementation for applying data filters\n    return query;\n  }\n\n  async fetchDataForExport(exportJob) {\n    // Implementation for fetching export data\n    return [];\n  }\n\n  async generateExportFile(data, format, filename) {\n    // Implementation for generating export files\n    return { fileUrl: '', fileSize: 0 };\n  }\n}\n\nmodule.exports = { ApiGateway };","usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\services\\advanced\\cache-manager.js","messages":[{"ruleId":"no-unused-vars","severity":1,"message":"'crypto' is assigned a value but never used.","line":9,"column":7,"nodeType":"Identifier","messageId":"unusedVar","endLine":9,"endColumn":13},{"ruleId":"no-unused-vars","severity":1,"message":"'value' is defined but never used.","line":143,"column":37,"nodeType":"Identifier","messageId":"unusedVar","endLine":143,"endColumn":42},{"ruleId":"no-unused-vars","severity":1,"message":"'value' is defined but never used.","line":148,"column":37,"nodeType":"Identifier","messageId":"unusedVar","endLine":148,"endColumn":42},{"ruleId":"no-unused-vars","severity":1,"message":"'value' is defined but never used.","line":153,"column":41,"nodeType":"Identifier","messageId":"unusedVar","endLine":153,"endColumn":46},{"ruleId":"no-unused-vars","severity":1,"message":"'strategy' is assigned a value but never used.","line":769,"column":17,"nodeType":"Identifier","messageId":"unusedVar","endLine":769,"endColumn":25}],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":5,"fixableErrorCount":0,"fixableWarningCount":0,"source":"∩╗┐/**\n * Cache Manager - Multi-Layer Caching System for APL AI Scraper 2.0\n * Implements local memory, Redis, and database caching with intelligent strategies\n */\n\nconst EventEmitter = require('events');\nconst NodeCache = require('node-cache');\nconst { createClient } = require('redis');\nconst crypto = require('crypto');\nconst logger = require('../core/logger');\nconst { supabase } = require('../core/supabase');\n\nclass CacheManager extends EventEmitter {\n  constructor(options = {}) {\n    super();\n        \n    // Configuration\n    this.localTtl = options.localTtl || 300; // 5 minutes for local cache\n    this.redisTtl = options.redisTtl || 3600; // 1 hour for Redis cache\n    this.dbTtl = options.dbTtl || 86400; // 24 hours for database cache\n    this.maxLocalSize = options.maxLocalSize || 1000; // Max items in local cache\n    this.compressionThreshold = options.compressionThreshold || 10240; // 10KB\n    this.enableCompression = options.enableCompression !== false;\n    this.enableMetrics = options.enableMetrics !== false;\n        \n    // Cache layers\n    this.localCache = new NodeCache({\n      stdTTL: this.localTtl,\n      maxKeys: this.maxLocalSize,\n      useClones: false,\n      deleteOnExpire: true\n    });\n        \n    this.redis = null;\n    this.isRedisConnected = false;\n        \n    // Metrics tracking\n    this.metrics = {\n      hits: { local: 0, redis: 0, database: 0 },\n      misses: { local: 0, redis: 0, database: 0 },\n      sets: { local: 0, redis: 0, database: 0 },\n      deletes: { local: 0, redis: 0, database: 0 },\n      errors: { local: 0, redis: 0, database: 0 }\n    };\n        \n    // Cache strategies\n    this.strategies = {\n      'template': { layers: ['local', 'redis', 'database'], ttl: { local: 600, redis: 3600, db: 86400 } },\n      'scraped_data': { layers: ['redis', 'database'], ttl: { redis: 1800, db: 86400 } },\n      'ai_analysis': { layers: ['local', 'redis'], ttl: { local: 300, redis: 1800 } },\n      'user_session': { layers: ['local', 'redis'], ttl: { local: 300, redis: 1800 } },\n      'api_response': { layers: ['local'], ttl: { local: 60 } },\n      'page_content': { layers: ['redis', 'database'], ttl: { redis: 3600, db: 604800 } }, // 7 days in DB\n      'training_data': { layers: ['database'], ttl: { db: 2592000 } }, // 30 days\n      'export_data': { layers: ['local', 'redis'], ttl: { local: 300, redis: 1800 } }\n    };\n        \n    // Bind methods\n    this.cleanupExpired = this.cleanupExpired.bind(this);\n    this.recordMetrics = this.recordMetrics.bind(this);\n        \n    // Start cleanup interval\n    this.cleanupInterval = setInterval(this.cleanupExpired, 300000); // 5 minutes\n        \n    // Metrics reporting interval\n    if (this.enableMetrics) {\n      this.metricsInterval = setInterval(this.recordMetrics, 60000); // 1 minute\n    }\n  }\n\n  /**\n     * Initialize the cache manager\n     */\n  async initialize() {\n    try {\n      logger.info('Initializing Cache Manager');\n            \n      // Connect to Redis\n      await this.connectToRedis();\n            \n      // Set up event listeners\n      this.setupEventListeners();\n            \n      // Warm up caches if configured\n      await this.warmUpCaches();\n            \n      logger.info('Cache Manager initialized successfully');\n      this.emit('ready');\n\n    } catch (error) {\n      logger.error('Failed to initialize Cache Manager', { error: error.message });\n      throw error;\n    }\n  }\n\n  /**\n     * Connect to Redis\n     */\n  async connectToRedis() {\n    try {\n      this.redis = createClient({\n        url: process.env.REDIS_URL || 'redis://localhost:6379',\n        retry_delay_on_failure: 1000,\n        max_attempts: 3\n      });\n\n      this.redis.on('error', (err) => {\n        logger.error('Redis connection error', { error: err.message });\n        this.isRedisConnected = false;\n        this.emit('redis_error', err);\n      });\n\n      this.redis.on('connect', () => {\n        logger.info('Connected to Redis for caching');\n        this.isRedisConnected = true;\n        this.emit('redis_connected');\n      });\n\n      this.redis.on('ready', () => {\n        logger.info('Redis client ready');\n        this.isRedisConnected = true;\n      });\n\n      this.redis.on('end', () => {\n        logger.warn('Redis connection closed');\n        this.isRedisConnected = false;\n      });\n\n      await this.redis.connect();\n\n    } catch (error) {\n      logger.warn('Failed to connect to Redis, continuing without Redis cache', { \n        error: error.message \n      });\n      this.isRedisConnected = false;\n    }\n  }\n\n  /**\n     * Set up event listeners for cache events\n     */\n  setupEventListeners() {\n    this.localCache.on('set', (key, value) => {\n      this.updateMetrics('sets', 'local');\n      logger.debug('Local cache set', { key: this.maskKey(key) });\n    });\n\n    this.localCache.on('del', (key, value) => {\n      this.updateMetrics('deletes', 'local');\n      logger.debug('Local cache delete', { key: this.maskKey(key) });\n    });\n\n    this.localCache.on('expired', (key, value) => {\n      logger.debug('Local cache expired', { key: this.maskKey(key) });\n    });\n  }\n\n  /**\n     * Get value from cache with multi-layer strategy\n     */\n  async get(key, type = 'default') {\n    const strategy = this.strategies[type] || this.strategies['template'];\n    const layers = strategy.layers;\n\n    logger.debug('Cache get request', { key: this.maskKey(key), type, layers });\n\n    try {\n      // Try each layer in order\n      for (const layer of layers) {\n        const value = await this.getFromLayer(key, layer);\n        if (value !== null && value !== undefined) {\n          this.updateMetrics('hits', layer);\n                    \n          // Backfill previous layers for faster future access\n          await this.backfillLayers(key, value, layers, layer, strategy);\n                    \n          logger.debug('Cache hit', { \n            key: this.maskKey(key), \n            layer, \n            type \n          });\n                    \n          return this.deserializeValue(value);\n        }\n        this.updateMetrics('misses', layer);\n      }\n\n      logger.debug('Cache miss (all layers)', { key: this.maskKey(key), type });\n      return null;\n\n    } catch (error) {\n      logger.error('Cache get error', { \n        error: error.message, \n        key: this.maskKey(key), \n        type \n      });\n      return null;\n    }\n  }\n\n  /**\n     * Set value in cache with multi-layer strategy\n     */\n  async set(key, value, type = 'default', ttlOverride = null) {\n    const strategy = this.strategies[type] || this.strategies['template'];\n    const layers = strategy.layers;\n\n    logger.debug('Cache set request', { \n      key: this.maskKey(key), \n      type, \n      layers,\n      valueSize: this.getValueSize(value)\n    });\n\n    try {\n      const serializedValue = this.serializeValue(value);\n      const promises = [];\n\n      // Set in all configured layers\n      for (const layer of layers) {\n        const ttl = ttlOverride || strategy.ttl[layer] || this.getDefaultTtl(layer);\n        promises.push(this.setInLayer(key, serializedValue, layer, ttl));\n      }\n\n      await Promise.allSettled(promises);\n            \n      logger.debug('Cache set completed', { \n        key: this.maskKey(key), \n        type,\n        layers: layers.length\n      });\n\n      return true;\n\n    } catch (error) {\n      logger.error('Cache set error', { \n        error: error.message, \n        key: this.maskKey(key), \n        type \n      });\n      return false;\n    }\n  }\n\n  /**\n     * Delete value from all cache layers\n     */\n  async delete(key) {\n    logger.debug('Cache delete request', { key: this.maskKey(key) });\n\n    try {\n      const promises = [\n        this.deleteFromLayer(key, 'local'),\n        this.deleteFromLayer(key, 'redis'),\n        this.deleteFromLayer(key, 'database')\n      ];\n\n      await Promise.allSettled(promises);\n            \n      logger.debug('Cache delete completed', { key: this.maskKey(key) });\n      return true;\n\n    } catch (error) {\n      logger.error('Cache delete error', { \n        error: error.message, \n        key: this.maskKey(key) \n      });\n      return false;\n    }\n  }\n\n  /**\n     * Clear cache by pattern\n     */\n  async clear(pattern = null, layers = ['local', 'redis', 'database']) {\n    logger.info('Cache clear request', { pattern, layers });\n\n    try {\n      const promises = [];\n\n      if (layers.includes('local')) {\n        if (pattern) {\n          // Clear by pattern in local cache\n          const keys = this.localCache.keys().filter(key => \n            key.includes(pattern) || key.match(new RegExp(pattern))\n          );\n          keys.forEach(key => this.localCache.del(key));\n        } else {\n          // Clear all local cache\n          this.localCache.flushAll();\n        }\n      }\n\n      if (layers.includes('redis') && this.isRedisConnected) {\n        if (pattern) {\n          // Clear by pattern in Redis\n          promises.push(this.clearRedisPattern(pattern));\n        } else {\n          // Clear all Redis cache\n          promises.push(this.redis.flushDb());\n        }\n      }\n\n      if (layers.includes('database')) {\n        promises.push(this.clearDatabaseCache(pattern));\n      }\n\n      await Promise.allSettled(promises);\n            \n      logger.info('Cache clear completed', { pattern, layers });\n      return true;\n\n    } catch (error) {\n      logger.error('Cache clear error', { \n        error: error.message, \n        pattern, \n        layers \n      });\n      return false;\n    }\n  }\n\n  /**\n     * Get value from specific cache layer\n     */\n  async getFromLayer(key, layer) {\n    try {\n      switch (layer) {\n      case 'local':\n        return this.localCache.get(key);\n\n      case 'redis':\n        if (!this.isRedisConnected) return null;\n        return await this.redis.get(key);\n\n      case 'database':\n        return await this.getFromDatabase(key);\n\n      default:\n        throw new Error(`Unknown cache layer: ${layer}`);\n      }\n    } catch (error) {\n      this.updateMetrics('errors', layer);\n      logger.error(`Cache get error from ${layer}`, { \n        error: error.message, \n        key: this.maskKey(key) \n      });\n      return null;\n    }\n  }\n\n  /**\n     * Set value in specific cache layer\n     */\n  async setInLayer(key, value, layer, ttl) {\n    try {\n      switch (layer) {\n      case 'local':\n        this.localCache.set(key, value, ttl);\n        this.updateMetrics('sets', 'local');\n        break;\n\n      case 'redis':\n        if (!this.isRedisConnected) return false;\n        await this.redis.setEx(key, ttl, value);\n        this.updateMetrics('sets', 'redis');\n        break;\n\n      case 'database':\n        await this.setInDatabase(key, value, ttl);\n        this.updateMetrics('sets', 'database');\n        break;\n\n      default:\n        throw new Error(`Unknown cache layer: ${layer}`);\n      }\n      return true;\n    } catch (error) {\n      this.updateMetrics('errors', layer);\n      logger.error(`Cache set error in ${layer}`, { \n        error: error.message, \n        key: this.maskKey(key) \n      });\n      return false;\n    }\n  }\n\n  /**\n     * Delete value from specific cache layer\n     */\n  async deleteFromLayer(key, layer) {\n    try {\n      switch (layer) {\n      case 'local':\n        this.localCache.del(key);\n        this.updateMetrics('deletes', 'local');\n        break;\n\n      case 'redis':\n        if (!this.isRedisConnected) return false;\n        await this.redis.del(key);\n        this.updateMetrics('deletes', 'redis');\n        break;\n\n      case 'database':\n        await this.deleteFromDatabase(key);\n        this.updateMetrics('deletes', 'database');\n        break;\n      }\n      return true;\n    } catch (error) {\n      this.updateMetrics('errors', layer);\n      logger.error(`Cache delete error from ${layer}`, { \n        error: error.message, \n        key: this.maskKey(key) \n      });\n      return false;\n    }\n  }\n\n  /**\n     * Get value from database cache\n     */\n  async getFromDatabase(key) {\n    try {\n      const { data, error } = await supabase\n        .from('cache_store')\n        .select('value, expires_at')\n        .eq('key', key)\n        .single();\n\n      if (error || !data) return null;\n\n      // Check if expired\n      if (data.expires_at && new Date(data.expires_at) < new Date()) {\n        // Delete expired entry\n        await this.deleteFromDatabase(key);\n        return null;\n      }\n\n      // Update access tracking\n      await supabase\n        .from('cache_store')\n        .update({\n          access_count: supabase.sql`access_count + 1`,\n          last_accessed: new Date().toISOString()\n        })\n        .eq('key', key);\n\n      return data.value;\n\n    } catch (error) {\n      logger.error('Database cache get error', { \n        error: error.message, \n        key: this.maskKey(key) \n      });\n      return null;\n    }\n  }\n\n  /**\n     * Set value in database cache\n     */\n  async setInDatabase(key, value, ttl) {\n    try {\n      const expiresAt = ttl ? new Date(Date.now() + ttl * 1000).toISOString() : null;\n            \n      const { error } = await supabase\n        .from('cache_store')\n        .upsert({\n          key,\n          value,\n          expires_at: expiresAt,\n          cache_type: 'general',\n          access_count: 1,\n          last_accessed: new Date().toISOString(),\n          updated_at: new Date().toISOString()\n        }, { \n          onConflict: 'key' \n        });\n\n      if (error) {\n        throw error;\n      }\n\n      return true;\n\n    } catch (error) {\n      logger.error('Database cache set error', { \n        error: error.message, \n        key: this.maskKey(key) \n      });\n      return false;\n    }\n  }\n\n  /**\n     * Delete value from database cache\n     */\n  async deleteFromDatabase(key) {\n    try {\n      const { error } = await supabase\n        .from('cache_store')\n        .delete()\n        .eq('key', key);\n\n      if (error) {\n        throw error;\n      }\n\n      return true;\n\n    } catch (error) {\n      logger.error('Database cache delete error', { \n        error: error.message, \n        key: this.maskKey(key) \n      });\n      return false;\n    }\n  }\n\n  /**\n     * Clear database cache by pattern\n     */\n  async clearDatabaseCache(pattern = null) {\n    try {\n      let query = supabase.from('cache_store').delete();\n            \n      if (pattern) {\n        query = query.like('key', `%${pattern}%`);\n      } else {\n        query = query.neq('key', ''); // Delete all\n      }\n\n      const { error } = await query;\n            \n      if (error) {\n        throw error;\n      }\n\n      return true;\n\n    } catch (error) {\n      logger.error('Database cache clear error', { error: error.message, pattern });\n      return false;\n    }\n  }\n\n  /**\n     * Clear Redis cache by pattern\n     */\n  async clearRedisPattern(pattern) {\n    try {\n      const keys = await this.redis.keys(`*${pattern}*`);\n      if (keys.length > 0) {\n        await this.redis.del(keys);\n      }\n      return true;\n\n    } catch (error) {\n      logger.error('Redis cache clear error', { error: error.message, pattern });\n      return false;\n    }\n  }\n\n  /**\n     * Backfill previous cache layers\n     */\n  async backfillLayers(key, value, layers, hitLayer, strategy) {\n    try {\n      const hitIndex = layers.indexOf(hitLayer);\n      if (hitIndex <= 0) return; // No previous layers to backfill\n\n      const backfillLayers = layers.slice(0, hitIndex);\n      const promises = [];\n\n      for (const layer of backfillLayers) {\n        const ttl = strategy.ttl[layer] || this.getDefaultTtl(layer);\n        promises.push(this.setInLayer(key, value, layer, ttl));\n      }\n\n      await Promise.allSettled(promises);\n\n    } catch (error) {\n      logger.error('Cache backfill error', { \n        error: error.message, \n        key: this.maskKey(key) \n      });\n    }\n  }\n\n  /**\n     * Serialize value for storage\n     */\n  serializeValue(value) {\n    try {\n      if (typeof value === 'string') {\n        return value;\n      }\n\n      const serialized = JSON.stringify(value);\n            \n      // Compress large values if compression is enabled\n      if (this.enableCompression && serialized.length > this.compressionThreshold) {\n        const zlib = require('zlib');\n        return `compressed:${zlib.gzipSync(serialized).toString('base64')}`;\n      }\n\n      return serialized;\n\n    } catch (error) {\n      logger.error('Value serialization error', { error: error.message });\n      return null;\n    }\n  }\n\n  /**\n     * Deserialize value from storage\n     */\n  deserializeValue(value) {\n    try {\n      if (typeof value !== 'string') {\n        return value;\n      }\n\n      // Handle compressed values\n      if (value.startsWith('compressed:')) {\n        const zlib = require('zlib');\n        const compressed = value.substring(11); // Remove 'compressed:' prefix\n        const decompressed = zlib.gunzipSync(Buffer.from(compressed, 'base64')).toString();\n        return JSON.parse(decompressed);\n      }\n\n      // Try to parse as JSON, return as string if it fails\n      try {\n        return JSON.parse(value);\n      } catch {\n        return value;\n      }\n\n    } catch (error) {\n      logger.error('Value deserialization error', { error: error.message });\n      return null;\n    }\n  }\n\n  /**\n     * Get default TTL for cache layer\n     */\n  getDefaultTtl(layer) {\n    switch (layer) {\n    case 'local':\n      return this.localTtl;\n    case 'redis':\n      return this.redisTtl;\n    case 'database':\n      return this.dbTtl;\n    default:\n      return 3600; // 1 hour default\n    }\n  }\n\n  /**\n     * Update metrics\n     */\n  updateMetrics(type, layer) {\n    if (this.enableMetrics && this.metrics[type] && this.metrics[type][layer] !== undefined) {\n      this.metrics[type][layer]++;\n    }\n  }\n\n  /**\n     * Record metrics to database\n     */\n  async recordMetrics() {\n    if (!this.enableMetrics) return;\n\n    try {\n      const timestamp = new Date().toISOString();\n      const metricsToRecord = [];\n\n      // Record hit/miss ratios for each layer\n      ['local', 'redis', 'database'].forEach(layer => {\n        const hits = this.metrics.hits[layer];\n        const misses = this.metrics.misses[layer];\n        const total = hits + misses;\n\n        if (total > 0) {\n          metricsToRecord.push({\n            cache_layer: layer,\n            type: 'hit',\n            timestamp\n          });\n        }\n      });\n\n      if (metricsToRecord.length > 0) {\n        const { error } = await supabase\n          .from('cache_metrics')\n          .insert(metricsToRecord);\n\n        if (error) {\n          logger.error('Failed to record cache metrics', { error: error.message });\n        }\n      }\n\n      // Reset metrics after recording\n      this.resetMetrics();\n\n    } catch (error) {\n      logger.error('Cache metrics recording error', { error: error.message });\n    }\n  }\n\n  /**\n     * Reset metrics counters\n     */\n  resetMetrics() {\n    this.metrics = {\n      hits: { local: 0, redis: 0, database: 0 },\n      misses: { local: 0, redis: 0, database: 0 },\n      sets: { local: 0, redis: 0, database: 0 },\n      deletes: { local: 0, redis: 0, database: 0 },\n      errors: { local: 0, redis: 0, database: 0 }\n    };\n  }\n\n  /**\n     * Clean up expired cache entries\n     */\n  async cleanupExpired() {\n    try {\n      logger.debug('Starting cache cleanup');\n\n      // Cleanup database cache\n      const { error } = await supabase\n        .from('cache_store')\n        .delete()\n        .lt('expires_at', new Date().toISOString());\n\n      if (error) {\n        logger.error('Database cache cleanup error', { error: error.message });\n      }\n\n      logger.debug('Cache cleanup completed');\n\n    } catch (error) {\n      logger.error('Cache cleanup error', { error: error.message });\n    }\n  }\n\n  /**\n     * Warm up caches with frequently accessed data\n     */\n  async warmUpCaches() {\n    try {\n      logger.info('Starting cache warm up');\n\n      // Get frequently accessed cache entries\n      const { data: frequentEntries } = await supabase\n        .from('cache_store')\n        .select('key, value, cache_type')\n        .gt('access_count', 10)\n        .order('access_count', { ascending: false })\n        .limit(100);\n\n      if (frequentEntries && frequentEntries.length > 0) {\n        const promises = frequentEntries.map(entry => {\n          const strategy = this.strategies[entry.cache_type] || this.strategies['template'];\n          return this.set(entry.key, entry.value, entry.cache_type);\n        });\n\n        await Promise.allSettled(promises);\n                \n        logger.info('Cache warm up completed', { \n          entriesLoaded: frequentEntries.length \n        });\n      }\n\n    } catch (error) {\n      logger.error('Cache warm up error', { error: error.message });\n    }\n  }\n\n  /**\n     * Get cache statistics\n     */\n  getCacheStats() {\n    return {\n      metrics: this.metrics,\n      localCache: {\n        keys: this.localCache.keys().length,\n        maxKeys: this.maxLocalSize,\n        hits: this.localCache.getStats().hits,\n        misses: this.localCache.getStats().misses\n      },\n      redis: {\n        connected: this.isRedisConnected\n      },\n      strategies: Object.keys(this.strategies)\n    };\n  }\n\n  /**\n     * Get value size for metrics\n     */\n  getValueSize(value) {\n    try {\n      return JSON.stringify(value).length;\n    } catch {\n      return 0;\n    }\n  }\n\n  /**\n     * Mask key for logging (security)\n     */\n  maskKey(key) {\n    if (!key || key.length <= 8) return key;\n    return `${key.substring(0, 4)}***${key.substring(key.length - 4)}`;\n  }\n\n  /**\n     * Shutdown cache manager\n     */\n  async shutdown() {\n    try {\n      logger.info('Shutting down Cache Manager');\n\n      // Clear intervals\n      if (this.cleanupInterval) {\n        clearInterval(this.cleanupInterval);\n      }\n            \n      if (this.metricsInterval) {\n        clearInterval(this.metricsInterval);\n      }\n\n      // Record final metrics\n      if (this.enableMetrics) {\n        await this.recordMetrics();\n      }\n\n      // Close Redis connection\n      if (this.redis && this.isRedisConnected) {\n        await this.redis.disconnect();\n      }\n\n      // Clear local cache\n      this.localCache.flushAll();\n\n      logger.info('Cache Manager shutdown completed');\n      this.emit('shutdown');\n\n    } catch (error) {\n      logger.error('Cache Manager shutdown error', { error: error.message });\n      throw error;\n    }\n  }\n}\n\nmodule.exports = CacheManager;","usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\services\\advanced\\cluster-manager.js","messages":[{"ruleId":"no-unused-vars","severity":1,"message":"'healthMetrics' is defined but never used.","line":471,"column":27,"nodeType":"Identifier","messageId":"unusedVar","endLine":471,"endColumn":40},{"ruleId":"no-unused-vars","severity":1,"message":"'data' is assigned a value but never used.","line":634,"column":15,"nodeType":"Identifier","messageId":"unusedVar","endLine":634,"endColumn":19}],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":2,"fixableErrorCount":0,"fixableWarningCount":0,"source":"∩╗┐/**\n * Cluster Manager - Distributed Architecture Management for APL AI Scraper 2.0\n * Manages worker nodes, load balancing, and distributed job processing\n */\n\nconst EventEmitter = require('events');\nconst os = require('os');\nconst { v4: uuidv4 } = require('uuid');\nconst logger = require('../core/logger');\nconst { supabase } = require('../core/supabase');\nconst { createClient } = require('redis');\n\nclass ClusterManager extends EventEmitter {\n  constructor(options = {}) {\n    super();\n        \n    this.workerId = options.workerId || this.generateWorkerId();\n    this.nodeName = options.nodeName || os.hostname();\n    this.capabilities = options.capabilities || this.detectCapabilities();\n    this.maxConcurrentJobs = options.maxConcurrentJobs || os.cpus().length * 2;\n    this.heartbeatInterval = options.heartbeatInterval || 30000; // 30 seconds\n    this.healthCheckInterval = options.healthCheckInterval || 60000; // 1 minute\n        \n    this.status = 'starting';\n    this.currentJobs = new Map();\n    this.totalJobs = 0;\n    this.startTime = Date.now();\n        \n    this.redis = null;\n    this.heartbeatTimer = null;\n    this.healthCheckTimer = null;\n        \n    // Bind methods to preserve context\n    this.handleJobRequest = this.handleJobRequest.bind(this);\n    this.processJob = this.processJob.bind(this);\n    this.sendHeartbeat = this.sendHeartbeat.bind(this);\n    this.performHealthCheck = this.performHealthCheck.bind(this);\n  }\n\n  /**\n     * Initialize the cluster manager\n     */\n  async initialize() {\n    try {\n      logger.info('Initializing Cluster Manager', { \n        workerId: this.workerId, \n        nodeName: this.nodeName \n      });\n\n      // Connect to Redis for distributed coordination\n      await this.connectToRedis();\n            \n      // Register this worker node\n      await this.registerWorkerNode();\n            \n      // Start heartbeat and health monitoring\n      this.startHeartbeat();\n      this.startHealthMonitoring();\n            \n      // Set up job listeners\n      this.setupJobListeners();\n            \n      this.status = 'ready';\n      this.emit('ready');\n            \n      logger.info('Cluster Manager initialized successfully', { \n        workerId: this.workerId \n      });\n\n    } catch (error) {\n      logger.error('Failed to initialize Cluster Manager', { \n        error: error.message,\n        workerId: this.workerId\n      });\n      throw error;\n    }\n  }\n\n  /**\n     * Connect to Redis for distributed coordination\n     */\n  async connectToRedis() {\n    try {\n      this.redis = createClient({\n        url: process.env.REDIS_URL || 'redis://localhost:6379'\n      });\n\n      this.redis.on('error', (err) => {\n        logger.error('Redis connection error', { error: err.message });\n      });\n\n      this.redis.on('connect', () => {\n        logger.info('Connected to Redis for cluster coordination');\n      });\n\n      await this.redis.connect();\n\n    } catch (error) {\n      logger.error('Failed to connect to Redis', { error: error.message });\n      throw error;\n    }\n  }\n\n  /**\n     * Register this worker node in the database\n     */\n  async registerWorkerNode() {\n    try {\n      const nodeData = {\n        worker_id: this.workerId,\n        node_name: this.nodeName,\n        status: this.status,\n        capabilities: this.capabilities,\n        current_jobs: this.currentJobs.size,\n        total_jobs: this.totalJobs,\n        memory_usage: this.getMemoryUsage(),\n        cpu_usage: await this.getCpuUsage(),\n        started_at: new Date().toISOString()\n      };\n\n      const { error } = await supabase\n        .from('worker_nodes')\n        .upsert(nodeData, { \n          onConflict: 'worker_id',\n          ignoreDuplicates: false \n        });\n\n      if (error) {\n        throw new Error(`Failed to register worker node: ${error.message}`);\n      }\n\n      logger.info('Worker node registered successfully', { \n        workerId: this.workerId,\n        capabilities: this.capabilities\n      });\n\n    } catch (error) {\n      logger.error('Failed to register worker node', { \n        error: error.message,\n        workerId: this.workerId\n      });\n      throw error;\n    }\n  }\n\n  /**\n     * Start sending regular heartbeats\n     */\n  startHeartbeat() {\n    this.heartbeatTimer = setInterval(this.sendHeartbeat, this.heartbeatInterval);\n    logger.info('Heartbeat started', { \n      interval: this.heartbeatInterval,\n      workerId: this.workerId\n    });\n  }\n\n  /**\n     * Send heartbeat to update node status\n     */\n  async sendHeartbeat() {\n    try {\n      const updateData = {\n        status: this.status,\n        current_jobs: this.currentJobs.size,\n        total_jobs: this.totalJobs,\n        memory_usage: this.getMemoryUsage(),\n        cpu_usage: await this.getCpuUsage(),\n        last_heartbeat: new Date().toISOString()\n      };\n\n      const { error } = await supabase\n        .from('worker_nodes')\n        .update(updateData)\n        .eq('worker_id', this.workerId);\n\n      if (error) {\n        logger.error('Failed to send heartbeat', { \n          error: error.message,\n          workerId: this.workerId\n        });\n      }\n\n      // Also update Redis for real-time coordination\n      await this.redis.hSet(\n        `worker:${this.workerId}`, \n        updateData\n      );\n      await this.redis.expire(`worker:${this.workerId}`, 120); // 2 minutes TTL\n\n    } catch (error) {\n      logger.error('Heartbeat failed', { \n        error: error.message,\n        workerId: this.workerId\n      });\n    }\n  }\n\n  /**\n     * Start health monitoring\n     */\n  startHealthMonitoring() {\n    this.healthCheckTimer = setInterval(this.performHealthCheck, this.healthCheckInterval);\n    logger.info('Health monitoring started', { \n      interval: this.healthCheckInterval,\n      workerId: this.workerId\n    });\n  }\n\n  /**\n     * Perform comprehensive health check\n     */\n  async performHealthCheck() {\n    try {\n      const healthMetrics = {\n        memory: this.getMemoryUsage(),\n        cpu: await this.getCpuUsage(),\n        jobs: this.currentJobs.size,\n        uptime: Date.now() - this.startTime,\n        redis: await this.checkRedisHealth(),\n        database: await this.checkDatabaseHealth()\n      };\n\n      // Determine overall health status\n      let healthStatus = 'healthy';\n      if (healthMetrics.memory > 90 || healthMetrics.cpu > 90) {\n        healthStatus = 'warning';\n      }\n      if (healthMetrics.memory > 95 || healthMetrics.cpu > 95 || !healthMetrics.redis || !healthMetrics.database) {\n        healthStatus = 'critical';\n      }\n\n      // Store health metrics\n      await this.storeHealthMetrics(healthStatus, healthMetrics);\n\n      // Emit health status\n      this.emit('health', { status: healthStatus, metrics: healthMetrics });\n\n      // Auto-scale based on health\n      await this.handleAutoScaling(healthMetrics);\n\n    } catch (error) {\n      logger.error('Health check failed', { \n        error: error.message,\n        workerId: this.workerId\n      });\n    }\n  }\n\n  /**\n     * Set up job listeners for distributed processing\n     */\n  setupJobListeners() {\n    // Listen for job assignments from Redis\n    this.redis.subscribe(`jobs:${this.workerId}`, (message) => {\n      const jobData = JSON.parse(message);\n      this.handleJobRequest(jobData);\n    });\n\n    // Listen for cluster-wide broadcasts\n    this.redis.subscribe('cluster:broadcast', (message) => {\n      const broadcastData = JSON.parse(message);\n      this.handleClusterBroadcast(broadcastData);\n    });\n\n    logger.info('Job listeners set up', { workerId: this.workerId });\n  }\n\n  /**\n     * Handle incoming job requests\n     */\n  async handleJobRequest(jobData) {\n    try {\n      // Check if we can accept the job\n      if (this.currentJobs.size >= this.maxConcurrentJobs) {\n        logger.warn('Job rejected - at capacity', { \n          workerId: this.workerId,\n          currentJobs: this.currentJobs.size,\n          maxJobs: this.maxConcurrentJobs,\n          jobId: jobData.id\n        });\n                \n        // Publish rejection back to coordinator\n        await this.redis.publish('job:rejected', JSON.stringify({\n          jobId: jobData.id,\n          workerId: this.workerId,\n          reason: 'at_capacity'\n        }));\n        return;\n      }\n\n      // Accept and process the job\n      logger.info('Job accepted for processing', { \n        workerId: this.workerId,\n        jobId: jobData.id,\n        type: jobData.type\n      });\n\n      this.currentJobs.set(jobData.id, {\n        ...jobData,\n        startTime: Date.now(),\n        status: 'processing'\n      });\n\n      // Acknowledge job acceptance\n      await this.redis.publish('job:accepted', JSON.stringify({\n        jobId: jobData.id,\n        workerId: this.workerId\n      }));\n\n      // Process the job asynchronously\n      this.processJob(jobData).catch(error => {\n        logger.error('Job processing failed', { \n          error: error.message,\n          jobId: jobData.id,\n          workerId: this.workerId\n        });\n      });\n\n    } catch (error) {\n      logger.error('Failed to handle job request', { \n        error: error.message,\n        jobId: jobData?.id,\n        workerId: this.workerId\n      });\n    }\n  }\n\n  /**\n     * Process a job\n     */\n  async processJob(jobData) {\n    const startTime = Date.now();\n    let jobResult = null;\n\n    try {\n      logger.info('Starting job processing', { \n        jobId: jobData.id,\n        type: jobData.type,\n        workerId: this.workerId\n      });\n\n      // Update job status\n      this.currentJobs.set(jobData.id, {\n        ...this.currentJobs.get(jobData.id),\n        status: 'processing'\n      });\n\n      // Process based on job type\n      switch (jobData.type) {\n      case 'scraping':\n        jobResult = await this.processScrapeJob(jobData);\n        break;\n      case 'training':\n        jobResult = await this.processTrainingJob(jobData);\n        break;\n      case 'export':\n        jobResult = await this.processExportJob(jobData);\n        break;\n      default:\n        throw new Error(`Unknown job type: ${jobData.type}`);\n      }\n\n      // Mark job as completed\n      this.currentJobs.delete(jobData.id);\n      this.totalJobs++;\n\n      const duration = Date.now() - startTime;\n      logger.info('Job completed successfully', { \n        jobId: jobData.id,\n        workerId: this.workerId,\n        duration\n      });\n\n      // Publish completion\n      await this.redis.publish('job:completed', JSON.stringify({\n        jobId: jobData.id,\n        workerId: this.workerId,\n        result: jobResult,\n        duration\n      }));\n\n    } catch (error) {\n      // Mark job as failed\n      this.currentJobs.delete(jobData.id);\n            \n      const duration = Date.now() - startTime;\n      logger.error('Job processing failed', { \n        error: error.message,\n        jobId: jobData.id,\n        workerId: this.workerId,\n        duration\n      });\n\n      // Publish failure\n      await this.redis.publish('job:failed', JSON.stringify({\n        jobId: jobData.id,\n        workerId: this.workerId,\n        error: error.message,\n        duration\n      }));\n\n      throw error;\n    }\n  }\n\n  /**\n     * Process scraping job\n     */\n  async processScrapeJob(jobData) {\n    // This would integrate with your existing scraping engine\n    const ScrapingEngine = require('../core/scraping-engine');\n    const engine = new ScrapingEngine();\n        \n    return await engine.executeJob(jobData);\n  }\n\n  /**\n     * Process training job\n     */\n  async processTrainingJob(jobData) {\n    // This would integrate with your visual training system\n    const VisualTrainer = require('../ai/visual-trainer');\n    const trainer = new VisualTrainer();\n        \n    return await trainer.processTrainingJob(jobData);\n  }\n\n  /**\n     * Process export job\n     */\n  async processExportJob(jobData) {\n    // This would integrate with your data export system\n    const DataExporter = require('../services/data-exporter');\n    const exporter = new DataExporter();\n        \n    return await exporter.processExportJob(jobData);\n  }\n\n  /**\n     * Handle cluster-wide broadcasts\n     */\n  async handleClusterBroadcast(broadcastData) {\n    try {\n      switch (broadcastData.type) {\n      case 'shutdown':\n        logger.info('Received shutdown broadcast');\n        await this.gracefulShutdown();\n        break;\n      case 'scale_down':\n        logger.info('Received scale down broadcast');\n        await this.handleScaleDown();\n        break;\n      case 'health_check':\n        logger.info('Received health check broadcast');\n        await this.performHealthCheck();\n        break;\n      default:\n        logger.debug('Unknown broadcast type', { type: broadcastData.type });\n      }\n    } catch (error) {\n      logger.error('Failed to handle cluster broadcast', { \n        error: error.message,\n        broadcastType: broadcastData.type\n      });\n    }\n  }\n\n  /**\n     * Handle auto-scaling based on health metrics\n     */\n  async handleAutoScaling(healthMetrics) {\n    try {\n      // Get cluster-wide metrics\n      const clusterMetrics = await this.getClusterMetrics();\n            \n      // Scale up conditions\n      if (clusterMetrics.avgCpuUsage > 80 && clusterMetrics.totalQueueSize > 100) {\n        await this.requestScaleUp();\n      }\n            \n      // Scale down conditions\n      if (clusterMetrics.avgCpuUsage < 30 && clusterMetrics.totalQueueSize < 10) {\n        await this.requestScaleDown();\n      }\n\n    } catch (error) {\n      logger.error('Auto-scaling handling failed', { \n        error: error.message,\n        workerId: this.workerId\n      });\n    }\n  }\n\n  /**\n     * Get cluster-wide metrics\n     */\n  async getClusterMetrics() {\n    try {\n      const { data: workers } = await supabase\n        .from('worker_nodes')\n        .select('*')\n        .eq('status', 'ready');\n\n      const totalWorkers = workers?.length || 0;\n      const avgCpuUsage = workers?.reduce((sum, w) => sum + (w.cpu_usage || 0), 0) / totalWorkers || 0;\n      const avgMemoryUsage = workers?.reduce((sum, w) => sum + (w.memory_usage || 0), 0) / totalWorkers || 0;\n      const totalCurrentJobs = workers?.reduce((sum, w) => sum + (w.current_jobs || 0), 0) || 0;\n\n      // Get queue statistics\n      const { data: queueStats } = await supabase\n        .from('job_queue_stats')\n        .select('*')\n        .order('recorded_at', { ascending: false })\n        .limit(1);\n\n      const totalQueueSize = queueStats?.[0]?.pending_jobs || 0;\n\n      return {\n        totalWorkers,\n        avgCpuUsage,\n        avgMemoryUsage,\n        totalCurrentJobs,\n        totalQueueSize\n      };\n\n    } catch (error) {\n      logger.error('Failed to get cluster metrics', { error: error.message });\n      return {\n        totalWorkers: 1,\n        avgCpuUsage: 0,\n        avgMemoryUsage: 0,\n        totalCurrentJobs: 0,\n        totalQueueSize: 0\n      };\n    }\n  }\n\n  /**\n     * Request cluster scale up\n     */\n  async requestScaleUp() {\n    try {\n      logger.info('Requesting cluster scale up', { workerId: this.workerId });\n            \n      await this.redis.publish('cluster:scale_request', JSON.stringify({\n        type: 'scale_up',\n        requestedBy: this.workerId,\n        timestamp: Date.now()\n      }));\n\n    } catch (error) {\n      logger.error('Failed to request scale up', { error: error.message });\n    }\n  }\n\n  /**\n     * Request cluster scale down\n     */\n  async requestScaleDown() {\n    try {\n      logger.info('Requesting cluster scale down', { workerId: this.workerId });\n            \n      await this.redis.publish('cluster:scale_request', JSON.stringify({\n        type: 'scale_down',\n        requestedBy: this.workerId,\n        timestamp: Date.now()\n      }));\n\n    } catch (error) {\n      logger.error('Failed to request scale down', { error: error.message });\n    }\n  }\n\n  /**\n     * Handle scale down request\n     */\n  async handleScaleDown() {\n    if (this.currentJobs.size === 0) {\n      logger.info('Worker eligible for scale down - no active jobs');\n      await this.gracefulShutdown();\n    } else {\n      logger.info('Worker not eligible for scale down - has active jobs', { \n        activeJobs: this.currentJobs.size \n      });\n    }\n  }\n\n  /**\n     * Store health metrics in database\n     */\n  async storeHealthMetrics(status, metrics) {\n    try {\n      const { error } = await supabase\n        .from('system_health')\n        .insert({\n          component: `worker:${this.workerId}`,\n          status,\n          metrics,\n          details: {\n            node_name: this.nodeName,\n            capabilities: this.capabilities\n          }\n        });\n\n      if (error) {\n        throw error;\n      }\n\n    } catch (error) {\n      logger.error('Failed to store health metrics', { \n        error: error.message,\n        workerId: this.workerId\n      });\n    }\n  }\n\n  /**\n     * Check Redis health\n     */\n  async checkRedisHealth() {\n    try {\n      const pong = await this.redis.ping();\n      return pong === 'PONG';\n    } catch (error) {\n      return false;\n    }\n  }\n\n  /**\n     * Check database health\n     */\n  async checkDatabaseHealth() {\n    try {\n      const { data, error } = await supabase\n        .from('worker_nodes')\n        .select('id')\n        .limit(1);\n\n      return !error;\n    } catch (error) {\n      return false;\n    }\n  }\n\n  /**\n     * Get memory usage percentage\n     */\n  getMemoryUsage() {\n    const usage = process.memoryUsage();\n    const totalMemory = os.totalmem();\n    return Math.round((usage.rss / totalMemory) * 100);\n  }\n\n  /**\n     * Get CPU usage percentage\n     */\n  async getCpuUsage() {\n    return new Promise((resolve) => {\n      const startUsage = process.cpuUsage();\n      const startTime = process.hrtime();\n\n      setTimeout(() => {\n        const currentUsage = process.cpuUsage(startUsage);\n        const currentTime = process.hrtime(startTime);\n                \n        const totalTime = currentTime[0] * 1000000 + currentTime[1] / 1000;\n        const cpuTime = (currentUsage.user + currentUsage.system);\n        const cpuPercent = Math.round((cpuTime / totalTime) * 100);\n                \n        resolve(Math.min(cpuPercent, 100));\n      }, 100);\n    });\n  }\n\n  /**\n     * Detect worker capabilities\n     */\n  detectCapabilities() {\n    return {\n      cpu_cores: os.cpus().length,\n      memory_gb: Math.round(os.totalmem() / (1024 * 1024 * 1024)),\n      platform: os.platform(),\n      arch: os.arch(),\n      node_version: process.version,\n      supports_headless: true, // Assume Playwright support\n      max_concurrent_jobs: this.maxConcurrentJobs\n    };\n  }\n\n  /**\n     * Generate unique worker ID\n     */\n  generateWorkerId() {\n    return `worker-${this.nodeName}-${uuidv4().substring(0, 8)}`;\n  }\n\n  /**\n     * Graceful shutdown\n     */\n  async gracefulShutdown() {\n    try {\n      logger.info('Starting graceful shutdown', { workerId: this.workerId });\n            \n      this.status = 'shutting_down';\n            \n      // Stop accepting new jobs\n      if (this.heartbeatTimer) {\n        clearInterval(this.heartbeatTimer);\n        this.heartbeatTimer = null;\n      }\n            \n      if (this.healthCheckTimer) {\n        clearInterval(this.healthCheckTimer);\n        this.healthCheckTimer = null;\n      }\n            \n      // Wait for current jobs to complete\n      const maxWaitTime = 300000; // 5 minutes\n      const startTime = Date.now();\n            \n      while (this.currentJobs.size > 0 && (Date.now() - startTime) < maxWaitTime) {\n        logger.info('Waiting for jobs to complete', { \n          remainingJobs: this.currentJobs.size,\n          workerId: this.workerId\n        });\n        await new Promise(resolve => setTimeout(resolve, 5000));\n      }\n            \n      // Force stop any remaining jobs\n      if (this.currentJobs.size > 0) {\n        logger.warn('Force stopping remaining jobs', { \n          remainingJobs: this.currentJobs.size,\n          workerId: this.workerId\n        });\n        this.currentJobs.clear();\n      }\n            \n      // Update node status to offline\n      await supabase\n        .from('worker_nodes')\n        .update({ status: 'offline' })\n        .eq('worker_id', this.workerId);\n            \n      // Close Redis connection\n      if (this.redis) {\n        await this.redis.disconnect();\n      }\n            \n      this.status = 'offline';\n      this.emit('shutdown');\n            \n      logger.info('Graceful shutdown completed', { workerId: this.workerId });\n\n    } catch (error) {\n      logger.error('Error during graceful shutdown', { \n        error: error.message,\n        workerId: this.workerId\n      });\n      throw error;\n    }\n  }\n\n  /**\n     * Get worker status\n     */\n  getStatus() {\n    return {\n      workerId: this.workerId,\n      nodeName: this.nodeName,\n      status: this.status,\n      currentJobs: this.currentJobs.size,\n      totalJobs: this.totalJobs,\n      capabilities: this.capabilities,\n      uptime: Date.now() - this.startTime,\n      memoryUsage: this.getMemoryUsage()\n    };\n  }\n}\n\nmodule.exports = ClusterManager;","usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\services\\advanced\\self-healing-engine.js","messages":[{"ruleId":"no-unused-vars","severity":1,"message":"'context' is defined but never used.","line":107,"column":24,"nodeType":"Identifier","messageId":"unusedVar","endLine":107,"endColumn":31}],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":0,"source":"∩╗┐// Self-Healing Scraper Engine\nconst { AIService } = require('../ai-service');\n\nclass SelfHealingEngine {\n  constructor() {\n    this.aiService = new AIService();\n    this.supabase = null; // Will be injected\n    this.retryStrategies = [\n      { name: 'selector_variation', weight: 0.8, priority: 1 },\n      { name: 'xpath_fallback', weight: 0.7, priority: 2 },\n      { name: 'text_content_matching', weight: 0.6, priority: 3 },\n      { name: 'ai_selector_generation', weight: 0.9, priority: 4 }\n    ];\n  }\n\n  setSupabase(supabase) {\n    this.supabase = supabase;\n  }\n\n  async handleScrapingFailure(job, error, page) {\n    console.log(`├░┼╕ΓÇ¥┬º Attempting self-healing for job ${job.id}: ${error.message}`);\n    \n    const context = await this.analyzeFailureContext(job, error, page);\n    const healingStrategies = this.getHealingStrategies(context);\n    \n    // Try different healing strategies in order of effectiveness\n    for (const strategy of healingStrategies) {\n      console.log(`├░┼╕┼╜┬» Trying healing strategy: ${strategy.name} (priority: ${strategy.priority})`);\n      \n      try {\n        const result = await strategy.apply(context);\n        \n        if (result.success) {\n          await this.recordHealingSuccess(job, strategy, result);\n          console.log(`├ó┼ôΓÇª Self-healing successful with strategy: ${strategy.name}`);\n          return result;\n        } else {\n          console.log(`├ó┬¥┼Æ Strategy ${strategy.name} failed: ${result.reason || 'Unknown reason'}`);\n        }\n      } catch (strategyError) {\n        console.error(`Strategy ${strategy.name} threw error:`, strategyError.message);\n      }\n    }\n    \n    // All strategies failed\n    await this.recordHealingFailure(job, context);\n    throw new Error(`Self-healing failed for job ${job.id} - all strategies exhausted`);\n  }\n\n  async analyzeFailureContext(job, error, page) {\n    console.log(`├░┼╕ΓÇ¥┬ì Analyzing failure context for job ${job.id}`);\n    \n    let screenshot = null;\n    let htmlContent = null;\n    let pageTitle = 'Unknown';\n    let pageUrl = 'Unknown';\n\n    try {\n      screenshot = await page.screenshot({ encoding: 'base64', fullPage: false });\n      htmlContent = await page.content();\n      pageTitle = await page.title();\n      pageUrl = page.url();\n    } catch (pageError) {\n      console.warn('Could not capture page context:', pageError.message);\n    }\n    \n    return {\n      job: job,\n      error: error.message,\n      errorStack: error.stack,\n      url: pageUrl,\n      screenshot: screenshot,\n      html: htmlContent,\n      timestamp: new Date().toISOString(),\n      selectors: this.extractSelectorsFromError(error),\n      pageTitle: pageTitle,\n      page: page // Include page reference for strategy execution\n    };\n  }\n\n  extractSelectorsFromError(error) {\n    const selectorRegex = /selector[^'\"`]*['\"`]([^'\"`]+)['\"`]/g;\n    const xpathRegex = /xpath[^'\"`]*['\"`]([^'\"`]+)['\"`]/g;\n    const matches = [];\n    let match;\n    \n    // Extract CSS selectors\n    while ((match = selectorRegex.exec(error.message)) !== null) {\n      matches.push({ type: 'css', selector: match[1] });\n    }\n    \n    // Extract XPath expressions\n    while ((match = xpathRegex.exec(error.message)) !== null) {\n      matches.push({ type: 'xpath', selector: match[1] });\n    }\n    \n    // Also check error stack for additional selectors\n    if (error.stack) {\n      while ((match = selectorRegex.exec(error.stack)) !== null) {\n        matches.push({ type: 'css', selector: match[1] });\n      }\n    }\n    \n    return matches;\n  }\n\n  getHealingStrategies(context) {\n    return this.retryStrategies\n      .sort((a, b) => a.priority - b.priority) // Sort by priority (lower number = higher priority)\n      .map(strategy => ({\n        ...strategy,\n        apply: this[`apply${this.camelCase(strategy.name)}`].bind(this)\n      }));\n  }\n\n  async applySelectorVariation(context) {\n    console.log('├░┼╕ΓÇ¥ΓÇ₧ Applying selector variation strategy');\n    \n    const failedSelectors = context.selectors.filter(s => s.type === 'css');\n    \n    for (const selectorInfo of failedSelectors) {\n      const variations = this.generateSelectorVariations(selectorInfo.selector);\n      \n      for (const variation of variations) {\n        try {\n          const element = await context.page.$(variation);\n          if (element) {\n            // Verify element is visible and has content\n            const isVisible = await element.isVisible().catch(() => false);\n            const hasContent = await element.textContent().then(text => text.trim().length > 0).catch(() => false);\n            \n            if (isVisible || hasContent) {\n              return {\n                success: true,\n                strategy: 'selector_variation',\n                newSelector: variation,\n                originalSelector: selectorInfo.selector,\n                confidence: 0.8\n              };\n            }\n          }\n        } catch (error) {\n          // Continue to next variation\n          console.log(`Selector variation failed: ${variation} - ${error.message}`);\n        }\n      }\n    }\n    \n    return { success: false, reason: 'No working selector variations found' };\n  }\n\n  generateSelectorVariations(selector) {\n    const variations = [];\n    \n    // Original selector (for completeness)\n    variations.push(selector);\n    \n    // Remove specificity levels\n    if (selector.includes('.')) {\n      // Try without specific classes\n      const withoutClasses = selector.replace(/\\.[\\w-]+/g, '');\n      if (withoutClasses.trim()) variations.push(withoutClasses.trim());\n      \n      // Try with different class combinations\n      const classes = selector.match(/\\.[\\w-]+/g) || [];\n      if (classes.length > 1) {\n        // Try with just the first class\n        const baseElement = selector.replace(/\\.[\\w-]+/g, '').trim();\n        variations.push(`${baseElement}${classes[0]}`);\n        \n        // Try with just the last class\n        variations.push(`${baseElement}${classes[classes.length - 1]}`);\n        \n        // Try all combinations of 2 classes\n        for (let i = 0; i < classes.length - 1; i++) {\n          for (let j = i + 1; j < classes.length; j++) {\n            variations.push(`${baseElement}${classes[i]}${classes[j]}`);\n          }\n        }\n      }\n    }\n    \n    // Try attribute selectors\n    if (selector.includes('[')) {\n      const withoutAttributes = selector.replace(/\\[[^\\]]+\\]/g, '');\n      if (withoutAttributes.trim()) variations.push(withoutAttributes.trim());\n    }\n    \n    // Try less specific parent-child relationships\n    if (selector.includes('>')) {\n      const withDescendant = selector.replace(/\\s*>\\s*/g, ' ');\n      variations.push(withDescendant);\n    }\n    \n    // Try with :first-child, :last-child variations\n    if (!selector.includes(':')) {\n      variations.push(`${selector}:first-child`);\n      variations.push(`${selector}:last-child`);\n      variations.push(`${selector}:nth-child(1)`);\n    }\n    \n    return [...new Set(variations)].filter(v => v && v !== selector);\n  }\n\n  async applyXpathFallback(context) {\n    console.log('├░┼╕ΓÇ¥ΓÇ₧ Applying XPath fallback strategy');\n    \n    const failedSelectors = context.selectors.filter(s => s.type === 'css');\n    \n    for (const selectorInfo of failedSelectors) {\n      try {\n        const xpath = this.cssToXPath(selectorInfo.selector);\n        const elements = await context.page.$$(`xpath=${xpath}`);\n        \n        if (elements.length > 0) {\n          // Test the first element\n          const element = elements[0];\n          const isVisible = await element.isVisible().catch(() => false);\n          const hasContent = await element.textContent().then(text => text.trim().length > 0).catch(() => false);\n          \n          if (isVisible || hasContent) {\n            return {\n              success: true,\n              strategy: 'xpath_fallback',\n              xpath: xpath,\n              originalSelector: selectorInfo.selector,\n              elementsFound: elements.length,\n              confidence: 0.7\n            };\n          }\n        }\n      } catch (error) {\n        console.log(`XPath conversion failed for ${selectorInfo.selector}:`, error.message);\n      }\n    }\n    \n    return { success: false, reason: 'XPath fallback selectors not found' };\n  }\n\n  cssToXPath(selector) {\n    // Enhanced CSS to XPath conversion\n    let xpath = selector\n      // Handle direct child combinator\n      .replace(/\\s*>\\s*/g, '/')\n      // Handle adjacent sibling combinator\n      .replace(/\\s*\\+\\s*/g, '/following-sibling::*[1]/')\n      // Handle general sibling combinator\n      .replace(/\\s*~\\s*/g, '/following-sibling::')\n      // Handle descendant combinator\n      .replace(/\\s+/g, '//')\n      // Handle class selectors\n      .replace(/\\.([a-zA-Z][\\w-]*)/g, '[contains(concat(\" \", @class, \" \"), \" $1 \")]')\n      // Handle ID selectors\n      .replace(/#([a-zA-Z][\\w-]*)/g, '[@id=\"$1\"]')\n      // Handle attribute selectors\n      .replace(/\\[([a-zA-Z][\\w-]*)=(['\"])([^'\"]+)\\2\\]/g, '[@$1=\"$3\"]')\n      .replace(/\\[([a-zA-Z][\\w-]*)\\]/g, '[@$1]')\n      // Handle pseudo-selectors\n      .replace(/:first-child/g, '[1]')\n      .replace(/:last-child/g, '[last()]')\n      .replace(/:nth-child\\((\\d+)\\)/g, '[$1]');\n    \n    // Ensure XPath starts correctly\n    if (!xpath.startsWith('/') && !xpath.startsWith('//')) {\n      xpath = '//' + xpath;\n    }\n    \n    return xpath;\n  }\n\n  async applyTextContentMatching(context) {\n    console.log('├░┼╕ΓÇ¥ΓÇ₧ Applying text content matching strategy');\n    \n    if (!context.screenshot) {\n      return { success: false, reason: 'No screenshot available for analysis' };\n    }\n\n    try {\n      // Use AI to find elements by their text content\n      const prompt = `\n        Analyze this webpage screenshot and find elements that match the scraping intent.\n        \n        FAILED SELECTORS: ${context.selectors.map(s => s.selector).join(', ')}\n        ERROR: ${context.error}\n        PAGE TITLE: ${context.pageTitle}\n        \n        Based on the error and typical web structures, suggest new CSS selectors that might work.\n        Consider:\n        1. Elements with similar text content or positioning\n        2. Elements with similar structural relationships\n        3. Alternative attribute combinations\n        4. Common selector patterns for this type of content\n        \n        Return JSON: {\n          \"suggested_selectors\": [\n            {\n              \"selector\": \"css_selector\",\n              \"confidence\": 0.9,\n              \"reason\": \"Explanation for why this selector should work\"\n            }\n          ]\n        }\n      `;\n\n      const analysis = await this.aiService.analyzeWithGPT4V(\n        Buffer.from(context.screenshot, 'base64'),\n        prompt\n      );\n\n      const suggestions = JSON.parse(analysis).suggested_selectors || [];\n      \n      // Try suggested selectors by confidence order\n      for (const suggestion of suggestions.sort((a, b) => b.confidence - a.confidence)) {\n        try {\n          const elements = await context.page.$$(suggestion.selector);\n          if (elements.length > 0) {\n            const element = elements[0];\n            const isVisible = await element.isVisible().catch(() => false);\n            const hasContent = await element.textContent().then(text => text.trim().length > 0).catch(() => false);\n            \n            if (isVisible || hasContent) {\n              return {\n                success: true,\n                strategy: 'text_content_matching',\n                newSelector: suggestion.selector,\n                confidence: suggestion.confidence,\n                reason: suggestion.reason,\n                elementsFound: elements.length\n              };\n            }\n          }\n        } catch (error) {\n          console.log(`AI suggested selector failed: ${suggestion.selector} - ${error.message}`);\n        }\n      }\n    } catch (error) {\n      console.error('AI-based text content matching failed:', error);\n    }\n    \n    return { success: false, reason: 'AI text content analysis did not find working selectors' };\n  }\n\n  async applyAiSelectorGeneration(context) {\n    console.log('├░┼╕ΓÇ¥ΓÇ₧ Applying AI selector generation strategy');\n    \n    if (!context.html) {\n      return { success: false, reason: 'No HTML content available for analysis' };\n    }\n\n    try {\n      // Use AI to generate entirely new selectors based on page content\n      const prompt = `\n        Given this webpage HTML and the original scraping intent, generate new CSS selectors.\n        \n        ORIGINAL INTENT: Extract data that was previously selected by: ${context.selectors.map(s => s.selector).join(', ')}\n        \n        PAGE HTML STRUCTURE (key elements):\n        ${this.extractRelevantHtml(context.html)}\n        \n        ERROR CONTEXT: ${context.error}\n        \n        Generate robust CSS selectors that are likely to survive minor DOM changes.\n        Focus on:\n        1. Semantic HTML elements (article, section, main, etc.)\n        2. Data attributes (data-*, role, etc.)\n        3. Structural relationships that are stable\n        4. Content patterns and text-based matching\n        \n        Return JSON: {\n          \"generated_selectors\": [\n            {\n              \"selector\": \"css_selector\",\n              \"type\": \"primary|fallback\",\n              \"explanation\": \"Why this selector should work and be robust\"\n            }\n          ]\n        }\n      `;\n\n      const response = await this.aiService.queryClaude(prompt);\n      const generated = JSON.parse(response).generated_selectors || [];\n      \n      for (const genSelector of generated) {\n        try {\n          const elements = await context.page.$$(genSelector.selector);\n          if (elements.length > 0) {\n            const element = elements[0];\n            const isVisible = await element.isVisible().catch(() => false);\n            const hasContent = await element.textContent().then(text => text.trim().length > 0).catch(() => false);\n            \n            if (isVisible || hasContent) {\n              return {\n                success: true,\n                strategy: 'ai_selector_generation',\n                newSelector: genSelector.selector,\n                type: genSelector.type,\n                explanation: genSelector.explanation,\n                elementsFound: elements.length,\n                confidence: 0.9\n              };\n            }\n          }\n        } catch (error) {\n          console.log(`AI generated selector failed: ${genSelector.selector} - ${error.message}`);\n        }\n      }\n    } catch (error) {\n      console.error('AI selector generation failed:', error);\n    }\n    \n    return { success: false, reason: 'AI selector generation did not produce working selectors' };\n  }\n\n  extractRelevantHtml(html) {\n    // Extract key structural elements for AI analysis\n    const parser = new DOMParser();\n    const doc = parser.parseFromString(html, 'text/html');\n    \n    const relevantElements = [];\n    \n    // Find semantic containers\n    const semanticTags = ['main', 'article', 'section', 'nav', 'aside', 'header', 'footer'];\n    semanticTags.forEach(tag => {\n      const elements = doc.querySelectorAll(tag);\n      elements.forEach(el => {\n        relevantElements.push(`<${tag} ${this.getRelevantAttributes(el)}>`);\n      });\n    });\n    \n    // Find elements with data attributes\n    const dataElements = doc.querySelectorAll('[data-*]');\n    dataElements.forEach(el => {\n      if (relevantElements.length < 20) { // Limit to avoid overwhelming AI\n        relevantElements.push(`<${el.tagName.toLowerCase()} ${this.getRelevantAttributes(el)}>`);\n      }\n    });\n    \n    return relevantElements.join('\\n').substring(0, 2000); // Limit size\n  }\n\n  getRelevantAttributes(element) {\n    const attrs = [];\n    if (element.id) attrs.push(`id=\"${element.id}\"`);\n    if (element.className) attrs.push(`class=\"${element.className}\"`);\n    \n    // Include data attributes\n    Array.from(element.attributes).forEach(attr => {\n      if (attr.name.startsWith('data-') || attr.name === 'role' || attr.name === 'aria-label') {\n        attrs.push(`${attr.name}=\"${attr.value}\"`);\n      }\n    });\n    \n    return attrs.join(' ');\n  }\n\n  async recordHealingSuccess(job, strategy, result) {\n    console.log(`├ó┼ôΓÇª Recording healing success for job ${job.id} with strategy ${strategy.name}`);\n    \n    if (this.supabase) {\n      await this.supabase\n        .from('healing_events')\n        .insert([{\n          job_id: job.id,\n          strategy: strategy.name,\n          success: true,\n          original_error: job.error_message || 'Unknown error',\n          healing_result: result,\n          timestamp: new Date().toISOString()\n        }]);\n    }\n\n    // Update template with learned improvements\n    await this.updateTemplateWithLearning(job.template_id, strategy, result);\n  }\n\n  async recordHealingFailure(job, context) {\n    console.log(`├ó┬¥┼Æ Recording healing failure for job ${job.id}`);\n    \n    if (this.supabase) {\n      await this.supabase\n        .from('healing_events')\n        .insert([{\n          job_id: job.id,\n          strategy: 'all_failed',\n          success: false,\n          original_error: context.error,\n          healing_result: {\n            attempted_strategies: this.retryStrategies.map(s => s.name),\n            failure_context: {\n              selectors_found: context.selectors,\n              page_title: context.pageTitle,\n              timestamp: context.timestamp\n            }\n          },\n          timestamp: new Date().toISOString()\n        }]);\n    }\n  }\n\n  async updateTemplateWithLearning(templateId, strategy, result) {\n    if (!this.supabase || !templateId) return;\n\n    try {\n      const { data: template } = await this.supabase\n        .from('scraper_templates')\n        .select('*')\n        .eq('id', templateId)\n        .single();\n\n      if (!template) return;\n\n      // Add fallback selectors to template configuration\n      const updatedConfig = {\n        ...template.config,\n        fallback_selectors: [\n          ...(template.config.fallback_selectors || []),\n          {\n            selector: result.newSelector || result.xpath,\n            strategy: strategy.name,\n            added_at: new Date().toISOString(),\n            confidence: result.confidence || 0.8,\n            original_failure: result.originalSelector,\n            success_context: {\n              elements_found: result.elementsFound,\n              explanation: result.explanation || result.reason\n            }\n          }\n        ].slice(-10) // Keep only last 10 fallback selectors\n      };\n\n      await this.supabase\n        .from('scraper_templates')\n        .update({ \n          config: updatedConfig,\n          last_healed: new Date().toISOString()\n        })\n        .eq('id', templateId);\n\n      console.log(`├░┼╕ΓÇ£┼í Updated template ${templateId} with learned selector: ${result.newSelector || result.xpath}`);\n    } catch (error) {\n      console.error('Error updating template with learning:', error);\n    }\n  }\n\n  async getHealingStats(templateId = null, days = 30) {\n    if (!this.supabase) return null;\n\n    const startDate = new Date(Date.now() - days * 24 * 60 * 60 * 1000).toISOString();\n    \n    let query = this.supabase\n      .from('healing_events')\n      .select('strategy, success, timestamp')\n      .gte('timestamp', startDate);\n\n    if (templateId) {\n      // Need to join with jobs to filter by template\n      query = this.supabase\n        .from('healing_events')\n        .select(`\n          strategy, \n          success, \n          timestamp,\n          scraping_jobs!inner(template_id)\n        `)\n        .eq('scraping_jobs.template_id', templateId)\n        .gte('timestamp', startDate);\n    }\n\n    const { data } = await query.order('timestamp', { ascending: false });\n\n    if (!data) return null;\n\n    const stats = {\n      total_attempts: data.length,\n      successful_healings: data.filter(e => e.success).length,\n      success_rate: 0,\n      by_strategy: {},\n      recent_events: data.slice(0, 10)\n    };\n\n    stats.success_rate = stats.total_attempts > 0 ? stats.successful_healings / stats.total_attempts : 0;\n\n    // Group by strategy\n    for (const event of data) {\n      if (!stats.by_strategy[event.strategy]) {\n        stats.by_strategy[event.strategy] = { attempts: 0, successes: 0, success_rate: 0 };\n      }\n      \n      stats.by_strategy[event.strategy].attempts++;\n      if (event.success) {\n        stats.by_strategy[event.strategy].successes++;\n      }\n      \n      stats.by_strategy[event.strategy].success_rate = \n        stats.by_strategy[event.strategy].successes / stats.by_strategy[event.strategy].attempts;\n    }\n\n    return stats;\n  }\n\n  camelCase(str) {\n    return str.replace(/_([a-z])/g, (g) => g[1].toUpperCase());\n  }\n}\n\nmodule.exports = { SelfHealingEngine };","usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\services\\advanced\\site-change-detector.js","messages":[{"ruleId":"no-unused-vars","severity":1,"message":"'crypto' is assigned a value but never used.","line":3,"column":7,"nodeType":"Identifier","messageId":"unusedVar","endLine":3,"endColumn":13},{"ruleId":"no-unused-vars","severity":1,"message":"'execution' is defined but never used.","line":62,"column":44,"nodeType":"Identifier","messageId":"unusedVar","endLine":62,"endColumn":53},{"ruleId":"no-unused-vars","severity":1,"message":"'changes' is assigned a value but never used.","line":213,"column":11,"nodeType":"Identifier","messageId":"unusedVar","endLine":213,"endColumn":18},{"ruleId":"no-unused-vars","severity":1,"message":"'template' is defined but never used.","line":487,"column":29,"nodeType":"Identifier","messageId":"unusedVar","endLine":487,"endColumn":37},{"ruleId":"no-unused-vars","severity":1,"message":"'data' is defined but never used.","line":591,"column":35,"nodeType":"Identifier","messageId":"unusedVar","endLine":591,"endColumn":39}],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":5,"fixableErrorCount":0,"fixableWarningCount":0,"source":"∩╗┐// AI-Powered Site Change Detection System\nconst { AIService } = require('../ai-service');\nconst crypto = require('crypto');\n\nclass SiteChangeDetector {\n  constructor() {\n    this.aiService = new AIService();\n    this.changeThreshold = 0.15; // 15% change threshold\n    this.supabase = null; // Will be injected\n    this.monitoringIntervals = new Map();\n  }\n\n  setSupabase(supabase) {\n    this.supabase = supabase;\n  }\n\n  async monitorTemplate(templateId, baselineData) {\n    // Clear existing monitoring for this template\n    if (this.monitoringIntervals.has(templateId)) {\n      clearInterval(this.monitoringIntervals.get(templateId));\n    }\n\n    // Start monitoring for site changes\n    const interval = setInterval(async () => {\n      await this.checkForChanges(templateId, baselineData);\n    }, 3600000); // Check every hour\n\n    this.monitoringIntervals.set(templateId, interval);\n\n    // Also monitor after each scrape\n    await this.setupRealtimeMonitoring(templateId);\n    \n    console.log(`├░┼╕ΓÇ¥┬ì Started monitoring template ${templateId} for site changes`);\n  }\n\n  async setupRealtimeMonitoring(templateId) {\n    if (!this.supabase) return;\n\n    // Set up real-time monitoring for scraping executions\n    const subscription = this.supabase\n      .channel(`template_monitoring_${templateId}`)\n      .on(\n        'postgres_changes',\n        {\n          event: 'INSERT',\n          schema: 'public',\n          table: 'scraping_executions',\n          filter: `template_id=eq.${templateId}`\n        },\n        async (payload) => {\n          const execution = payload.new;\n          if (execution.status === 'failed') {\n            await this.handleExecutionFailure(templateId, execution);\n          }\n        }\n      )\n      .subscribe();\n\n    return subscription;\n  }\n\n  async handleExecutionFailure(templateId, execution) {\n    console.log(`├░┼╕┼í┬¿ Execution failure detected for template ${templateId}, checking for site changes`);\n    \n    try {\n      const { data: baseline } = await this.supabase\n        .from('template_baselines')\n        .select('baseline_data')\n        .eq('template_id', templateId)\n        .order('created_at', { ascending: false })\n        .limit(1)\n        .single();\n\n      if (baseline) {\n        await this.checkForChanges(templateId, baseline.baseline_data);\n      }\n    } catch (error) {\n      console.error('Error checking for changes after execution failure:', error);\n    }\n  }\n\n  async checkForChanges(templateId, baseline) {\n    const template = await this.getTemplate(templateId);\n    if (!template) {\n      console.warn(`Template ${templateId} not found for change detection`);\n      return;\n    }\n\n    try {\n      console.log(`├░┼╕ΓÇ¥┬ì Checking for changes in template ${templateId}`);\n\n      // Execute template with test URL\n      const testResult = await this.executeTemplateTest(template);\n      \n      if (!testResult.success) {\n        await this.flagPotentialChange(templateId, 'scraping_failure', {\n          error: testResult.error,\n          timestamp: new Date().toISOString(),\n          template_name: template.name\n        });\n        return;\n      }\n\n      // Compare with baseline\n      const changes = await this.analyzeChanges(baseline, testResult.data);\n      \n      if (changes.significant) {\n        await this.handleSignificantChange(templateId, changes, template);\n      } else {\n        console.log(`├ó┼ôΓÇª No significant changes detected for template ${templateId}`);\n      }\n\n    } catch (error) {\n      console.error(`Change detection failed for template ${templateId}:`, error);\n      await this.recordChangeDetectionError(templateId, error);\n    }\n  }\n\n  async analyzeChanges(baseline, currentData) {\n    const changes = {\n      structural: [],\n      content: [],\n      significant: false,\n      confidence: 0,\n      timestamp: new Date().toISOString()\n    };\n\n    // Structural analysis\n    const baselineStructure = this.extractDataStructure(baseline);\n    const currentStructure = this.extractDataStructure(currentData);\n    \n    changes.structural = this.compareStructures(baselineStructure, currentStructure);\n\n    // Content pattern analysis\n    changes.content = await this.analyzeContentPatterns(baseline, currentData);\n\n    // Calculate change significance\n    changes.confidence = this.calculateChangeConfidence(changes);\n    changes.significant = changes.confidence > this.changeThreshold;\n\n    console.log(`├░┼╕ΓÇ£┼á Change analysis complete: confidence=${changes.confidence.toFixed(3)}, significant=${changes.significant}`);\n\n    return changes;\n  }\n\n  extractDataStructure(data) {\n    const structure = {};\n    \n    const extract = (obj, path = '') => {\n      if (!obj || typeof obj !== 'object') return;\n      \n      for (const [key, value] of Object.entries(obj)) {\n        const currentPath = path ? `${path}.${key}` : key;\n        \n        if (value && typeof value === 'object' && !Array.isArray(value)) {\n          structure[currentPath] = 'object';\n          extract(value, currentPath);\n        } else if (Array.isArray(value)) {\n          structure[currentPath] = 'array';\n          structure[`${currentPath}.length`] = value.length;\n          if (value.length > 0 && typeof value[0] === 'object') {\n            extract(value[0], `${currentPath}[0]`);\n          }\n        } else {\n          structure[currentPath] = typeof value;\n        }\n      }\n    };\n\n    extract(data);\n    return structure;\n  }\n\n  compareStructures(baseline, current) {\n    const changes = [];\n    \n    // Check for removed fields\n    for (const [path, type] of Object.entries(baseline)) {\n      if (!(path in current)) {\n        changes.push({\n          type: 'field_removed',\n          path: path,\n          previous_type: type,\n          severity: 'high'\n        });\n      }\n    }\n    \n    // Check for added fields\n    for (const [path, type] of Object.entries(current)) {\n      if (!(path in baseline)) {\n        changes.push({\n          type: 'field_added',\n          path: path,\n          new_type: type,\n          severity: 'medium'\n        });\n      } else if (baseline[path] !== type) {\n        changes.push({\n          type: 'type_changed',\n          path: path,\n          previous_type: baseline[path],\n          new_type: type,\n          severity: 'high'\n        });\n      }\n    }\n    \n    return changes;\n  }\n\n  async analyzeContentPatterns(baseline, current) {\n    const changes = [];\n    \n    try {\n      // Use AI to analyze content patterns\n      const prompt = `\n        Compare these two datasets from the same website and identify significant content pattern changes.\n        \n        BASELINE DATA:\n        ${JSON.stringify(baseline, null, 2).substring(0, 3000)}...\n        \n        CURRENT DATA:\n        ${JSON.stringify(current, null, 2).substring(0, 3000)}...\n        \n        Analyze for:\n        1. Changes in data format (dates, prices, etc.)\n        2. Changes in content structure\n        3. Missing or new data sections\n        4. Changes in data density or completeness\n        \n        Return JSON: {\n          \"changes\": [\n            {\n              \"type\": \"format_change|structure_change|content_change\",\n              \"description\": \"Detailed description\",\n              \"field\": \"field_path\",\n              \"significance\": \"low|medium|high\"\n            }\n          ],\n          \"overall_significance\": \"low|medium|high\"\n        }\n      `;\n\n      const analysis = await this.aiService.queryClaude(prompt);\n      const parsedAnalysis = JSON.parse(analysis);\n      return parsedAnalysis.changes || [];\n    } catch (error) {\n      console.warn('AI content analysis failed, falling back to basic analysis:', error.message);\n      return this.basicContentAnalysis(baseline, current);\n    }\n  }\n\n  basicContentAnalysis(baseline, current) {\n    const changes = [];\n    \n    const compareValues = (base, curr, path = '') => {\n      if (Array.isArray(base) && Array.isArray(curr)) {\n        if (base.length !== curr.length) {\n          const lengthDiff = Math.abs(base.length - curr.length);\n          const percentChange = lengthDiff / Math.max(base.length, 1);\n          \n          changes.push({\n            type: 'array_length_change',\n            path: path,\n            previous_length: base.length,\n            new_length: curr.length,\n            significance: percentChange > 0.5 ? 'high' : percentChange > 0.2 ? 'medium' : 'low',\n            description: `Array length changed from ${base.length} to ${curr.length}`\n          });\n        }\n        \n        // Compare first few items for pattern changes\n        const sampleSize = Math.min(3, base.length, curr.length);\n        for (let i = 0; i < sampleSize; i++) {\n          if (base[i] && curr[i]) {\n            compareValues(base[i], curr[i], `${path}[${i}]`);\n          }\n        }\n      } else if (typeof base === 'string' && typeof curr === 'string') {\n        // Check for format changes in common patterns\n        const baseIsPrice = /^\\$?\\d+\\.?\\d*$/.test(base);\n        const currIsPrice = /^\\$?\\d+\\.?\\d*$/.test(curr);\n        \n        if (baseIsPrice !== currIsPrice) {\n          changes.push({\n            type: 'format_change',\n            path: path,\n            description: 'Price format changed',\n            significance: 'medium'\n          });\n        }\n\n        const baseIsDate = /\\d{4}-\\d{2}-\\d{2}|\\d{2}\\/\\d{2}\\/\\d{4}/.test(base);\n        const currIsDate = /\\d{4}-\\d{2}-\\d{2}|\\d{2}\\/\\d{2}\\/\\d{4}/.test(curr);\n        \n        if (baseIsDate !== currIsDate) {\n          changes.push({\n            type: 'format_change',\n            path: path,\n            description: 'Date format changed',\n            significance: 'medium'\n          });\n        }\n      }\n    };\n    \n    compareValues(baseline, current);\n    return changes;\n  }\n\n  calculateChangeConfidence(changes) {\n    let score = 0;\n    \n    for (const change of changes.structural) {\n      switch (change.type) {\n      case 'field_removed':\n        score += 0.4;\n        break;\n      case 'field_added':\n        score += 0.2;\n        break;\n      case 'type_changed':\n        score += 0.5;\n        break;\n      }\n    }\n    \n    for (const change of changes.content) {\n      switch (change.significance) {\n      case 'high':\n        score += 0.6;\n        break;\n      case 'medium':\n        score += 0.3;\n        break;\n      case 'low':\n        score += 0.1;\n        break;\n      }\n    }\n    \n    return Math.min(1, score);\n  }\n\n  async handleSignificantChange(templateId, changes, template) {\n    console.log(`├░┼╕┼í┬¿ Significant change detected for template ${templateId}`);\n    \n    // Record the change detection\n    await this.recordChangeDetection(templateId, changes);\n\n    // Update template status\n    if (this.supabase) {\n      await this.supabase\n        .from('scraper_templates')\n        .update({\n          status: 'needs_review',\n          last_change_detected: new Date().toISOString(),\n          change_details: changes\n        })\n        .eq('id', templateId);\n    }\n\n    // Attempt automatic repair\n    const repairResult = await this.attemptAutomaticRepair(templateId, changes, template);\n    \n    if (repairResult.success) {\n      console.log(`├ó┼ôΓÇª Template ${templateId} automatically repaired`);\n    } else {\n      console.log(`├ó┬¥┼Æ Automatic repair failed for template ${templateId}`);\n      // Notify administrators\n      await this.sendChangeNotification(templateId, changes, repairResult);\n    }\n  }\n\n  async attemptAutomaticRepair(templateId, changes, template) {\n    try {\n      console.log(`├░┼╕ΓÇ¥┬º Attempting automatic repair for template ${templateId}`);\n\n      const repairPrompt = `\n        A web scraper template has detected significant changes on the target website.\n        \n        TEMPLATE NAME: ${template.name}\n        \n        CHANGES DETECTED:\n        ${JSON.stringify(changes, null, 2)}\n        \n        CURRENT TEMPLATE CODE:\n        ${template.code}\n        \n        Please analyze the changes and provide an updated template code that addresses these changes.\n        Focus on:\n        1. Updating CSS selectors that may have changed\n        2. Adjusting data extraction logic for new structure\n        3. Maintaining the original data output format\n        4. Adding fallback selectors for robustness\n        \n        Return only valid JavaScript code without explanations or markdown formatting.\n      `;\n\n      const repairedCode = await this.aiService.queryClaude(repairPrompt);\n      \n      // Validate the repaired code\n      const validation = await this.validateRepairedCode(repairedCode, template);\n      \n      if (validation.valid) {\n        // Update the template\n        if (this.supabase) {\n          await this.supabase\n            .from('scraper_templates')\n            .update({\n              code: repairedCode,\n              status: 'repaired',\n              version: this.incrementVersion(template.version),\n              last_repaired: new Date().toISOString(),\n              repair_details: {\n                changes: changes,\n                repair_timestamp: new Date().toISOString(),\n                validation: validation\n              }\n            })\n            .eq('id', templateId);\n        }\n\n        return { success: true, code: repairedCode };\n      } else {\n        return { success: false, error: validation.error };\n      }\n\n    } catch (error) {\n      console.error(`Repair attempt failed for template ${templateId}:`, error);\n      return { success: false, error: error.message };\n    }\n  }\n\n  async validateRepairedCode(code, originalTemplate) {\n    // Basic syntax validation\n    try {\n      new Function(code);\n    } catch (error) {\n      return { valid: false, error: `Syntax error: ${error.message}` };\n    }\n\n    // Test execution with sample URL\n    try {\n      const testResult = await this.executeTemplateTest({\n        ...originalTemplate,\n        code: code\n      });\n\n      return { \n        valid: testResult.success, \n        error: testResult.error || null,\n        testData: testResult.data \n      };\n    } catch (error) {\n      return { valid: false, error: error.message };\n    }\n  }\n\n  incrementVersion(currentVersion) {\n    if (!currentVersion) return '1.0.0';\n    \n    const parts = currentVersion.split('.');\n    const patch = parseInt(parts[2] || 0) + 1;\n    return `${parts[0] || 1}.${parts[1] || 0}.${patch}`;\n  }\n\n  async getTemplate(templateId) {\n    if (!this.supabase) return null;\n    \n    try {\n      const { data, error } = await this.supabase\n        .from('scraper_templates')\n        .select('*')\n        .eq('id', templateId)\n        .single();\n\n      if (error) throw error;\n      return data;\n    } catch (error) {\n      console.error(`Error fetching template ${templateId}:`, error);\n      return null;\n    }\n  }\n\n  async executeTemplateTest(template) {\n    // Mock template execution for testing\n    // In real implementation, this would run the scraper\n    try {\n      // Simulate scraping execution\n      await new Promise(resolve => setTimeout(resolve, 1000));\n      \n      // Mock successful result\n      return {\n        success: true,\n        data: {\n          title: 'Sample Title',\n          price: '$99.99',\n          items: [\n            { name: 'Item 1', value: 'Value 1' },\n            { name: 'Item 2', value: 'Value 2' }\n          ]\n        }\n      };\n    } catch (error) {\n      return {\n        success: false,\n        error: error.message\n      };\n    }\n  }\n\n  async flagPotentialChange(templateId, changeType, details) {\n    console.log(`├░┼╕┼í┬⌐ Flagging potential change for template ${templateId}: ${changeType}`);\n    \n    if (this.supabase) {\n      await this.supabase\n        .from('site_change_detections')\n        .insert([{\n          template_id: templateId,\n          change_type: changeType,\n          change_details: details,\n          confidence: 0.8,\n          detected_at: new Date().toISOString()\n        }]);\n    }\n  }\n\n  async recordChangeDetection(templateId, changes) {\n    if (!this.supabase) return;\n\n    try {\n      await this.supabase\n        .from('site_change_detections')\n        .insert([{\n          template_id: templateId,\n          change_type: 'structural_change',\n          change_details: changes,\n          confidence: changes.confidence,\n          detected_at: new Date().toISOString()\n        }]);\n    } catch (error) {\n      console.error('Error recording change detection:', error);\n    }\n  }\n\n  async recordChangeDetectionError(templateId, error) {\n    if (!this.supabase) return;\n\n    try {\n      await this.supabase\n        .from('site_change_detections')\n        .insert([{\n          template_id: templateId,\n          change_type: 'detection_error',\n          change_details: { error: error.message, stack: error.stack },\n          confidence: 0.0,\n          detected_at: new Date().toISOString()\n        }]);\n    } catch (dbError) {\n      console.error('Error recording change detection error:', dbError);\n    }\n  }\n\n  async sendChangeNotification(templateId, changes, repairResult) {\n    const notification = {\n      template_id: templateId,\n      type: 'site_change',\n      title: 'Website Structure Change Detected',\n      message: `Significant changes detected for template ${templateId}`,\n      severity: 'high',\n      data: {\n        changes: changes,\n        auto_repair_attempted: !repairResult.success,\n        repair_error: repairResult.error\n      },\n      created_at: new Date().toISOString()\n    };\n\n    if (this.supabase) {\n      await this.supabase\n        .from('system_alerts')\n        .insert([notification]);\n    }\n\n    // Send to webhook if configured\n    await this.triggerWebhook('site_change', notification);\n  }\n\n  async triggerWebhook(eventType, data) {\n    // Webhook triggering will be handled by WebhookManager\n    console.log(`├░┼╕ΓÇ£┬ó Would trigger webhook for event: ${eventType}`);\n  }\n\n  async stopMonitoring(templateId) {\n    if (this.monitoringIntervals.has(templateId)) {\n      clearInterval(this.monitoringIntervals.get(templateId));\n      this.monitoringIntervals.delete(templateId);\n      console.log(`├░┼╕ΓÇ║ΓÇÿ Stopped monitoring template ${templateId}`);\n    }\n  }\n\n  async getChangeDetectionStats(templateId = null) {\n    if (!this.supabase) return null;\n\n    let query = this.supabase\n      .from('site_change_detections')\n      .select('change_type, confidence, detected_at');\n\n    if (templateId) {\n      query = query.eq('template_id', templateId);\n    }\n\n    const { data } = await query\n      .order('detected_at', { ascending: false })\n      .limit(100);\n\n    if (!data) return null;\n\n    const stats = {\n      total_detections: data.length,\n      by_type: {},\n      avg_confidence: 0,\n      recent_detections: data.slice(0, 5)\n    };\n\n    let totalConfidence = 0;\n    for (const detection of data) {\n      stats.by_type[detection.change_type] = (stats.by_type[detection.change_type] || 0) + 1;\n      totalConfidence += detection.confidence || 0;\n    }\n\n    stats.avg_confidence = data.length > 0 ? totalConfidence / data.length : 0;\n\n    return stats;\n  }\n}\n\nmodule.exports = { SiteChangeDetector };","usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\services\\advanced\\webhook-manager.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\services\\ai-service.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\services\\auth\\auth-service.js","messages":[{"ruleId":"no-unused-vars","severity":1,"message":"'hashedPassword' is assigned a value but never used.","line":128,"column":13,"nodeType":"Identifier","messageId":"unusedVar","endLine":128,"endColumn":27},{"ruleId":"no-unused-vars","severity":1,"message":"'email' is defined but never used.","line":698,"column":29,"nodeType":"Identifier","messageId":"unusedVar","endLine":698,"endColumn":34},{"ruleId":"no-unused-vars","severity":1,"message":"'ipAddress' is defined but never used.","line":698,"column":36,"nodeType":"Identifier","messageId":"unusedVar","endLine":698,"endColumn":45},{"ruleId":"no-unused-vars","severity":1,"message":"'firstName' is defined but never used.","line":765,"column":45,"nodeType":"Identifier","messageId":"unusedVar","endLine":765,"endColumn":54}],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":4,"fixableErrorCount":0,"fixableWarningCount":0,"source":"∩╗┐/**\n * Advanced Authentication & Authorization Service - APL AI Scraper 2.0 Phase 6\n * Comprehensive security implementation with JWT, 2FA, rate limiting, and RBAC\n */\n\nconst bcrypt = require('bcryptjs');\nconst jwt = require('jsonwebtoken');\nconst rateLimit = require('express-rate-limit');\nconst { authenticator } = require('otplib');\nconst crypto = require('crypto');\nconst logger = require('../core/logger');\nconst { supabase } = require('../core/supabase');\n\nclass AuthService {\n  constructor() {\n    this.jwtSecret = process.env.JWT_SECRET || this.generateSecureSecret();\n    this.jwtRefreshSecret = process.env.JWT_REFRESH_SECRET || this.generateSecureSecret();\n    this.tokenExpiry = '15m'; // Access token expires in 15 minutes\n    this.refreshTokenExpiry = '7d'; // Refresh token expires in 7 days\n        \n    this.setupRateLimiting();\n    this.setupPasswordPolicy();\n        \n    logger.info('AuthService initialized with enhanced security features');\n  }\n\n  /**\n     * Set up comprehensive rate limiting\n     */\n  setupRateLimiting() {\n    // Login attempts rate limiting\n    this.loginLimiter = rateLimit({\n      windowMs: 15 * 60 * 1000, // 15 minutes\n      max: 5, // 5 login attempts per IP per window\n      message: {\n        error: 'Too many login attempts',\n        retryAfter: '15 minutes'\n      },\n      standardHeaders: true,\n      legacyHeaders: false,\n      skipSuccessfulRequests: true,\n      keyGenerator: (req) => {\n        // Use IP + email combination for more targeted limiting\n        const email = req.body?.email || 'unknown';\n        return `${req.ip}:${email}`;\n      }\n    });\n\n    // Registration rate limiting\n    this.registrationLimiter = rateLimit({\n      windowMs: 60 * 60 * 1000, // 1 hour\n      max: 3, // 3 registrations per IP per hour\n      message: {\n        error: 'Too many registration attempts',\n        retryAfter: '1 hour'\n      },\n      standardHeaders: true,\n      legacyHeaders: false\n    });\n\n    // API requests rate limiting\n    this.apiLimiter = rateLimit({\n      windowMs: 15 * 60 * 1000, // 15 minutes\n      max: 1000, // 1000 requests per IP per window\n      message: {\n        error: 'API rate limit exceeded',\n        retryAfter: '15 minutes'\n      },\n      standardHeaders: true,\n      legacyHeaders: false\n    });\n\n    // Password reset rate limiting\n    this.passwordResetLimiter = rateLimit({\n      windowMs: 60 * 60 * 1000, // 1 hour\n      max: 3, // 3 password reset attempts per IP per hour\n      message: {\n        error: 'Too many password reset attempts',\n        retryAfter: '1 hour'\n      },\n      standardHeaders: true,\n      legacyHeaders: false\n    });\n  }\n\n  /**\n     * Set up password security policy\n     */\n  setupPasswordPolicy() {\n    this.passwordPolicy = {\n      minLength: 8,\n      maxLength: 128,\n      requireUppercase: true,\n      requireLowercase: true,\n      requireNumbers: true,\n      requireSpecialChars: true,\n      preventCommonPasswords: true,\n      preventUserInfoInPassword: true\n    };\n  }\n\n  /**\n     * Register a new user with comprehensive validation\n     */\n  async registerUser(userData, ipAddress, userAgent) {\n    try {\n      const { email, password, firstName, lastName, ...additionalData } = userData;\n            \n      logger.info('User registration attempt', { email, ip: ipAddress });\n\n      // Validate input data\n      await this.validateRegistrationData(userData);\n\n      // Check if user already exists\n      const { data: existingUser } = await supabase\n        .from('user_profiles')\n        .select('id, email')\n        .eq('email', email.toLowerCase())\n        .single();\n\n      if (existingUser) {\n        await this.logSecurityEvent(null, 'registration_duplicate_email', 'low', \n          `Registration attempt with existing email: ${email}`, ipAddress, userAgent);\n        throw new Error('An account with this email already exists');\n      }\n\n      // Hash password\n      const hashedPassword = await bcrypt.hash(password, 12);\n\n      // Create user with Supabase Auth\n      const { data: authUser, error: authError } = await supabase.auth.signUp({\n        email: email.toLowerCase(),\n        password: password,\n        options: {\n          data: {\n            first_name: firstName,\n            last_name: lastName,\n            ...additionalData\n          }\n        }\n      });\n\n      if (authError) {\n        logger.error('Supabase auth registration failed', { error: authError.message, email });\n        throw new Error('Registration failed. Please try again.');\n      }\n\n      // Create detailed user profile\n      const { data: userProfile, error: profileError } = await supabase\n        .from('user_profiles')\n        .insert({\n          id: authUser.user.id,\n          email: email.toLowerCase(),\n          first_name: firstName,\n          last_name: lastName,\n          role: 'user',\n          is_active: true,\n          email_verified: false,\n          two_factor_enabled: false\n        })\n        .select()\n        .single();\n\n      if (profileError) {\n        logger.error('User profile creation failed', { error: profileError.message, userId: authUser.user.id });\n        // Clean up auth user if profile creation fails\n        await supabase.auth.admin.deleteUser(authUser.user.id);\n        throw new Error('Registration failed. Please try again.');\n      }\n\n      // Generate email verification token\n      const verificationToken = await this.generateVerificationToken(userProfile.id);\n\n      // Send verification email\n      await this.sendVerificationEmail(email, verificationToken, firstName);\n\n      // Log successful registration\n      await this.logSecurityEvent(userProfile.id, 'user_registered', 'info', \n        'New user registration completed', ipAddress, userAgent);\n\n      logger.info('User registration successful', { userId: userProfile.id, email });\n\n      return {\n        success: true,\n        user: {\n          id: userProfile.id,\n          email: userProfile.email,\n          firstName: userProfile.first_name,\n          lastName: userProfile.last_name,\n          role: userProfile.role\n        },\n        message: 'Registration successful. Please check your email to verify your account.'\n      };\n\n    } catch (error) {\n      logger.error('Registration failed', { error: error.message, email: userData?.email });\n      return {\n        success: false,\n        error: error.message\n      };\n    }\n  }\n\n  /**\n     * Authenticate user with comprehensive security checks\n     */\n  async loginUser(email, password, ipAddress, userAgent, twoFactorToken = null) {\n    try {\n      logger.info('Login attempt', { email, ip: ipAddress });\n\n      // Input validation\n      if (!email || !password) {\n        throw new Error('Email and password are required');\n      }\n\n      // Check for account lockout\n      await this.checkAccountLockout(email, ipAddress);\n\n      // Attempt authentication with Supabase\n      const { data: authData, error: authError } = await supabase.auth.signInWithPassword({\n        email: email.toLowerCase(),\n        password: password\n      });\n\n      if (authError) {\n        await this.logFailedLoginAttempt(email, ipAddress, userAgent, authError.message);\n        throw new Error('Invalid email or password');\n      }\n\n      // Get detailed user profile\n      const { data: user } = await supabase\n        .from('user_profiles')\n        .select('*')\n        .eq('id', authData.user.id)\n        .single();\n\n      if (!user) {\n        await this.logSecurityEvent(authData.user.id, 'login_no_profile', 'high', \n          'Login attempt with valid auth but no user profile', ipAddress, userAgent);\n        throw new Error('Account not found');\n      }\n\n      // Check account status\n      if (!user.is_active) {\n        await this.logSecurityEvent(user.id, 'login_inactive_account', 'medium', \n          'Login attempt on inactive account', ipAddress, userAgent);\n        throw new Error('Account is disabled. Please contact support.');\n      }\n\n      if (!user.email_verified) {\n        await this.logSecurityEvent(user.id, 'login_unverified_email', 'low', \n          'Login attempt with unverified email', ipAddress, userAgent);\n        throw new Error('Please verify your email before logging in.');\n      }\n\n      // Check two-factor authentication\n      if (user.two_factor_enabled) {\n        if (!twoFactorToken) {\n          return {\n            success: false,\n            requiresTwoFactor: true,\n            tempToken: this.generateTempToken(user.id),\n            message: 'Two-factor authentication required'\n          };\n        }\n\n        const isValidTwoFactor = await this.verifyTwoFactorToken(user.id, twoFactorToken);\n        if (!isValidTwoFactor) {\n          await this.logSecurityEvent(user.id, 'login_invalid_2fa', 'medium', \n            'Invalid 2FA token provided', ipAddress, userAgent);\n          throw new Error('Invalid two-factor authentication code');\n        }\n      }\n\n      // Generate tokens\n      const accessToken = this.generateJWT(user, 'access');\n      const refreshToken = this.generateJWT(user, 'refresh');\n\n      // Store refresh token\n      await this.storeRefreshToken(user.id, refreshToken, ipAddress, userAgent);\n\n      // Update last login\n      await this.updateLastLogin(user.id, ipAddress);\n\n      // Log successful login\n      await this.logSecurityEvent(user.id, 'login_success', 'info', \n        'Successful user login', ipAddress, userAgent);\n\n      logger.info('Login successful', { userId: user.id, email: user.email });\n\n      return {\n        success: true,\n        accessToken,\n        refreshToken,\n        user: {\n          id: user.id,\n          email: user.email,\n          firstName: user.first_name,\n          lastName: user.last_name,\n          role: user.role,\n          twoFactorEnabled: user.two_factor_enabled\n        },\n        expiresIn: this.tokenExpiry\n      };\n\n    } catch (error) {\n      logger.error('Login failed', { error: error.message, email });\n      return {\n        success: false,\n        error: error.message\n      };\n    }\n  }\n\n  /**\n     * Generate JWT token with enhanced security\n     */\n  generateJWT(user, tokenType = 'access') {\n    const isRefreshToken = tokenType === 'refresh';\n    const secret = isRefreshToken ? this.jwtRefreshSecret : this.jwtSecret;\n    const expiresIn = isRefreshToken ? this.refreshTokenExpiry : this.tokenExpiry;\n        \n    const payload = {\n      userId: user.id,\n      email: user.email,\n      role: user.role,\n      tokenType,\n      iss: 'apl-ai-scraper',\n      aud: 'apl-ai-scraper-client',\n      jti: crypto.randomUUID(), // Unique token ID for revocation\n      iat: Math.floor(Date.now() / 1000),\n      // Add security context\n      permissions: this.getUserPermissions(user.role)\n    };\n\n    return jwt.sign(payload, secret, {\n      expiresIn,\n      algorithm: 'HS256',\n      issuer: 'apl-ai-scraper',\n      audience: 'apl-ai-scraper-client'\n    });\n  }\n\n  /**\n     * Verify JWT token with comprehensive validation\n     */\n  async verifyToken(token, tokenType = 'access') {\n    try {\n      const isRefreshToken = tokenType === 'refresh';\n      const secret = isRefreshToken ? this.jwtRefreshSecret : this.jwtSecret;\n            \n      // Verify token signature and decode\n      const decoded = jwt.verify(token, secret, {\n        issuer: 'apl-ai-scraper',\n        audience: 'apl-ai-scraper-client',\n        algorithms: ['HS256']\n      });\n\n      // Validate token type\n      if (decoded.tokenType !== tokenType) {\n        throw new Error('Invalid token type');\n      }\n\n      // Check if token is blacklisted\n      const isBlacklisted = await this.isTokenBlacklisted(decoded.jti);\n      if (isBlacklisted) {\n        throw new Error('Token has been revoked');\n      }\n\n      // Verify user still exists and is active\n      const { data: user } = await supabase\n        .from('user_profiles')\n        .select('id, email, role, is_active, email_verified')\n        .eq('id', decoded.userId)\n        .single();\n\n      if (!user) {\n        throw new Error('User not found');\n      }\n\n      if (!user.is_active) {\n        throw new Error('Account is disabled');\n      }\n\n      if (!user.email_verified) {\n        throw new Error('Email not verified');\n      }\n\n      return {\n        valid: true,\n        user: {\n          ...decoded,\n          isActive: user.is_active,\n          emailVerified: user.email_verified\n        }\n      };\n\n    } catch (error) {\n      logger.debug('Token verification failed', { error: error.message });\n      return {\n        valid: false,\n        error: error.message\n      };\n    }\n  }\n\n  /**\n     * Refresh access token using refresh token\n     */\n  async refreshAccessToken(refreshToken, ipAddress, userAgent) {\n    try {\n      // Verify refresh token\n      const verification = await this.verifyToken(refreshToken, 'refresh');\n      if (!verification.valid) {\n        throw new Error('Invalid refresh token');\n      }\n\n      // Get current user data\n      const { data: user } = await supabase\n        .from('user_profiles')\n        .select('*')\n        .eq('id', verification.user.userId)\n        .single();\n\n      if (!user) {\n        throw new Error('User not found');\n      }\n\n      // Generate new access token\n      const newAccessToken = this.generateJWT(user, 'access');\n\n      // Log token refresh\n      await this.logSecurityEvent(user.id, 'token_refreshed', 'info', \n        'Access token refreshed', ipAddress, userAgent);\n\n      return {\n        success: true,\n        accessToken: newAccessToken,\n        expiresIn: this.tokenExpiry\n      };\n\n    } catch (error) {\n      logger.error('Token refresh failed', { error: error.message });\n      return {\n        success: false,\n        error: error.message\n      };\n    }\n  }\n\n  /**\n     * Validate user registration data\n     */\n  async validateRegistrationData(userData) {\n    const { email, password, firstName, lastName } = userData;\n\n    // Email validation\n    if (!this.isValidEmail(email)) {\n      throw new Error('Please provide a valid email address');\n    }\n\n    // Password validation\n    const passwordValidation = this.validatePassword(password, userData);\n    if (!passwordValidation.valid) {\n      throw new Error(passwordValidation.message);\n    }\n\n    // Name validation\n    if (!firstName || firstName.length < 1 || firstName.length > 50) {\n      throw new Error('First name must be between 1 and 50 characters');\n    }\n\n    if (!lastName || lastName.length < 1 || lastName.length > 50) {\n      throw new Error('Last name must be between 1 and 50 characters');\n    }\n\n    // Check for disposable email domains\n    if (await this.isDisposableEmail(email)) {\n      throw new Error('Disposable email addresses are not allowed');\n    }\n  }\n\n  /**\n     * Comprehensive password validation\n     */\n  validatePassword(password, userData = {}) {\n    const policy = this.passwordPolicy;\n    const issues = [];\n\n    // Length check\n    if (password.length < policy.minLength) {\n      issues.push(`Password must be at least ${policy.minLength} characters long`);\n    }\n    if (password.length > policy.maxLength) {\n      issues.push(`Password must be no more than ${policy.maxLength} characters long`);\n    }\n\n    // Character requirements\n    if (policy.requireUppercase && !/[A-Z]/.test(password)) {\n      issues.push('Password must contain at least one uppercase letter');\n    }\n    if (policy.requireLowercase && !/[a-z]/.test(password)) {\n      issues.push('Password must contain at least one lowercase letter');\n    }\n    if (policy.requireNumbers && !/\\d/.test(password)) {\n      issues.push('Password must contain at least one number');\n    }\n    if (policy.requireSpecialChars && !/[!@#$%^&*(),.?\":{}|<>]/.test(password)) {\n      issues.push('Password must contain at least one special character');\n    }\n\n    // Common password check\n    if (policy.preventCommonPasswords && this.isCommonPassword(password)) {\n      issues.push('This password is too common. Please choose a more unique password');\n    }\n\n    // User info in password check\n    if (policy.preventUserInfoInPassword && this.containsUserInfo(password, userData)) {\n      issues.push('Password should not contain your personal information');\n    }\n\n    return {\n      valid: issues.length === 0,\n      message: issues.join('. '),\n      issues\n    };\n  }\n\n  /**\n     * Check if password contains user information\n     */\n  containsUserInfo(password, userData) {\n    const lowerPassword = password.toLowerCase();\n    const { email, firstName, lastName } = userData;\n        \n    if (email && lowerPassword.includes(email.split('@')[0].toLowerCase())) {\n      return true;\n    }\n    if (firstName && lowerPassword.includes(firstName.toLowerCase())) {\n      return true;\n    }\n    if (lastName && lowerPassword.includes(lastName.toLowerCase())) {\n      return true;\n    }\n        \n    return false;\n  }\n\n  /**\n     * Check against common passwords\n     */\n  isCommonPassword(password) {\n    const commonPasswords = [\n      'password', '123456', '123456789', 'qwerty', 'abc123',\n      'password123', 'admin', 'letmein', 'welcome', '123123',\n      'password1', 'admin123', 'root', 'toor', 'pass'\n    ];\n        \n    return commonPasswords.includes(password.toLowerCase());\n  }\n\n  /**\n     * Check for disposable email domains\n     */\n  async isDisposableEmail(email) {\n    const domain = email.split('@')[1]?.toLowerCase();\n    const disposableDomains = [\n      '10minutemail.com', 'tempmail.org', 'guerrillamail.com',\n      'mailinator.com', 'throwaway.email', 'temp-mail.org'\n    ];\n        \n    return disposableDomains.includes(domain);\n  }\n\n  /**\n     * Generate secure random secret\n     */\n  generateSecureSecret(length = 64) {\n    return crypto.randomBytes(length).toString('hex');\n  }\n\n  /**\n     * Get user permissions based on role\n     */\n  getUserPermissions(role) {\n    const rolePermissions = {\n      'admin': [\n        'users:*', 'projects:*', 'scrapers:*', 'data:*', \n        'settings:*', 'analytics:*', 'system:*'\n      ],\n      'manager': [\n        'projects:*', 'scrapers:*', 'data:*', \n        'analytics:read', 'users:read'\n      ],\n      'user': [\n        'projects:read', 'projects:create', 'projects:update',\n        'scrapers:read', 'scrapers:create', 'scrapers:execute',\n        'data:read', 'data:export'\n      ],\n      'viewer': [\n        'projects:read', 'scrapers:read', 'data:read'\n      ]\n    };\n\n    return rolePermissions[role] || rolePermissions['viewer'];\n  }\n\n  /**\n     * Check authorization for resource and action\n     */\n  async authorize(user, resource, action) {\n    try {\n      const userPermissions = this.getUserPermissions(user.role);\n            \n      // Check for wildcard permissions\n      if (userPermissions.includes(`${resource}:*`) || userPermissions.includes('*:*')) {\n        return true;\n      }\n            \n      // Check for specific permission\n      return userPermissions.includes(`${resource}:${action}`);\n            \n    } catch (error) {\n      logger.error('Authorization check failed', { error: error.message, user: user.userId });\n      return false;\n    }\n  }\n\n  /**\n     * Generate email verification token\n     */\n  async generateVerificationToken(userId) {\n    const token = crypto.randomBytes(32).toString('hex');\n    const expiresAt = new Date(Date.now() + 24 * 60 * 60 * 1000); // 24 hours\n\n    await supabase\n      .from('verification_tokens')\n      .insert({\n        user_id: userId,\n        token: token,\n        expires_at: expiresAt.toISOString()\n      });\n\n    return token;\n  }\n\n  /**\n     * Store refresh token securely\n     */\n  async storeRefreshToken(userId, refreshToken, ipAddress, userAgent) {\n    // Hash the token before storing\n    const tokenHash = crypto.createHash('sha256').update(refreshToken).digest('hex');\n        \n    await supabase\n      .from('user_sessions')\n      .insert({\n        user_id: userId,\n        token_hash: tokenHash,\n        ip_address: ipAddress,\n        user_agent: userAgent,\n        expires_at: new Date(Date.now() + 7 * 24 * 60 * 60 * 1000).toISOString()\n      });\n  }\n\n  /**\n     * Log security events\n     */\n  async logSecurityEvent(userId, eventType, severity, description, ipAddress, userAgent) {\n    try {\n      await supabase\n        .from('security_events')\n        .insert({\n          user_id: userId,\n          event_type: eventType,\n          severity: severity,\n          description: description,\n          ip_address: ipAddress,\n          user_agent: userAgent\n        });\n    } catch (error) {\n      logger.error('Failed to log security event', { error: error.message });\n    }\n  }\n\n  /**\n     * Additional utility methods for completeness\n     */\n  isValidEmail(email) {\n    const emailRegex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return emailRegex.test(email);\n  }\n\n  // Simple boolean helper used by unit tests\n  isStrongPassword(password) {\n    return this.validatePassword(password).valid;\n  }\n\n  async checkAccountLockout(email, ipAddress) {\n    // Implementation for account lockout checking\n    // This would check failed login attempts and implement progressive delays\n    return true;\n  }\n\n  async logFailedLoginAttempt(email, ipAddress, userAgent, reason) {\n    // Log failed login attempt\n    await supabase\n      .from('login_attempts')\n      .insert({\n        email: email,\n        success: false,\n        ip_address: ipAddress,\n        user_agent: userAgent,\n        failure_reason: reason\n      });\n  }\n\n  async updateLastLogin(userId, ipAddress) {\n    await supabase\n      .from('user_profiles')\n      .update({\n        last_login: new Date().toISOString(),\n        last_login_ip: ipAddress\n      })\n      .eq('id', userId);\n  }\n\n  generateTempToken(userId) {\n    // Generate temporary token for 2FA flow\n    return jwt.sign(\n      { userId, type: 'temp_2fa' },\n      this.jwtSecret,\n      { expiresIn: '5m' }\n    );\n  }\n\n  async verifyTwoFactorToken(userId, token) {\n    // Get user's 2FA secret\n    const { data: user } = await supabase\n      .from('user_profiles')\n      .select('two_factor_secret')\n      .eq('id', userId)\n      .single();\n\n    if (!user?.two_factor_secret) {\n      return false;\n    }\n\n    return authenticator.verify({\n      token: token,\n      secret: user.two_factor_secret\n    });\n  }\n\n  async isTokenBlacklisted(tokenId) {\n    // Check if token is in blacklist\n    const { data } = await supabase\n      .from('token_blacklist')\n      .select('id')\n      .eq('token_id', tokenId)\n      .single();\n\n    return !!data;\n  }\n\n  async sendVerificationEmail(email, token, firstName) {\n    const verificationUrl = `${process.env.FRONTEND_URL}/verify-email?token=${token}`;\n        \n    logger.info('Email verification sent', { email, verificationUrl });\n        \n    // TODO: Implement actual email service integration\n    // await emailService.sendVerificationEmail(email, verificationUrl, firstName);\n  }\n}\n\nmodule.exports = { AuthService };","usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\services\\auth\\credential-manager.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\services\\auth\\encryption-service.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\services\\captcha-handler.js","messages":[{"ruleId":"no-unused-vars","severity":1,"message":"'executionId' is assigned a value but never used.","line":208,"column":30,"nodeType":"Identifier","messageId":"unusedVar","endLine":208,"endColumn":41},{"ruleId":"no-unused-vars","severity":1,"message":"'executionId' is assigned a value but never used.","line":277,"column":29,"nodeType":"Identifier","messageId":"unusedVar","endLine":277,"endColumn":40},{"ruleId":"no-unused-vars","severity":1,"message":"'executionId' is assigned a value but never used.","line":286,"column":33,"nodeType":"Identifier","messageId":"unusedVar","endLine":286,"endColumn":44},{"ruleId":"no-unused-vars","severity":1,"message":"'executionId' is assigned a value but never used.","line":331,"column":32,"nodeType":"Identifier","messageId":"unusedVar","endLine":331,"endColumn":43},{"ruleId":"no-unused-vars","severity":1,"message":"'executionId' is assigned a value but never used.","line":370,"column":30,"nodeType":"Identifier","messageId":"unusedVar","endLine":370,"endColumn":41},{"ruleId":"no-unused-vars","severity":1,"message":"'page' is defined but never used.","line":511,"column":30,"nodeType":"Identifier","messageId":"unusedVar","endLine":511,"endColumn":34},{"ruleId":"no-unused-vars","severity":1,"message":"'captchaType' is defined but never used.","line":511,"column":36,"nodeType":"Identifier","messageId":"unusedVar","endLine":511,"endColumn":47}],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":7,"fixableErrorCount":0,"fixableWarningCount":0,"source":"∩╗┐const { AIService } = require('./ai-service');\nconst { createClient } = require('@supabase/supabase-js');\n\nclass CaptchaHandler {\n  constructor() {\n    this.aiService = new AIService();\n    try {\n      const url = process.env.SUPABASE_URL;\n      if (!url || url === 'your_supabase_url_here' || url.trim() === '') {\n        this.supabase = require('../core/supabase').supabase;\n      } else {\n        try {\n          this.supabase = createClient(url, process.env.SUPABASE_SERVICE_KEY);\n        } catch (e) {\n          console.warn('CaptchaHandler: Supabase init failed, using stub', e && e.message);\n          this.supabase = require('../core/supabase').supabase;\n        }\n      }\n    } catch (e) {\n      console.warn('CaptchaHandler: unexpected supabase init error, using stub', e && e.message);\n      this.supabase = require('../core/supabase').supabase;\n    }\n    this.captchaServices = new Map();\n    this.setupCaptchaServices();\n  }\n\n  setupCaptchaServices() {\n    // Integration with captcha solving services\n    if (process.env.TWO_CAPTCHA_API_KEY) {\n      this.captchaServices.set('2captcha', {\n        name: '2captcha',\n        apiKey: process.env.TWO_CAPTCHA_API_KEY,\n        solve: this.solveWith2Captcha.bind(this),\n        baseUrl: 'http://2captcha.com',\n        supportedTypes: ['recaptcha', 'hcaptcha', 'image', 'text']\n      });\n    }\n    \n    if (process.env.ANTI_CAPTCHA_API_KEY) {\n      this.captchaServices.set('anticaptcha', {\n        name: 'anticaptcha',\n        apiKey: process.env.ANTI_CAPTCHA_API_KEY,\n        solve: this.solveWithAntiCaptcha.bind(this),\n        baseUrl: 'https://api.anti-captcha.com',\n        supportedTypes: ['recaptcha', 'hcaptcha', 'image']\n      });\n    }\n\n    console.log(`├░┼╕ΓÇ¥┬É Initialized ${this.captchaServices.size} CAPTCHA solving services`);\n  }\n\n  async detectCaptcha(page, executionId = null) {\n    const startTime = Date.now();\n    let detectionMethod = 'dom_selector';\n    \n    try {\n      console.log('├░┼╕ΓÇ¥┬ì Detecting CAPTCHA on page...');\n\n      // Check for common CAPTCHA indicators via DOM selectors\n      const captchaIndicators = [\n        { selector: 'iframe[src*=\"recaptcha\"]', type: 'recaptcha' },\n        { selector: 'iframe[src*=\"hcaptcha\"]', type: 'hcaptcha' },\n        { selector: '.g-recaptcha', type: 'recaptcha' },\n        { selector: '.h-captcha', type: 'hcaptcha' },\n        { selector: 'div[class*=\"captcha\"]', type: 'generic_captcha' },\n        { selector: 'img[src*=\"captcha\"]', type: 'image_captcha' },\n        { selector: 'input[name*=\"captcha\"]', type: 'text_captcha' },\n        { selector: '[data-sitekey]', type: 'recaptcha' },\n        { selector: '.cf-turnstile', type: 'turnstile' }\n      ];\n\n      for (const indicator of captchaIndicators) {\n        const element = await page.$(indicator.selector);\n        if (element) {\n          const boundingBox = await element.boundingBox();\n          const isVisible = boundingBox && boundingBox.width > 0 && boundingBox.height > 0;\n          \n          if (isVisible) {\n            console.log(`├ó┼ôΓÇª CAPTCHA detected via DOM: ${indicator.type}`);\n            \n            if (executionId) {\n              await this.logCaptchaDetection(executionId, indicator.type, detectionMethod, true, Date.now() - startTime);\n            }\n            \n            return { \n              detected: true, \n              type: indicator.type,\n              method: detectionMethod,\n              element: element,\n              confidence: 0.9 \n            };\n          }\n        }\n      }\n\n      // Use AI to detect CAPTCHA in screenshots if DOM detection fails\n      console.log('├░┼╕┬ñΓÇô Using AI vision for CAPTCHA detection...');\n      detectionMethod = 'ai_vision';\n      \n      const screenshot = await page.screenshot({ \n        type: 'png',\n        clip: { x: 0, y: 0, width: 1200, height: 800 } // Focus on main content area\n      });\n      \n      const aiDetection = await this.detectCaptchaWithAI(screenshot);\n      \n      if (executionId) {\n        await this.logCaptchaDetection(executionId, aiDetection.type, detectionMethod, aiDetection.detected, Date.now() - startTime);\n      }\n      \n      return {\n        ...aiDetection,\n        method: detectionMethod\n      };\n\n    } catch (error) {\n      console.error('├ó┬¥┼Æ Error detecting CAPTCHA:', error);\n      \n      if (executionId) {\n        await this.logCaptchaDetection(executionId, 'unknown', detectionMethod, false, Date.now() - startTime, error.message);\n      }\n      \n      return { detected: false, type: 'unknown', confidence: 0, error: error.message };\n    }\n  }\n\n  async detectCaptchaWithAI(imageBuffer) {\n    try {\n      const prompt = `\n        Analyze this web page screenshot and determine if there's a CAPTCHA present.\n        Look for:\n        - reCAPTCHA checkboxes (\"I'm not a robot\")\n        - hCaptcha challenges\n        - Image selection CAPTCHAs (\"Select all traffic lights\")\n        - Text-based CAPTCHAs\n        - Cloudflare Turnstile\n        - Any other bot verification challenges\n        \n        Return JSON format only:\n        {\n          \"detected\": boolean,\n          \"type\": \"recaptcha|hcaptcha|image_captcha|text_captcha|turnstile|unknown\",\n          \"confidence\": number (0-1),\n          \"description\": \"brief description of what you see\"\n        }\n      `;\n\n      const analysis = await this.aiService.analyzeWithGPT4V(imageBuffer, prompt);\n      \n      try {\n        const result = JSON.parse(analysis);\n        console.log(`├░┼╕┬ñΓÇô AI CAPTCHA detection: ${result.detected ? 'DETECTED' : 'NOT DETECTED'} (${result.confidence * 100}% confidence)`);\n        return result;\n      } catch (parseError) {\n        console.error('├ó┬¥┼Æ Failed to parse AI CAPTCHA detection response:', analysis);\n        return { detected: false, type: 'unknown', confidence: 0 };\n      }\n    } catch (error) {\n      console.error('├ó┬¥┼Æ AI CAPTCHA detection failed:', error);\n      return { detected: false, type: 'unknown', confidence: 0, error: error.message };\n    }\n  }\n\n  async handleCaptcha(page, captchaInfo, executionId = null) {\n    const startTime = Date.now();\n    \n    try {\n      console.log(`├░┼╕ΓÇ¥┬É Handling ${captchaInfo.type} CAPTCHA...`);\n\n      let result;\n      switch (captchaInfo.type) {\n      case 'recaptcha':\n        result = await this.solveRecaptcha(page, executionId);\n        break;\n      case 'hcaptcha':\n        result = await this.solveHCaptcha(page, executionId);\n        break;\n      case 'image_captcha':\n        result = await this.solveImageCaptcha(page, executionId);\n        break;\n      case 'text_captcha':\n        result = await this.solveTextCaptcha(page, executionId);\n        break;\n      case 'turnstile':\n        result = await this.solveTurnstile(page, executionId);\n        break;\n      default:\n        result = await this.solveGenericCaptcha(page, executionId);\n      }\n\n      if (executionId) {\n        await this.logCaptchaSolving(executionId, captchaInfo.type, result.method, Date.now() - startTime, result.success, result.cost, result.error);\n      }\n\n      return result;\n\n    } catch (error) {\n      console.error(`├ó┬¥┼Æ CAPTCHA handling failed: ${error.message}`);\n      \n      if (executionId) {\n        await this.logCaptchaSolving(executionId, captchaInfo.type, 'error', Date.now() - startTime, false, 0, error.message);\n      }\n      \n      return { success: false, method: 'error', error: error.message };\n    }\n  }\n\n  async solveRecaptcha(page, executionId = null) {\n    try {\n      console.log('├░┼╕ΓÇ¥ΓÇ₧ Attempting reCAPTCHA bypass...');\n\n      // First, try automated bypass techniques\n      const bypassResult = await this.bypassRecaptchaAutomatically(page);\n      if (bypassResult.success) {\n        return bypassResult;\n      }\n\n      // If automated bypass fails, use solving service\n      console.log('├░┼╕ΓÇ¥┬º Using CAPTCHA solving service for reCAPTCHA...');\n      return await this.solveWithCaptchaService(page, 'recaptcha');\n\n    } catch (error) {\n      console.error('├ó┬¥┼Æ reCAPTCHA solving failed:', error);\n      return { success: false, method: 'service_error', error: error.message };\n    }\n  }\n\n  async bypassRecaptchaAutomatically(page) {\n    try {\n      // Method 1: Simulate natural human behavior\n      await page.evaluate(() => {\n        // Dispatch realistic mouse and keyboard events\n        const events = ['mousemove', 'mousedown', 'mouseup', 'click'];\n        events.forEach(eventType => {\n          const event = new MouseEvent(eventType, {\n            bubbles: true,\n            cancelable: true,\n            clientX: Math.random() * window.innerWidth,\n            clientY: Math.random() * window.innerHeight\n          });\n          document.dispatchEvent(event);\n        });\n\n        // Focus and blur events\n        window.dispatchEvent(new Event('focus'));\n        setTimeout(() => window.dispatchEvent(new Event('blur')), 100);\n      });\n\n      // Method 2: Check for and click the reCAPTCHA checkbox\n      const recaptchaFrame = await page.frameLocator('iframe[src*=\"recaptcha\"]').first();\n      const checkbox = recaptchaFrame.locator('#recaptcha-anchor');\n      \n      if (await checkbox.isVisible()) {\n        // Simulate human-like clicking with random delay\n        await page.waitForTimeout(Math.random() * 2000 + 1000);\n        await checkbox.click();\n        \n        // Wait for verification\n        await page.waitForTimeout(3000);\n        \n        // Check if bypass worked\n        const stillPresent = await this.detectCaptcha(page);\n        if (!stillPresent.detected) {\n          console.log('├ó┼ôΓÇª reCAPTCHA bypassed automatically');\n          return { success: true, method: 'automated_bypass', cost: 0 };\n        }\n      }\n\n      return { success: false, method: 'automated_bypass' };\n\n    } catch (error) {\n      console.error('├ó┬¥┼Æ Automated reCAPTCHA bypass failed:', error);\n      return { success: false, method: 'automated_bypass', error: error.message };\n    }\n  }\n\n  async solveHCaptcha(page, executionId = null) {\n    try {\n      console.log('├░┼╕ΓÇ¥┬º Solving hCaptcha with service...');\n      return await this.solveWithCaptchaService(page, 'hcaptcha');\n    } catch (error) {\n      return { success: false, method: 'service_error', error: error.message };\n    }\n  }\n\n  async solveImageCaptcha(page, executionId = null) {\n    try {\n      // Take screenshot of CAPTCHA image\n      const captchaElement = await page.$('img[src*=\"captcha\"], .captcha-image, [class*=\"captcha\"] img');\n      if (!captchaElement) {\n        throw new Error('CAPTCHA image not found');\n      }\n\n      const screenshot = await captchaElement.screenshot();\n      \n      // Use AI to solve image CAPTCHA\n      const solution = await this.solveImageCaptchaWithAI(screenshot);\n      \n      if (solution) {\n        // Find input field and enter solution\n        const inputField = await page.$('input[name*=\"captcha\"], input[id*=\"captcha\"], .captcha-input');\n        if (inputField) {\n          await inputField.fill(solution);\n          console.log('├ó┼ôΓÇª Image CAPTCHA solved with AI');\n          return { success: true, method: 'ai_ocr', cost: 0 };\n        }\n      }\n\n      return { success: false, method: 'ai_ocr', error: 'Could not solve image CAPTCHA' };\n\n    } catch (error) {\n      return { success: false, method: 'ai_ocr', error: error.message };\n    }\n  }\n\n  async solveImageCaptchaWithAI(imageBuffer) {\n    try {\n      const prompt = `\n        This is a CAPTCHA image. Please read the text/numbers shown in the image.\n        Return only the text/numbers you see, nothing else.\n      `;\n\n      const result = await this.aiService.analyzeWithGPT4V(imageBuffer, prompt);\n      return result.trim();\n    } catch (error) {\n      console.error('├ó┬¥┼Æ AI image CAPTCHA solving failed:', error);\n      return null;\n    }\n  }\n\n  async solveTextCaptcha(page, executionId = null) {\n    try {\n      // Similar to image CAPTCHA but for text-based challenges\n      const textElement = await page.$('.captcha-question, [class*=\"captcha\"] .question');\n      if (textElement) {\n        const question = await textElement.textContent();\n        const answer = await this.solveTextCaptchaWithAI(question);\n        \n        if (answer) {\n          const inputField = await page.$('input[name*=\"captcha\"], input[id*=\"captcha\"]');\n          if (inputField) {\n            await inputField.fill(answer);\n            return { success: true, method: 'ai_text', cost: 0 };\n          }\n        }\n      }\n\n      return { success: false, method: 'ai_text', error: 'Could not solve text CAPTCHA' };\n    } catch (error) {\n      return { success: false, method: 'ai_text', error: error.message };\n    }\n  }\n\n  async solveTextCaptchaWithAI(question) {\n    try {\n      const prompt = `\n        Solve this CAPTCHA question: \"${question}\"\n        This is typically a simple math problem or basic question.\n        Return only the answer, nothing else.\n      `;\n\n      const result = await this.aiService.callClaude(prompt);\n      return result.trim();\n    } catch (error) {\n      console.error('├ó┬¥┼Æ AI text CAPTCHA solving failed:', error);\n      return null;\n    }\n  }\n\n  async solveTurnstile(page, executionId = null) {\n    try {\n      // Cloudflare Turnstile typically requires waiting\n      console.log('├ó┬Å┬│ Waiting for Turnstile to complete...');\n      \n      await page.waitForTimeout(5000);\n      \n      // Check if Turnstile completed automatically\n      const stillPresent = await this.detectCaptcha(page);\n      if (!stillPresent.detected) {\n        return { success: true, method: 'wait', cost: 0 };\n      }\n\n      return { success: false, method: 'wait', error: 'Turnstile did not complete automatically' };\n    } catch (error) {\n      return { success: false, method: 'wait', error: error.message };\n    }\n  }\n\n  async solveGenericCaptcha(page, executionId = null) {\n    try {\n      // Try multiple approaches for unknown CAPTCHA types\n      const approaches = [\n        () => this.bypassRecaptchaAutomatically(page),\n        () => this.solveImageCaptcha(page, executionId),\n        () => this.solveTextCaptcha(page, executionId)\n      ];\n\n      for (const approach of approaches) {\n        const result = await approach();\n        if (result.success) {\n          return result;\n        }\n      }\n\n      return { success: false, method: 'generic', error: 'All generic approaches failed' };\n    } catch (error) {\n      return { success: false, method: 'generic', error: error.message };\n    }\n  }\n\n  async solveWithCaptchaService(page, captchaType) {\n    // Get first available service that supports this CAPTCHA type\n    const availableService = Array.from(this.captchaServices.values())\n      .find(service => service.supportedTypes.includes(captchaType));\n\n    if (!availableService) {\n      throw new Error(`No CAPTCHA solving service available for type: ${captchaType}`);\n    }\n\n    try {\n      console.log(`├░┼╕ΓÇ¥┬º Using ${availableService.name} for ${captchaType}`);\n      const solution = await availableService.solve(page, captchaType);\n      \n      if (solution.success) {\n        console.log(`├ó┼ôΓÇª CAPTCHA solved using ${availableService.name}`);\n        return solution;\n      } else {\n        throw new Error(`CAPTCHA solving failed: ${solution.error}`);\n      }\n    } catch (error) {\n      throw new Error(`CAPTCHA service error: ${error.message}`);\n    }\n  }\n\n  async solveWith2Captcha(page, captchaType) {\n    try {\n      const siteKey = await this.extractSiteKey(page);\n      const pageUrl = page.url();\n\n      if (!siteKey) {\n        throw new Error('Could not extract site key');\n      }\n\n      // Submit CAPTCHA to 2captcha\n      const submitResponse = await fetch('http://2captcha.com/in.php', {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/x-www-form-urlencoded' },\n        body: new URLSearchParams({\n          key: this.captchaServices.get('2captcha').apiKey,\n          method: captchaType === 'hcaptcha' ? 'hcaptcha' : 'userrecaptcha',\n          googlekey: siteKey,\n          pageurl: pageUrl,\n          json: 1\n        })\n      });\n\n      const submitData = await submitResponse.json();\n      \n      if (submitData.status !== 1) {\n        throw new Error(`2Captcha submission error: ${submitData.request}`);\n      }\n\n      const captchaId = submitData.request;\n      \n      // Poll for solution\n      for (let i = 0; i < 30; i++) {\n        await new Promise(resolve => setTimeout(resolve, 5000));\n        \n        const resultResponse = await fetch(\n          `http://2captcha.com/res.php?key=${this.captchaServices.get('2captcha').apiKey}&action=get&id=${captchaId}&json=1`\n        );\n        \n        const result = await resultResponse.json();\n        \n        if (result.status === 1) {\n          // Submit the solution\n          await page.evaluate((token, type) => {\n            if (type === 'hcaptcha') {\n              const textarea = document.querySelector('[name=\"h-captcha-response\"]');\n              if (textarea) {\n                textarea.innerHTML = token;\n                textarea.dispatchEvent(new Event('change'));\n              }\n            } else {\n              const textarea = document.querySelector('[name=\"g-recaptcha-response\"]');\n              if (textarea) {\n                textarea.innerHTML = token;\n                textarea.dispatchEvent(new Event('change'));\n              }\n            }\n          }, result.request, captchaType);\n          \n          return { \n            success: true, \n            token: result.request, \n            method: '2captcha',\n            cost: 0.002 // Approximate cost\n          };\n        } else if (result.status === 0 && result.request !== 'CAPCHA_NOT_READY') {\n          throw new Error(`2Captcha error: ${result.request}`);\n        }\n      }\n\n      throw new Error('CAPTCHA solving timeout');\n\n    } catch (error) {\n      throw new Error(`2Captcha error: ${error.message}`);\n    }\n  }\n\n  async solveWithAntiCaptcha(page, captchaType) {\n    // Similar implementation for AntiCaptcha service\n    // This would follow AntiCaptcha's API specification\n    throw new Error('AntiCaptcha integration not yet implemented');\n  }\n\n  async extractSiteKey(page) {\n    return await page.evaluate(() => {\n      // Look for reCAPTCHA site key\n      const recaptchaDiv = document.querySelector('.g-recaptcha, [data-sitekey]');\n      if (recaptchaDiv) {\n        return recaptchaDiv.getAttribute('data-sitekey');\n      }\n\n      // Look for hCaptcha site key\n      const hcaptchaDiv = document.querySelector('.h-captcha, [data-sitekey]');\n      if (hcaptchaDiv) {\n        return hcaptchaDiv.getAttribute('data-sitekey');\n      }\n\n      // Look in script tags\n      const scripts = Array.from(document.querySelectorAll('script'));\n      for (const script of scripts) {\n        const content = script.textContent || script.innerHTML;\n        const siteKeyMatch = content.match(/['\"](6[0-9A-Za-z_-]{39})['\"]/);\n        if (siteKeyMatch) {\n          return siteKeyMatch[1];\n        }\n      }\n\n      return null;\n    });\n  }\n\n  async logCaptchaDetection(executionId, captchaType, detectionMethod, success, duration, errorMessage = null) {\n    try {\n      await this.supabase\n        .from('captcha_logs')\n        .insert([{\n          execution_id: executionId,\n          captcha_type: captchaType,\n          detection_method: detectionMethod,\n          solving_method: null,\n          solving_duration_ms: duration,\n          success: success,\n          cost_usd: 0,\n          error_message: errorMessage,\n          confidence_score: success ? 0.9 : 0.0\n        }]);\n    } catch (error) {\n      console.error('Error logging CAPTCHA detection:', error);\n    }\n  }\n\n  async logCaptchaSolving(executionId, captchaType, solvingMethod, duration, success, cost = 0, errorMessage = null) {\n    try {\n      await this.supabase\n        .from('captcha_logs')\n        .insert([{\n          execution_id: executionId,\n          captcha_type: captchaType,\n          detection_method: 'logged_separately',\n          solving_method: solvingMethod,\n          solving_duration_ms: duration,\n          success: success,\n          cost_usd: cost,\n          error_message: errorMessage,\n          confidence_score: success ? 0.9 : 0.0\n        }]);\n    } catch (error) {\n      console.error('Error logging CAPTCHA solving:', error);\n    }\n  }\n\n  async getCaptchaStats(templateId = null, days = 7) {\n    try {\n      let query = this.supabase\n        .from('captcha_logs')\n        .select(`\n          captcha_type,\n          detection_method,\n          solving_method,\n          success,\n          cost_usd,\n          solving_duration_ms,\n          created_at\n        `)\n        .gte('created_at', new Date(Date.now() - days * 24 * 60 * 60 * 1000).toISOString());\n\n      if (templateId) {\n        query = query.eq('execution.template_id', templateId);\n      }\n\n      const { data, error } = await query;\n      if (error) throw error;\n\n      const stats = {\n        total: data.length,\n        successful: data.filter(log => log.success).length,\n        failed: data.filter(log => !log.success).length,\n        totalCost: data.reduce((sum, log) => sum + (log.cost_usd || 0), 0),\n        avgSolvingTime: data.reduce((sum, log) => sum + (log.solving_duration_ms || 0), 0) / data.length,\n        byType: {},\n        byMethod: {}\n      };\n\n      // Group by type\n      data.forEach(log => {\n        stats.byType[log.captcha_type] = (stats.byType[log.captcha_type] || 0) + 1;\n      });\n\n      // Group by solving method\n      data.forEach(log => {\n        if (log.solving_method) {\n          stats.byMethod[log.solving_method] = (stats.byMethod[log.solving_method] || 0) + 1;\n        }\n      });\n\n      return stats;\n    } catch (error) {\n      console.error('Error getting CAPTCHA stats:', error);\n      throw error;\n    }\n  }\n}\n\nmodule.exports = { CaptchaHandler };","usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\services\\code-generator.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\services\\compliance-manager.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\services\\core\\logger.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\services\\core\\supabase.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\services\\data-processor.js","messages":[{"ruleId":"no-unused-vars","severity":1,"message":"'format' is defined but never used.","line":315,"column":25,"nodeType":"Identifier","messageId":"unusedVar","endLine":315,"endColumn":31},{"ruleId":"no-unused-vars","severity":1,"message":"'config' is assigned a value but never used.","line":527,"column":29,"nodeType":"Identifier","messageId":"unusedVar","endLine":527,"endColumn":35},{"ruleId":"no-unused-vars","severity":1,"message":"'fieldConfig' is defined but never used.","line":544,"column":42,"nodeType":"Identifier","messageId":"unusedVar","endLine":544,"endColumn":53}],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":3,"fixableErrorCount":0,"fixableWarningCount":0,"source":"∩╗┐const { createClient } = require('@supabase/supabase-js');\n\nclass DataProcessor {\n  constructor() {\n    try {\n      const url = process.env.SUPABASE_URL;\n      if (!url || url === 'your_supabase_url_here' || url.trim() === '') {\n        this.supabase = require('../core/supabase').supabase;\n      } else {\n        try {\n          this.supabase = createClient(url, process.env.SUPABASE_SERVICE_KEY);\n        } catch (e) {\n          console.warn('DataProcessor: Supabase init failed, using stub', e && e.message);\n          this.supabase = require('../core/supabase').supabase;\n        }\n      }\n    } catch (e) {\n      console.warn('DataProcessor: unexpected supabase init error, using stub', e && e.message);\n      this.supabase = require('../core/supabase').supabase;\n    }\n    this.validators = new Map();\n    this.normalizers = new Map();\n    this.transformers = new Map();\n    this.geocodeCache = new Map();\n    \n    this.setupDefaultProcessors();\n  }\n\n  setupDefaultProcessors() {\n    // Price normalization and validation\n    this.normalizers.set('price', this.normalizePrice.bind(this));\n    this.validators.set('price', this.validatePrice.bind(this));\n    \n    // Date normalization and validation\n    this.normalizers.set('date', this.normalizeDate.bind(this));\n    this.validators.set('date', this.validateDate.bind(this));\n    \n    // Phone number normalization and validation\n    this.normalizers.set('phone', this.normalizePhone.bind(this));\n    this.validators.set('phone', this.validatePhone.bind(this));\n    \n    // Email validation and normalization\n    this.validators.set('email', this.validateEmail.bind(this));\n    this.normalizers.set('email', this.normalizeEmail.bind(this));\n    \n    // URL validation and normalization\n    this.validators.set('url', this.validateUrl.bind(this));\n    this.normalizers.set('url', this.normalizeUrl.bind(this));\n\n    // Text processing\n    this.normalizers.set('text', this.normalizeText.bind(this));\n    this.validators.set('text', this.validateText.bind(this));\n\n    // Number processing\n    this.normalizers.set('number', this.normalizeNumber.bind(this));\n    this.validators.set('number', this.validateNumber.bind(this));\n\n    // Address processing\n    this.normalizers.set('address', this.normalizeAddress.bind(this));\n    this.transformers.set('geocode', this.geocodeAddress.bind(this));\n\n    console.log('├░┼╕ΓÇ£┼á Data processors initialized successfully');\n  }\n\n  async processScrapedData(rawData, schema, options = {}) {\n    const startTime = Date.now();\n    \n    try {\n      console.log('├░┼╕ΓÇ¥ΓÇ₧ Processing scraped data with schema validation...');\n\n      const processed = {};\n      const errors = [];\n      const warnings = [];\n      const transformations = [];\n\n      // Process each field according to schema\n      for (const [field, value] of Object.entries(rawData)) {\n        try {\n          const fieldConfig = schema.fields?.[field] || {};\n          \n          // Skip processing if field not in schema and strictMode is enabled\n          if (options.strictMode && !fieldConfig.type) {\n            warnings.push({\n              field: field,\n              warning: 'Field not defined in schema (skipped in strict mode)'\n            });\n            continue;\n          }\n\n          // Clean the value first\n          let cleanedValue = this.cleanValue(value, fieldConfig);\n          \n          // Apply transformations\n          if (fieldConfig.transforms) {\n            for (const transform of fieldConfig.transforms) {\n              cleanedValue = await this.applyTransform(cleanedValue, transform, fieldConfig);\n              transformations.push({\n                field: field,\n                transform: transform.type,\n                original: value,\n                result: cleanedValue\n              });\n            }\n          }\n\n          // Validate if validator exists\n          if (fieldConfig.type && this.validators.has(fieldConfig.type)) {\n            const validator = this.validators.get(fieldConfig.type);\n            const isValid = validator(cleanedValue, fieldConfig);\n            \n            if (!isValid) {\n              throw new Error(`Validation failed for field '${field}' of type '${fieldConfig.type}'`);\n            }\n          }\n\n          // Normalize if normalizer exists\n          if (fieldConfig.type && this.normalizers.has(fieldConfig.type)) {\n            const normalizer = this.normalizers.get(fieldConfig.type);\n            cleanedValue = await normalizer(cleanedValue, fieldConfig);\n          }\n\n          processed[field] = cleanedValue;\n\n        } catch (error) {\n          errors.push({\n            field: field,\n            value: this.sanitizeValue(value),\n            error: error.message,\n            type: 'processing_error'\n          });\n          \n          // Apply fallback if specified\n          if (schema.fields?.[field]?.fallback !== undefined) {\n            processed[field] = schema.fields[field].fallback;\n            warnings.push({\n              field: field,\n              warning: `Using fallback value due to error: ${error.message}`\n            });\n          }\n        }\n      }\n\n      // Check for required fields\n      if (schema.required) {\n        for (const requiredField of schema.required) {\n          if (!(requiredField in processed) || processed[requiredField] === null || processed[requiredField] === undefined) {\n            errors.push({\n              field: requiredField,\n              error: 'Required field is missing or null',\n              type: 'required_field_error'\n            });\n          }\n        }\n      }\n\n      // Remove duplicates if needed\n      if (schema.deduplicate && options.deduplicate !== false) {\n        const duplicateCheck = await this.checkForDuplicates(processed, schema.deduplicate);\n        if (duplicateCheck.isDuplicate) {\n          errors.push({\n            field: schema.deduplicate.key,\n            error: `Duplicate record found: ${duplicateCheck.duplicateValue}`,\n            type: 'duplicate_error',\n            existingId: duplicateCheck.existingId\n          });\n        }\n      }\n\n      // Calculate quality scores\n      const qualityMetrics = this.calculateQualityMetrics(processed, rawData, schema, errors, warnings);\n\n      const processingTime = Date.now() - startTime;\n\n      const result = {\n        data: processed,\n        originalData: rawData,\n        errors: errors,\n        warnings: warnings,\n        transformations: transformations,\n        qualityMetrics: qualityMetrics,\n        valid: errors.filter(e => e.type !== 'duplicate_error').length === 0,\n        processingTime: processingTime,\n        processedAt: new Date().toISOString()\n      };\n\n      console.log(`├ó┼ôΓÇª Data processing completed in ${processingTime}ms (Quality: ${(qualityMetrics.overall * 100).toFixed(1)}%)`);\n      return result;\n\n    } catch (error) {\n      console.error('├ó┬¥┼Æ Data processing failed:', error);\n      throw error;\n    }\n  }\n\n  cleanValue(value, config = {}) {\n    if (value === null || value === undefined) {\n      return config.default !== undefined ? config.default : null;\n    }\n\n    // Convert to string for processing\n    let cleaned = String(value).trim();\n\n    // Remove extra whitespace\n    if (config.collapseWhitespace !== false) {\n      cleaned = cleaned.replace(/\\s+/g, ' ');\n    }\n\n    // Strip HTML if requested\n    if (config.stripHtml) {\n      cleaned = cleaned.replace(/<[^>]*>/g, '');\n      cleaned = cleaned.replace(/&[a-zA-Z0-9#]+;/g, ' '); // Remove HTML entities\n    }\n\n    // Remove special characters if requested\n    if (config.removeSpecialChars) {\n      cleaned = cleaned.replace(/[^\\w\\s]/g, '');\n    }\n\n    // Apply length limits\n    if (config.maxLength && cleaned.length > config.maxLength) {\n      cleaned = cleaned.substring(0, config.maxLength);\n      if (config.truncateEllipsis) {\n        cleaned = cleaned.substring(0, config.maxLength - 3) + '...';\n      }\n    }\n\n    // Convert case\n    if (config.case === 'lower') {\n      cleaned = cleaned.toLowerCase();\n    } else if (config.case === 'upper') {\n      cleaned = cleaned.toUpperCase();\n    } else if (config.case === 'title') {\n      cleaned = cleaned.replace(/\\w\\S*/g, (txt) => \n        txt.charAt(0).toUpperCase() + txt.substr(1).toLowerCase()\n      );\n    }\n\n    return cleaned;\n  }\n\n  // Price processing\n  normalizePrice(price, config = {}) {\n    if (!price || price === '') return null;\n\n    // Extract numeric value from price strings\n    const priceString = String(price).replace(/,/g, '.');\n    const match = priceString.match(/([0-9]+[.]?[0-9]*)/);\n    \n    if (!match) return null;\n\n    let value = parseFloat(match[1]);\n    \n    // Handle currency conversion if needed\n    if (config.fromCurrency && config.toCurrency && config.exchangeRates) {\n      const rate = config.exchangeRates[`${config.fromCurrency}_${config.toCurrency}`];\n      if (rate) {\n        value = value * rate;\n      }\n    }\n\n    // Apply formatting\n    if (config.format === 'integer') {\n      value = Math.round(value);\n    } else if (config.decimalPlaces !== undefined) {\n      value = parseFloat(value.toFixed(config.decimalPlaces));\n    }\n\n    return value;\n  }\n\n  validatePrice(price, config = {}) {\n    if (price === null && !config.required) return true;\n    if (typeof price !== 'number' || isNaN(price)) return false;\n    \n    if (config.min !== undefined && price < config.min) return false;\n    if (config.max !== undefined && price > config.max) return false;\n    \n    return price >= 0; // Prices should generally be positive\n  }\n\n  // Date processing\n  normalizeDate(dateString, config = {}) {\n    if (!dateString || dateString === '') return null;\n\n    const formats = config.inputFormats || [\n      'YYYY-MM-DD',\n      'MM/DD/YYYY',\n      'DD/MM/YYYY',\n      'MMMM D, YYYY',\n      'D MMMM YYYY',\n      'YYYY-MM-DD HH:mm:ss',\n      'MM/DD/YYYY HH:mm:ss'\n    ];\n\n    // Try to parse with various formats\n    for (const format of formats) {\n      const parsed = this.parseDate(dateString, format);\n      if (parsed) {\n        if (config.outputFormat) {\n          return this.formatDate(parsed, config.outputFormat);\n        }\n        return parsed.toISOString();\n      }\n    }\n\n    // Try native Date parsing as last resort\n    const nativeDate = new Date(dateString);\n    if (!isNaN(nativeDate.getTime())) {\n      return nativeDate.toISOString();\n    }\n\n    throw new Error(`Unable to parse date: ${dateString}`);\n  }\n\n  parseDate(dateString, format) {\n    // Simple date parsing - could be enhanced with a proper date library\n    try {\n      const date = new Date(dateString);\n      return isNaN(date.getTime()) ? null : date;\n    } catch {\n      return null;\n    }\n  }\n\n  formatDate(date, format) {\n    // Simple date formatting - could be enhanced with a proper date library\n    if (format === 'ISO') return date.toISOString();\n    if (format === 'YYYY-MM-DD') return date.toISOString().split('T')[0];\n    return date.toISOString();\n  }\n\n  validateDate(date, config = {}) {\n    if (date === null && !config.required) return true;\n    \n    const dateObj = typeof date === 'string' ? new Date(date) : date;\n    if (!(dateObj instanceof Date) || isNaN(dateObj.getTime())) return false;\n\n    if (config.minDate) {\n      const minDate = new Date(config.minDate);\n      if (dateObj < minDate) return false;\n    }\n\n    if (config.maxDate) {\n      const maxDate = new Date(config.maxDate);\n      if (dateObj > maxDate) return false;\n    }\n\n    return true;\n  }\n\n  // Phone number processing\n  normalizePhone(phone, config = {}) {\n    if (!phone || phone === '') return null;\n\n    // Remove all non-digit characters\n    const digits = String(phone).replace(/\\D/g, '');\n\n    if (digits.length === 0) return null;\n\n    // Apply country-specific formatting\n    if (config.countryCode) {\n      return this.formatPhoneNumber(digits, config.countryCode);\n    }\n\n    // Default international format\n    if (digits.length >= 10) {\n      return `+${digits}`;\n    }\n\n    return digits;\n  }\n\n  formatPhoneNumber(digits, countryCode) {\n    // Simple phone formatting - could be enhanced with proper phone library\n    switch (countryCode.toUpperCase()) {\n    case 'US':\n    case 'CA':\n      if (digits.length === 10) {\n        return `+1${digits}`;\n      } else if (digits.length === 11 && digits[0] === '1') {\n        return `+${digits}`;\n      }\n      break;\n    case 'UK':\n      return `+44${digits}`;\n    default:\n      return `+${digits}`;\n    }\n    return digits;\n  }\n\n  validatePhone(phone, config = {}) {\n    if (phone === null && !config.required) return true;\n    if (typeof phone !== 'string') return false;\n\n    const digits = phone.replace(/\\D/g, '');\n    return digits.length >= 7; // Minimum reasonable phone number length\n  }\n\n  // Email processing\n  normalizeEmail(email, config = {}) {\n    if (!email || email === '') return null;\n    \n    const normalized = String(email).toLowerCase().trim();\n    \n    // Remove dots from Gmail addresses (they're ignored)\n    if (config.normalizeDots && normalized.includes('@gmail.')) {\n      const [localPart, domain] = normalized.split('@');\n      return localPart.replace(/\\./g, '') + '@' + domain;\n    }\n    \n    return normalized;\n  }\n\n  validateEmail(email, config = {}) {\n    if (email === null && !config.required) return true;\n    if (typeof email !== 'string') return false;\n    \n    const emailRegex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return emailRegex.test(email);\n  }\n\n  // URL processing\n  normalizeUrl(url, config = {}) {\n    if (!url || url === '') return null;\n    \n    let normalized = String(url).trim();\n    \n    // Add protocol if missing\n    if (!normalized.match(/^https?:\\/\\//)) {\n      normalized = 'https://' + normalized;\n    }\n    \n    try {\n      const urlObj = new URL(normalized);\n      \n      // Remove trailing slash if requested\n      if (config.removeTrailingSlash && urlObj.pathname.endsWith('/')) {\n        urlObj.pathname = urlObj.pathname.slice(0, -1);\n      }\n      \n      // Remove query parameters if requested\n      if (config.removeQuery) {\n        urlObj.search = '';\n      }\n      \n      // Remove hash fragment if requested\n      if (config.removeHash) {\n        urlObj.hash = '';\n      }\n      \n      return urlObj.toString();\n    } catch {\n      return normalized; // Return as-is if URL parsing fails\n    }\n  }\n\n  validateUrl(url, config = {}) {\n    if (url === null && !config.required) return true;\n    if (typeof url !== 'string') return false;\n    \n    try {\n      new URL(url);\n      return true;\n    } catch {\n      return false;\n    }\n  }\n\n  // Text processing\n  normalizeText(text, config = {}) {\n    if (!text || text === '') return config.default || null;\n    \n    let normalized = this.cleanValue(text, config);\n    \n    // Remove excessive punctuation\n    if (config.normalizePunctuation) {\n      normalized = normalized.replace(/[.]{2,}/g, '...');\n      normalized = normalized.replace(/[!]{2,}/g, '!');\n      normalized = normalized.replace(/[?]{2,}/g, '?');\n    }\n    \n    return normalized;\n  }\n\n  validateText(text, config = {}) {\n    if (text === null && !config.required) return true;\n    if (typeof text !== 'string') return false;\n    \n    if (config.minLength && text.length < config.minLength) return false;\n    if (config.maxLength && text.length > config.maxLength) return false;\n    if (config.pattern && !new RegExp(config.pattern).test(text)) return false;\n    \n    return true;\n  }\n\n  // Number processing\n  normalizeNumber(number, config = {}) {\n    if (number === null || number === undefined || number === '') return null;\n    \n    const num = parseFloat(String(number).replace(/[^\\d.-]/g, ''));\n    \n    if (isNaN(num)) return null;\n    \n    if (config.round) {\n      return Math.round(num);\n    }\n    \n    if (config.decimalPlaces !== undefined) {\n      return parseFloat(num.toFixed(config.decimalPlaces));\n    }\n    \n    return num;\n  }\n\n  validateNumber(number, config = {}) {\n    if (number === null && !config.required) return true;\n    if (typeof number !== 'number' || isNaN(number)) return false;\n    \n    if (config.min !== undefined && number < config.min) return false;\n    if (config.max !== undefined && number > config.max) return false;\n    \n    return true;\n  }\n\n  // Address processing\n  normalizeAddress(address, config = {}) {\n    if (!address || address === '') return null;\n    \n    let normalized = this.cleanValue(address, { stripHtml: true, collapseWhitespace: true });\n    \n    // Basic address normalization\n    normalized = normalized\n      .replace(/\\bSt\\b/gi, 'Street')\n      .replace(/\\bAve\\b/gi, 'Avenue')\n      .replace(/\\bRd\\b/gi, 'Road')\n      .replace(/\\bBlvd\\b/gi, 'Boulevard')\n      .replace(/\\bDr\\b/gi, 'Drive');\n    \n    return normalized;\n  }\n\n  // Transform functions\n  async applyTransform(value, transform, fieldConfig) {\n    if (!transform || !transform.type) return value;\n    \n    const transformFunction = this.transformers.get(transform.type);\n    if (transformFunction) {\n      return await transformFunction(value, transform.options || {});\n    }\n    \n    // Built-in transforms\n    switch (transform.type) {\n    case 'uppercase':\n      return typeof value === 'string' ? value.toUpperCase() : value;\n    case 'lowercase':\n      return typeof value === 'string' ? value.toLowerCase() : value;\n    case 'trim':\n      return typeof value === 'string' ? value.trim() : value;\n    case 'multiply':\n      return typeof value === 'number' ? value * (transform.factor || 1) : value;\n    case 'prefix':\n      return typeof value === 'string' ? (transform.prefix || '') + value : value;\n    case 'suffix':\n      return typeof value === 'string' ? value + (transform.suffix || '') : value;\n    default:\n      console.warn(`Unknown transform type: ${transform.type}`);\n      return value;\n    }\n  }\n\n  async geocodeAddress(address, options = {}) {\n    if (!address || typeof address !== 'string') return null;\n    \n    // Check cache first\n    const cacheKey = address.toLowerCase().trim();\n    if (this.geocodeCache.has(cacheKey)) {\n      return this.geocodeCache.get(cacheKey);\n    }\n    \n    try {\n      if (!process.env.GOOGLE_MAPS_API_KEY && !options.provider) {\n        console.warn('No geocoding service configured');\n        return null;\n      }\n      \n      const provider = options.provider || 'google';\n      let result = null;\n      \n      if (provider === 'google' && process.env.GOOGLE_MAPS_API_KEY) {\n        result = await this.geocodeWithGoogle(address);\n      }\n      \n      // Cache the result\n      if (result) {\n        this.geocodeCache.set(cacheKey, result);\n      }\n      \n      return result;\n      \n    } catch (error) {\n      console.error('Geocoding failed:', error);\n      return null;\n    }\n  }\n\n  async geocodeWithGoogle(address) {\n    const response = await fetch(\n      `https://maps.googleapis.com/maps/api/geocode/json?address=${encodeURIComponent(address)}&key=${process.env.GOOGLE_MAPS_API_KEY}`\n    );\n    \n    const data = await response.json();\n    \n    if (data.results && data.results.length > 0) {\n      const location = data.results[0].geometry.location;\n      return {\n        latitude: location.lat,\n        longitude: location.lng,\n        formatted_address: data.results[0].formatted_address,\n        components: data.results[0].address_components\n      };\n    }\n    \n    return null;\n  }\n\n  async checkForDuplicates(data, deduplicateConfig) {\n    try {\n      const { key, table, scope } = deduplicateConfig;\n      \n      if (!key || !data[key]) {\n        return { isDuplicate: false };\n      }\n      \n      let query = this.supabase\n        .from(table || 'scraped_data')\n        .select('id')\n        .eq(key, data[key]);\n      \n      // Add scope filters if specified\n      if (scope) {\n        Object.entries(scope).forEach(([field, value]) => {\n          query = query.eq(field, value);\n        });\n      }\n      \n      const { data: existing, error } = await query.single();\n      \n      if (error && error.code !== 'PGRST116') { // PGRST116 = no rows found\n        throw error;\n      }\n      \n      return {\n        isDuplicate: !!existing,\n        duplicateValue: data[key],\n        existingId: existing?.id\n      };\n      \n    } catch (error) {\n      console.error('Error checking for duplicates:', error);\n      return { isDuplicate: false, error: error.message };\n    }\n  }\n\n  calculateQualityMetrics(processedData, originalData, schema, errors, warnings) {\n    const totalFields = Object.keys(originalData).length;\n    const processedFields = Object.keys(processedData).length;\n    const errorFields = new Set(errors.map(e => e.field)).size;\n    const warningFields = new Set(warnings.map(w => w.field)).size;\n    \n    // Completeness: How many fields were successfully processed\n    const completeness = totalFields > 0 ? processedFields / totalFields : 1;\n    \n    // Accuracy: How many fields processed without errors\n    const accuracy = totalFields > 0 ? (totalFields - errorFields) / totalFields : 1;\n    \n    // Consistency: Based on data type consistency and format adherence\n    const consistency = this.calculateConsistencyScore(processedData, schema);\n    \n    // Validation score: How many validations passed\n    const validationScore = totalFields > 0 ? (totalFields - errorFields) / totalFields : 1;\n    \n    // Overall quality score (weighted average)\n    const overall = (completeness * 0.3 + accuracy * 0.3 + consistency * 0.2 + validationScore * 0.2);\n    \n    return {\n      completeness: Math.round(completeness * 100) / 100,\n      accuracy: Math.round(accuracy * 100) / 100,\n      consistency: Math.round(consistency * 100) / 100,\n      validation: Math.round(validationScore * 100) / 100,\n      overall: Math.round(overall * 100) / 100,\n      fieldCounts: {\n        total: totalFields,\n        processed: processedFields,\n        errors: errorFields,\n        warnings: warningFields\n      }\n    };\n  }\n\n  calculateConsistencyScore(data, schema) {\n    let consistentFields = 0;\n    let totalFields = 0;\n    \n    Object.entries(data).forEach(([field, value]) => {\n      totalFields++;\n      const fieldConfig = schema.fields?.[field];\n      \n      if (!fieldConfig) {\n        // No schema definition, assume consistent\n        consistentFields++;\n        return;\n      }\n      \n      // Check type consistency\n      const expectedType = fieldConfig.type;\n      const actualType = this.getDataType(value);\n      \n      if (this.isTypeConsistent(actualType, expectedType)) {\n        consistentFields++;\n      }\n    });\n    \n    return totalFields > 0 ? consistentFields / totalFields : 1;\n  }\n\n  getDataType(value) {\n    if (value === null || value === undefined) return 'null';\n    if (typeof value === 'number') return 'number';\n    if (typeof value === 'boolean') return 'boolean';\n    if (typeof value === 'string') {\n      if (this.validateEmail(value)) return 'email';\n      if (this.validateUrl(value)) return 'url';\n      if (!isNaN(Date.parse(value))) return 'date';\n      if (!isNaN(parseFloat(value))) return 'number';\n      return 'text';\n    }\n    return 'unknown';\n  }\n\n  isTypeConsistent(actualType, expectedType) {\n    const typeMapping = {\n      'price': ['number'],\n      'number': ['number'],\n      'text': ['text', 'string'],\n      'string': ['text', 'string'],\n      'email': ['email', 'text'],\n      'url': ['url', 'text'],\n      'date': ['date', 'text'],\n      'phone': ['text', 'string']\n    };\n    \n    const allowedTypes = typeMapping[expectedType] || [expectedType];\n    return allowedTypes.includes(actualType);\n  }\n\n  sanitizeValue(value) {\n    if (typeof value === 'string' && value.length > 200) {\n      return value.substring(0, 200) + '...';\n    }\n    return value;\n  }\n\n  // Utility method to register custom processors\n  registerValidator(type, validatorFunction) {\n    this.validators.set(type, validatorFunction);\n  }\n\n  registerNormalizer(type, normalizerFunction) {\n    this.normalizers.set(type, normalizerFunction);\n  }\n\n  registerTransformer(type, transformerFunction) {\n    this.transformers.set(type, transformerFunction);\n  }\n\n  clearCache() {\n    this.geocodeCache.clear();\n    console.log('├░┼╕┬º┬╣ Data processor cache cleared');\n  }\n}\n\nmodule.exports = { DataProcessor };","usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\services\\distributed-orchestrator.js","messages":[{"ruleId":"no-unused-vars","severity":1,"message":"'queue' is assigned a value but never used.","line":101,"column":28,"nodeType":"Identifier","messageId":"unusedVar","endLine":101,"endColumn":33},{"ruleId":"no-unused-vars","severity":1,"message":"'options' is defined but never used.","line":343,"column":39,"nodeType":"Identifier","messageId":"unusedVar","endLine":343,"endColumn":46},{"ruleId":"no-unused-vars","severity":1,"message":"'error' is defined but never used.","line":562,"column":33,"nodeType":"Identifier","messageId":"unusedVar","endLine":562,"endColumn":38},{"ruleId":"no-unused-vars","severity":1,"message":"'name' is assigned a value but never used.","line":712,"column":19,"nodeType":"Identifier","messageId":"unusedVar","endLine":712,"endColumn":23},{"ruleId":"no-unused-vars","severity":1,"message":"'name' is assigned a value but never used.","line":716,"column":19,"nodeType":"Identifier","messageId":"unusedVar","endLine":716,"endColumn":23}],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":5,"fixableErrorCount":0,"fixableWarningCount":0,"source":"∩╗┐const { Worker, Queue, QueueEvents } = require('bullmq');\nconst IORedis = require('ioredis');\nconst { createClient } = require('@supabase/supabase-js');\nconst { chromium } = require('playwright');\nconst vm = require('vm');\n\nclass DistributedOrchestrator {\n  constructor() {\n    this.redis = new IORedis(process.env.REDIS_URL || 'redis://localhost:6379');\n    // Defensive Supabase initialization - prefer the in-repo stub when env is missing/invalid\n    try {\n      const url = process.env.SUPABASE_URL;\n      if (!url || url === 'your_supabase_url_here' || url.trim() === '') {\n        this.supabase = require('./core/supabase').supabase;\n      } else {\n        try {\n          this.supabase = createClient(url, process.env.SUPABASE_SERVICE_KEY);\n        } catch (e) {\n          console.warn('DistributedOrchestrator: Supabase client init failed, using local stub. Error:', e && e.message);\n          this.supabase = require('./core/supabase').supabase;\n        }\n      }\n    } catch (e) {\n      console.warn('DistributedOrchestrator: unexpected error initializing supabase, using stub:', e && e.message);\n      this.supabase = require('./core/supabase').supabase;\n    }\n    this.queues = new Map();\n    this.workers = new Map();\n    this.queueEvents = new Map();\n    this.isInitialized = false;\n    \n    this.setupQueues();\n  }\n\n  async initialize() {\n    if (this.isInitialized) return;\n    \n    try {\n      console.log('├░┼╕┼íΓé¼ Initializing Distributed Orchestrator...');\n      await this.setupQueues();\n      await this.setupQueueEvents();\n      this.isInitialized = true;\n      console.log('├ó┼ôΓÇª Distributed Orchestrator initialized successfully');\n    } catch (error) {\n      console.error('├ó┬¥┼Æ Failed to initialize Distributed Orchestrator:', error);\n      throw error;\n    }\n  }\n\n  async setupQueues() {\n    try {\n      // Main scraping queue with standard priorities\n      this.queues.set('scraping', new Queue('scraping', { \n        connection: this.redis,\n        defaultJobOptions: {\n          attempts: 3,\n          backoff: {\n            type: 'exponential',\n            delay: 2000\n          },\n          removeOnComplete: 100, // Keep last 100 completed jobs\n          removeOnFail: 50 // Keep last 50 failed jobs\n        }\n      }));\n      \n      // Priority queue for urgent jobs\n      this.queues.set('priority-scraping', new Queue('priority-scraping', { \n        connection: this.redis,\n        defaultJobOptions: {\n          priority: 1,\n          attempts: 5,\n          backoff: {\n            type: 'exponential',\n            delay: 1000\n          },\n          removeOnComplete: 50,\n          removeOnFail: 25\n        }\n      }));\n\n      // Background processing queue for analysis and cleanup\n      this.queues.set('background-processing', new Queue('background-processing', {\n        connection: this.redis,\n        defaultJobOptions: {\n          attempts: 2,\n          backoff: {\n            type: 'fixed',\n            delay: 5000\n          }\n        }\n      }));\n\n      console.log('├░┼╕ΓÇ£ΓÇ╣ Queues initialized successfully');\n    } catch (error) {\n      console.error('├ó┬¥┼Æ Failed to setup queues:', error);\n      throw error;\n    }\n  }\n\n  async setupQueueEvents() {\n    for (const [queueName, queue] of this.queues) {\n      const queueEvents = new QueueEvents(queueName, { connection: this.redis });\n      \n      queueEvents.on('completed', ({ jobId, returnvalue }) => {\n        console.log(`├ó┼ôΓÇª Job ${jobId} completed in queue ${queueName}`);\n        this.handleJobCompletion(jobId, returnvalue);\n      });\n\n      queueEvents.on('failed', ({ jobId, failedReason }) => {\n        console.error(`├ó┬¥┼Æ Job ${jobId} failed in queue ${queueName}: ${failedReason}`);\n        this.handleJobFailure(jobId, failedReason);\n      });\n\n      queueEvents.on('progress', ({ jobId, data }) => {\n        console.log(`├░┼╕ΓÇ£╦å Job ${jobId} progress: ${data}%`);\n      });\n\n      this.queueEvents.set(queueName, queueEvents);\n    }\n  }\n\n  async scheduleJob(template, urls, options = {}) {\n    try {\n      if (!Array.isArray(urls)) {\n        urls = [urls];\n      }\n\n      const queueName = options.priority === 'high' ? 'priority-scraping' : 'scraping';\n      const queue = this.queues.get(queueName);\n\n      if (!queue) {\n        throw new Error(`Queue ${queueName} not found`);\n      }\n\n      console.log(`├░┼╕ΓÇ£ΓÇª Scheduling ${urls.length} jobs for template ${template.id} in ${queueName} queue`);\n\n      const jobs = [];\n      const batchId = `batch_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n      \n      for (const [index, url] of urls.entries()) {\n        const jobData = {\n          templateId: template.id,\n          templateCode: template.code,\n          templateConfig: template.config || {},\n          url: url,\n          batchId: batchId,\n          batchIndex: index,\n          batchTotal: urls.length,\n          options: {\n            timeout: options.timeout || 30000,\n            retries: options.retries || 3,\n            proxy: options.proxy,\n            userAgent: options.userAgent || 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n            headless: options.headless !== false,\n            captchaSolving: options.captchaSolving !== false,\n            dataValidation: options.dataValidation !== false\n          }\n        };\n\n        const jobOptions = {\n          jobId: `${template.id}_${Date.now()}_${index}_${Math.random().toString(36).substr(2, 9)}`,\n          delay: options.delay ? options.delay + (index * (options.stagger || 0)) : 0,\n          priority: options.priority === 'high' ? 1 : (options.priority === 'low' ? 10 : 5)\n        };\n\n        // Create execution record in database\n        const { data: execution } = await this.supabase\n          .from('scraping_executions')\n          .insert([{\n            template_id: template.id,\n            url: url,\n            status: 'queued',\n            execution_metadata: {\n              batchId: batchId,\n              queueName: queueName,\n              jobId: jobOptions.jobId,\n              scheduledAt: new Date().toISOString()\n            }\n          }])\n          .select()\n          .single();\n\n        jobData.executionId = execution.id;\n\n        const job = await queue.add('scrape-url', jobData, jobOptions);\n        jobs.push({\n          jobId: job.id,\n          executionId: execution.id,\n          url: url,\n          status: 'queued'\n        });\n      }\n\n      console.log(`├ó┼ôΓÇª Successfully scheduled ${jobs.length} jobs in batch ${batchId}`);\n      return {\n        batchId: batchId,\n        jobs: jobs,\n        queueName: queueName\n      };\n\n    } catch (error) {\n      console.error('├ó┬¥┼Æ Error scheduling jobs:', error);\n      throw error;\n    }\n  }\n\n  async startWorkers(workerConfigs = []) {\n    try {\n      // Default worker configurations if none provided\n      if (workerConfigs.length === 0) {\n        workerConfigs = [\n          { name: 'scraper-worker-1', queues: ['scraping'], concurrency: 2 },\n          { name: 'scraper-worker-2', queues: ['scraping'], concurrency: 2 },\n          { name: 'priority-worker-1', queues: ['priority-scraping'], concurrency: 1 },\n          { name: 'background-worker-1', queues: ['background-processing'], concurrency: 1 }\n        ];\n      }\n\n      console.log(`├░┼╕┬Å╞Æ Starting ${workerConfigs.length} workers...`);\n\n      for (const config of workerConfigs) {\n        await this.startWorker(config);\n      }\n\n      console.log(`├ó┼ôΓÇª All ${workerConfigs.length} workers started successfully`);\n    } catch (error) {\n      console.error('├ó┬¥┼Æ Error starting workers:', error);\n      throw error;\n    }\n  }\n\n  async startWorker(config) {\n    try {\n      const { name, queues, concurrency = 1, limiter } = config;\n\n      for (const queueName of queues) {\n        const workerName = `${name}-${queueName}`;\n        \n        const worker = new Worker(queueName, async (job) => {\n          return await this.processScrapingJob(job);\n        }, {\n          connection: this.redis,\n          concurrency: concurrency,\n          limiter: limiter || {\n            max: 10, // Max 10 jobs per second\n            duration: 1000\n          }\n        });\n\n        worker.on('completed', (job) => {\n          console.log(`├ó┼ôΓÇª Job ${job.id} completed by worker ${workerName}`);\n        });\n\n        worker.on('failed', (job, err) => {\n          console.error(`├ó┬¥┼Æ Job ${job.id} failed in worker ${workerName}:`, err.message);\n        });\n\n        worker.on('progress', (job, progress) => {\n          console.log(`├░┼╕ΓÇ£┼á Job ${job.id} progress: ${progress}%`);\n        });\n\n        worker.on('error', (error) => {\n          console.error(`├░┼╕┼í┬¿ Worker ${workerName} error:`, error);\n        });\n\n        this.workers.set(workerName, worker);\n        console.log(`├░┼╕ΓÇ¥ΓÇ₧ Worker ${workerName} started (concurrency: ${concurrency})`);\n      }\n    } catch (error) {\n      console.error(`├ó┬¥┼Æ Error starting worker ${config.name}:`, error);\n      throw error;\n    }\n  }\n\n  async processScrapingJob(job) {\n    const startTime = Date.now();\n    const { templateCode, templateConfig, url, options, executionId } = job.data;\n    \n    try {\n      console.log(`├░┼╕ΓÇ¥ΓÇ₧ Processing job ${job.id} for URL: ${url}`);\n\n      // Update execution status\n      await this.updateExecutionStatus(executionId, 'running', {\n        startedAt: new Date().toISOString(),\n        workerId: job.name\n      });\n\n      // Update job progress\n      await job.updateProgress(10);\n\n      // Create scraper instance and execute\n      const scraper = this.createScraperInstance(templateCode, options);\n      \n      await job.updateProgress(20);\n      \n      const result = await scraper.execute(url, options, job);\n      \n      await job.updateProgress(80);\n      \n      // Validate result structure if configured\n      if (options.dataValidation && templateConfig.expectedFields) {\n        this.validateScrapingResult(result.data, templateConfig.expectedFields);\n      }\n\n      const duration = Date.now() - startTime;\n      \n      // Update execution with results\n      await this.updateExecutionStatus(executionId, 'completed', {\n        completedAt: new Date().toISOString(),\n        duration: duration\n      }, result.data, result.metadata);\n\n      await job.updateProgress(100);\n\n      console.log(`├ó┼ôΓÇª Job ${job.id} completed successfully in ${duration}ms`);\n\n      return {\n        success: true,\n        data: result.data,\n        metadata: {\n          ...result.metadata,\n          duration: duration,\n          url: url,\n          timestamp: new Date().toISOString(),\n          executionId: executionId\n        }\n      };\n\n    } catch (error) {\n      const duration = Date.now() - startTime;\n      \n      await this.updateExecutionStatus(executionId, 'failed', {\n        failedAt: new Date().toISOString(),\n        duration: duration,\n        errorMessage: error.message\n      });\n\n      console.error(`├ó┬¥┼Æ Job ${job.id} failed after ${duration}ms:`, error.message);\n      throw error;\n    }\n  }\n\n  createScraperInstance(templateCode, options) {\n    class ScraperExecutionContext {\n      constructor() {\n        this.browser = null;\n        this.page = null;\n      }\n\n      async execute(url, options, job) {\n        const startTime = Date.now();\n        \n        try {\n          // Launch browser with stealth options\n          this.browser = await chromium.launch({ \n            headless: options.headless,\n            args: [\n              '--no-sandbox',\n              '--disable-setuid-sandbox',\n              '--disable-dev-shm-usage',\n              '--disable-accelerated-2d-canvas',\n              '--no-first-run',\n              '--no-zygote',\n              '--disable-gpu'\n            ],\n            ...options.browserOptions\n          });\n\n          await job?.updateProgress(30);\n\n          this.page = await this.browser.newPage();\n          \n          // Apply stealth and configuration options\n          await this.applyStealthOptions(this.page, options);\n          \n          await job?.updateProgress(40);\n\n          // Set up proxy if provided\n          if (options.proxy) {\n            await this.setupProxy(this.page, options.proxy);\n          }\n\n          await job?.updateProgress(50);\n\n          // Execute the template code in VM context\n          const context = vm.createContext({\n            page: this.page,\n            browser: this.browser,\n            url: url,\n            options: options,\n            console: console,\n            setTimeout: setTimeout,\n            setInterval: setInterval,\n            clearTimeout: clearTimeout,\n            clearInterval: clearInterval,\n            JSON: JSON,\n            Date: Date,\n            Math: Math,\n            Buffer: Buffer,\n            require: require // Controlled require for specific modules\n          });\n\n          await job?.updateProgress(60);\n\n          const script = new vm.Script(templateCode);\n          const result = await script.runInContext(context, {\n            timeout: options.timeout || 30000\n          });\n          \n          await job?.updateProgress(70);\n\n          const duration = Date.now() - startTime;\n          \n          return {\n            data: result,\n            duration: duration,\n            metadata: {\n              browserUsed: 'chromium',\n              headless: options.headless,\n              proxyUsed: !!options.proxy,\n              pageLoadTime: this.page ? await this.page.evaluate(() => window.performance.timing.loadEventEnd - window.performance.timing.navigationStart) : null\n            }\n          };\n\n        } finally {\n          if (this.browser) {\n            await this.browser.close();\n          }\n        }\n      }\n\n      async applyStealthOptions(page, options) {\n        try {\n          // Set realistic viewport\n          await page.setViewportSize({ \n            width: 1920, \n            height: 1080 \n          });\n          \n          // Set user agent\n          await page.setUserAgent(options.userAgent);\n          \n          // Override webdriver detection\n          await page.addInitScript(() => {\n            Object.defineProperty(navigator, 'webdriver', {\n              get: () => undefined,\n            });\n          });\n\n          // Block unnecessary resources for performance\n          await page.route('**/*', (route) => {\n            const resourceType = route.request().resourceType();\n            const url = route.request().url();\n            \n            // Block ads, analytics, and heavy media\n            if (resourceType === 'image' && !options.loadImages) {\n              route.abort();\n            } else if (['font', 'media'].includes(resourceType) && !options.loadMedia) {\n              route.abort();\n            } else if (url.includes('google-analytics') || url.includes('facebook.com/tr')) {\n              route.abort();\n            } else {\n              route.continue();\n            }\n          });\n\n          // Set extra headers\n          await page.setExtraHTTPHeaders({\n            'Accept-Language': 'en-US,en;q=0.9',\n            'Accept-Encoding': 'gzip, deflate, br',\n            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n            'Upgrade-Insecure-Requests': '1',\n            'Sec-Fetch-Dest': 'document',\n            'Sec-Fetch-Mode': 'navigate',\n            'Sec-Fetch-Site': 'none'\n          });\n\n        } catch (error) {\n          console.error('Error applying stealth options:', error);\n        }\n      }\n\n      async setupProxy(page, proxyConfig) {\n        // Proxy setup is handled at browser launch level in Playwright\n        // This method is for any additional proxy-related page setup\n        console.log(`├░┼╕┼Æ┬É Using proxy: ${proxyConfig.host}:${proxyConfig.port}`);\n      }\n    }\n\n    return new ScraperExecutionContext();\n  }\n\n  validateScrapingResult(result, expectedFields) {\n    if (!result || typeof result !== 'object') {\n      throw new Error('Invalid result: expected object');\n    }\n\n    const missingFields = [];\n    for (const field of expectedFields) {\n      if (!(field in result) || result[field] === null || result[field] === undefined) {\n        missingFields.push(field);\n      }\n    }\n\n    if (missingFields.length > 0) {\n      throw new Error(`Missing expected fields: ${missingFields.join(', ')}`);\n    }\n  }\n\n  async updateExecutionStatus(executionId, status, metadata = {}, result = null, resultMetadata = {}) {\n    try {\n      const updateData = {\n        status: status,\n        execution_metadata: metadata\n      };\n\n      if (result) {\n        updateData.raw_result = result;\n      }\n\n      if (Object.keys(resultMetadata).length > 0) {\n        updateData.execution_metadata = {\n          ...metadata,\n          ...resultMetadata\n        };\n      }\n\n      await this.supabase\n        .from('scraping_executions')\n        .update(updateData)\n        .eq('id', executionId);\n\n    } catch (error) {\n      console.error('Error updating execution status:', error);\n    }\n  }\n\n  async handleJobCompletion(jobId, returnvalue) {\n    try {\n      // Extract execution ID from return value\n      const executionId = returnvalue?.metadata?.executionId;\n      \n      if (executionId) {\n        // Update template metrics\n        const { data: execution } = await this.supabase\n          .from('scraping_executions')\n          .select('template_id')\n          .eq('id', executionId)\n          .single();\n\n        if (execution?.template_id) {\n          await this.updateTemplateMetrics(execution.template_id, true, returnvalue?.metadata?.duration);\n        }\n      }\n\n      console.log(`├░┼╕ΓÇ£┼á Job completion handled for job ${jobId}`);\n    } catch (error) {\n      console.error('Error handling job completion:', error);\n    }\n  }\n\n  async handleJobFailure(jobId, error) {\n    try {\n      // Find execution by job ID (stored in metadata)\n      const { data: executions } = await this.supabase\n        .from('scraping_executions')\n        .select('id, template_id, execution_metadata')\n        .eq('execution_metadata->>jobId', jobId)\n        .limit(1);\n\n      if (executions && executions.length > 0) {\n        const execution = executions[0];\n        \n        if (execution.template_id) {\n          await this.updateTemplateMetrics(execution.template_id, false);\n        }\n      }\n\n      console.log(`├░┼╕ΓÇ£┼á Job failure handled for job ${jobId}`);\n    } catch (error) {\n      console.error('Error handling job failure:', error);\n    }\n  }\n\n  async updateTemplateMetrics(templateId, success, duration = 0) {\n    try {\n      // Get or create metrics record\n      let { data: metrics } = await this.supabase\n        .from('template_metrics')\n        .select('*')\n        .eq('template_id', templateId)\n        .single();\n\n      if (!metrics) {\n        // Create new metrics record\n        const { data: newMetrics, error } = await this.supabase\n          .from('template_metrics')\n          .insert([{\n            template_id: templateId,\n            total_runs: 1,\n            successful_runs: success ? 1 : 0,\n            failed_runs: success ? 0 : 1,\n            success_rate: success ? 1.0 : 0.0,\n            average_duration: duration || 0,\n            last_run: new Date().toISOString()\n          }])\n          .select()\n          .single();\n\n        if (error) throw error;\n        return newMetrics;\n      }\n\n      // Update existing metrics\n      const newTotalRuns = metrics.total_runs + 1;\n      const newSuccessfulRuns = metrics.successful_runs + (success ? 1 : 0);\n      const newFailedRuns = metrics.failed_runs + (success ? 0 : 1);\n      const newSuccessRate = newSuccessfulRuns / newTotalRuns;\n      \n      // Calculate new average duration\n      const currentTotalDuration = metrics.average_duration * metrics.total_runs;\n      const newAverageDuration = (currentTotalDuration + (duration || 0)) / newTotalRuns;\n\n      const { data: updatedMetrics, error } = await this.supabase\n        .from('template_metrics')\n        .update({\n          total_runs: newTotalRuns,\n          successful_runs: newSuccessfulRuns,\n          failed_runs: newFailedRuns,\n          success_rate: newSuccessRate,\n          average_duration: newAverageDuration,\n          last_run: new Date().toISOString()\n        })\n        .eq('template_id', templateId)\n        .select()\n        .single();\n\n      if (error) throw error;\n      return updatedMetrics;\n\n    } catch (error) {\n      console.error('Error updating template metrics:', error);\n      throw error;\n    }\n  }\n\n  async getQueueStats() {\n    try {\n      const stats = {};\n      \n      for (const [name, queue] of this.queues) {\n        const waiting = await queue.getWaiting();\n        const active = await queue.getActive();\n        const completed = await queue.getCompleted();\n        const failed = await queue.getFailed();\n\n        stats[name] = {\n          waiting: waiting.length,\n          active: active.length,\n          completed: completed.length,\n          failed: failed.length,\n          total: waiting.length + active.length + completed.length + failed.length\n        };\n      }\n\n      return stats;\n    } catch (error) {\n      console.error('Error getting queue stats:', error);\n      throw error;\n    }\n  }\n\n  async pauseQueue(queueName) {\n    const queue = this.queues.get(queueName);\n    if (queue) {\n      await queue.pause();\n      console.log(`├ó┬Å┬╕├»┬╕┬Å Queue ${queueName} paused`);\n    }\n  }\n\n  async resumeQueue(queueName) {\n    const queue = this.queues.get(queueName);\n    if (queue) {\n      await queue.resume();\n      console.log(`├óΓÇô┬╢├»┬╕┬Å Queue ${queueName} resumed`);\n    }\n  }\n\n  async stopAllWorkers() {\n    try {\n      console.log('├░┼╕ΓÇ║ΓÇÿ Stopping all workers...');\n      \n      for (const [name, worker] of this.workers) {\n        await worker.close();\n        console.log(`├░┼╕ΓÇ¥ΓÇ₧ Worker ${name} stopped`);\n      }\n\n      this.workers.clear();\n      console.log('├ó┼ôΓÇª All workers stopped successfully');\n    } catch (error) {\n      console.error('├ó┬¥┼Æ Error stopping workers:', error);\n      throw error;\n    }\n  }\n\n  async cleanup() {\n    try {\n      console.log('├░┼╕┬º┬╣ Cleaning up Distributed Orchestrator...');\n      \n      await this.stopAllWorkers();\n      \n      for (const [name, queueEvents] of this.queueEvents) {\n        await queueEvents.close();\n      }\n\n      for (const [name, queue] of this.queues) {\n        await queue.close();\n      }\n\n      await this.redis.quit();\n      \n      console.log('├ó┼ôΓÇª Cleanup completed successfully');\n    } catch (error) {\n      console.error('├ó┬¥┼Æ Error during cleanup:', error);\n      throw error;\n    }\n  }\n}\n\nmodule.exports = { DistributedOrchestrator };","usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\services\\job-queue.js","messages":[{"ruleId":"no-unused-vars","severity":1,"message":"'result' is defined but never used.","line":88,"column":39,"nodeType":"Identifier","messageId":"unusedVar","endLine":88,"endColumn":45}],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":0,"source":"∩╗┐// APL AI Scraper 2.0 - Job Queue System\nconst { Queue, Worker } = require('bullmq');\nconst IORedis = require('ioredis');\nconst { PlaywrightScraper } = require('../scrapers/playwright-scraper');\n\nclass JobQueue {\n  constructor(supabase) {\n    this.supabase = supabase;\n    this.connection = new IORedis(process.env.REDIS_URL || 'redis://localhost:6379', {\n      maxRetriesPerRequest: 3,\n      retryDelayOnFailover: 100,\n      lazyConnect: true\n    });\n\n    // Initialize queue\n    this.scrapingQueue = new Queue('scraping-queue', { \n      connection: this.connection,\n      defaultJobOptions: {\n        attempts: 3,\n        backoff: {\n          type: 'exponential',\n          delay: 2000,\n        },\n        removeOnComplete: 100,\n        removeOnFail: 50\n      }\n    });\n\n    this.startWorker();\n    console.log('├░┼╕┼íΓé¼ Job Queue initialized');\n  }\n\n  async addJob(jobId, priority = 0) {\n    try {\n      await this.scrapingQueue.add(\n        'scrape-job',\n        { jobId },\n        {\n          priority,\n          jobId: `job-${jobId}`, // Unique job ID\n          delay: 0\n        }\n      );\n      console.log(`├ó┼╛ΓÇó Job ${jobId} added to queue`);\n    } catch (error) {\n      console.error(`├ó┬¥┼Æ Failed to add job ${jobId} to queue:`, error);\n      throw error;\n    }\n  }\n\n  async addBulkJobs(jobs) {\n    try {\n      const jobData = jobs.map((job, index) => ({\n        name: 'scrape-job',\n        data: { jobId: job.id },\n        opts: {\n          priority: job.priority || 0,\n          jobId: `job-${job.id}`,\n          delay: index * 1000 // Stagger jobs by 1 second\n        }\n      }));\n\n      await this.scrapingQueue.addBulk(jobData);\n      console.log(`├ó┼╛ΓÇó ${jobs.length} jobs added to queue`);\n    } catch (error) {\n      console.error('├ó┬¥┼Æ Failed to add bulk jobs to queue:', error);\n      throw error;\n    }\n  }\n\n  startWorker() {\n    this.worker = new Worker(\n      'scraping-queue',\n      async (job) => {\n        return await this.processJob(job);\n      },\n      {\n        connection: this.connection,\n        concurrency: parseInt(process.env.WORKER_CONCURRENCY) || 3,\n        limiter: {\n          max: 10,\n          duration: 60000 // 10 requests per minute\n        }\n      }\n    );\n\n    // Worker event handlers\n    this.worker.on('completed', (job, result) => {\n      console.log(`├ó┼ôΓÇª Job ${job.data.jobId} completed successfully`);\n    });\n\n    this.worker.on('failed', (job, err) => {\n      console.error(`├ó┬¥┼Æ Job ${job.data.jobId} failed:`, err.message);\n    });\n\n    this.worker.on('error', (err) => {\n      console.error('├░┼╕ΓÇ¥┬Ñ Worker error:', err);\n    });\n\n    const concurrency = (this.worker?.opts?.concurrency ?? parseInt(process.env.WORKER_CONCURRENCY)) || 3;\n    console.log('├░┼╕ΓÇÿ┬╖ Worker started with concurrency:', concurrency);\n  }\n\n  async processJob(job) {\n    const { jobId } = job.data;\n    const startTime = Date.now();\n    \n    console.log(`├░┼╕ΓÇ¥ΓÇ₧ Processing job ${jobId}`);\n\n    // Update job status to running\n    await this.updateJobStatus(jobId, 'running', {\n      started_at: new Date().toISOString(),\n      attempts: job.attemptsMade + 1\n    });\n\n    let scraper = null;\n\n    try {\n      // Get job details from database\n      const { data: jobData, error } = await this.supabase\n        .from('scraping_jobs')\n        .select('*')\n        .eq('id', jobId)\n        .single();\n\n      if (error) {\n        throw new Error(`Failed to fetch job data: ${error.message}`);\n      }\n\n      console.log(`├░┼╕ΓÇ£┼á Job details: ${jobData.url}`);\n\n      // Initialize scraper\n      scraper = new PlaywrightScraper();\n      await scraper.init();\n\n      // Build scraping configuration\n      const scrapingConfig = {\n        url: jobData.url,\n        ...jobData.config\n      };\n\n      // Execute scraping\n      const result = await scraper.scrape(scrapingConfig);\n      await scraper.close();\n\n      if (result.success) {\n        // Store scraped data\n        const { error: insertError } = await this.supabase\n          .from('scraped_data')\n          .insert([{\n            job_id: jobId,\n            data: result.data,\n            url: jobData.url,\n            metadata: {\n              scraped_at: new Date().toISOString(),\n              processing_time: Date.now() - startTime,\n              scraper_version: '2.0',\n              config: scrapingConfig\n            }\n          }]);\n\n        if (insertError) {\n          throw new Error(`Failed to store scraped data: ${insertError.message}`);\n        }\n\n        // Update job as completed\n        await this.updateJobStatus(jobId, 'completed', {\n          completed_at: new Date().toISOString(),\n          result: {\n            success: true,\n            data_size: JSON.stringify(result.data).length,\n            processing_time: Date.now() - startTime\n          }\n        });\n\n        console.log(`├ó┼ôΓÇª Job ${jobId} completed in ${Date.now() - startTime}ms`);\n        return { success: true, jobId, processingTime: Date.now() - startTime };\n\n      } else {\n        throw new Error(result.error);\n      }\n\n    } catch (error) {\n      console.error(`├ó┬¥┼Æ Job ${jobId} failed:`, error.message);\n\n      // Close scraper if it was initialized\n      if (scraper) {\n        await scraper.close();\n      }\n\n      // Update job as failed\n      await this.updateJobStatus(jobId, 'failed', {\n        error_message: error.message,\n        attempts: job.attemptsMade + 1\n      });\n\n      // Determine if we should retry\n      const shouldRetry = job.attemptsMade < (job.opts.attempts || 3);\n      \n      if (!shouldRetry) {\n        console.log(`├░┼╕ΓÇÖΓé¼ Job ${jobId} exceeded max attempts`);\n      }\n\n      throw error;\n    }\n  }\n\n  async updateJobStatus(jobId, status, additionalFields = {}) {\n    try {\n      const updateData = {\n        status,\n        ...additionalFields\n      };\n\n      const { error } = await this.supabase\n        .from('scraping_jobs')\n        .update(updateData)\n        .eq('id', jobId);\n\n      if (error) {\n        console.error(`Failed to update job ${jobId} status:`, error);\n        throw error;\n      }\n    } catch (error) {\n      console.error(`Database update failed for job ${jobId}:`, error);\n      // Don't throw here as it would cause the job to fail unnecessarily\n    }\n  }\n\n  async getQueueStats() {\n    try {\n      const waiting = await this.scrapingQueue.getWaiting();\n      const active = await this.scrapingQueue.getActive();\n      const completed = await this.scrapingQueue.getCompleted();\n      const failed = await this.scrapingQueue.getFailed();\n\n      return {\n        waiting: waiting.length,\n        active: active.length,\n        completed: completed.length,\n        failed: failed.length,\n        total: waiting.length + active.length + completed.length + failed.length\n      };\n    } catch (error) {\n      console.error('Failed to get queue stats:', error);\n      return {\n        waiting: 0,\n        active: 0,\n        completed: 0,\n        failed: 0,\n        total: 0,\n        error: error.message\n      };\n    }\n  }\n\n  async pauseQueue() {\n    try {\n      await this.scrapingQueue.pause();\n      console.log('├ó┬Å┬╕├»┬╕┬Å Queue paused');\n    } catch (error) {\n      console.error('Failed to pause queue:', error);\n      throw error;\n    }\n  }\n\n  async resumeQueue() {\n    try {\n      await this.scrapingQueue.resume();\n      console.log('├óΓÇô┬╢├»┬╕┬Å Queue resumed');\n    } catch (error) {\n      console.error('Failed to resume queue:', error);\n      throw error;\n    }\n  }\n\n  async clearQueue() {\n    try {\n      await this.scrapingQueue.obliterate({ force: true });\n      console.log('├░┼╕┬º┬╣ Queue cleared');\n    } catch (error) {\n      console.error('Failed to clear queue:', error);\n      throw error;\n    }\n  }\n\n  async retryFailedJobs() {\n    try {\n      const failedJobs = await this.scrapingQueue.getFailed();\n      let retriedCount = 0;\n\n      for (const job of failedJobs) {\n        try {\n          await job.retry();\n          retriedCount++;\n        } catch (retryError) {\n          console.warn(`Failed to retry job ${job.id}:`, retryError.message);\n        }\n      }\n\n      console.log(`├░┼╕ΓÇ¥ΓÇ₧ Retried ${retriedCount} failed jobs`);\n      return retriedCount;\n    } catch (error) {\n      console.error('Failed to retry failed jobs:', error);\n      throw error;\n    }\n  }\n\n  async close() {\n    try {\n      await this.worker?.close();\n      await this.scrapingQueue?.close();\n      await this.connection?.disconnect();\n      console.log('├░┼╕ΓÇ¥ΓÇÖ Job Queue closed');\n    } catch (error) {\n      console.error('├ó┬¥┼Æ Error closing Job Queue:', error);\n    }\n  }\n}\n\nmodule.exports = { JobQueue };","usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\services\\privacy-manager.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\services\\proxy-manager.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\services\\visual-analysis-engine.js","messages":[{"ruleId":"no-unused-vars","severity":1,"message":"'fs' is assigned a value but never used.","line":4,"column":7,"nodeType":"Identifier","messageId":"unusedVar","endLine":4,"endColumn":9},{"ruleId":"no-unused-vars","severity":1,"message":"'path' is assigned a value but never used.","line":5,"column":7,"nodeType":"Identifier","messageId":"unusedVar","endLine":5,"endColumn":11},{"ruleId":"no-unused-vars","severity":1,"message":"'currentElement' is assigned a value but never used.","line":249,"column":9,"nodeType":"Identifier","messageId":"unusedVar","endLine":249,"endColumn":23},{"ruleId":"no-unused-vars","severity":1,"message":"'actions' is defined but never used.","line":600,"column":37,"nodeType":"Identifier","messageId":"unusedVar","endLine":600,"endColumn":44}],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":4,"fixableErrorCount":0,"fixableWarningCount":0,"source":"∩╗┐// APL AI Scraper 2.0 - Visual Analysis Engine\nconst { AIService } = require('./ai-service');\nconst sharp = require('sharp');\nconst fs = require('fs').promises;\nconst path = require('path');\n\nclass VisualAnalysisEngine {\n  constructor() {\n    this.aiService = new AIService();\n    this.analysisCache = new Map();\n    this.confidenceThreshold = 0.7;\n  }\n\n  async analyzeRecordingSession(sessionData) {\n    console.log('├░┼╕ΓÇ¥┬ì Starting visual analysis of recording session');\n    const { actions, screenshots } = sessionData;\n    \n    const analysis = {\n      identifiedElements: [],\n      dataFields: [],\n      actionPatterns: [],\n      navigationFlow: [],\n      pageStructure: {},\n      confidence: 0,\n      generatedCode: '',\n      recommendations: []\n    };\n\n    try {\n      // Analyze screenshots with GPT-4V\n      if (screenshots && screenshots.length > 0) {\n        console.log(`├░┼╕ΓÇ£┬╕ Analyzing ${screenshots.length} screenshots`);\n        \n        for (let i = 0; i < screenshots.length; i++) {\n          const screenshot = screenshots[i];\n          console.log(`├░┼╕ΓÇ¥┬ì Processing screenshot ${i + 1}/${screenshots.length}`);\n          \n          const screenshotAnalysis = await this.analyzeScreenshot(screenshot, i);\n          \n          // Merge results\n          analysis.identifiedElements.push(...screenshotAnalysis.elements);\n          analysis.dataFields.push(...screenshotAnalysis.dataFields);\n          \n          // Store page structure for first screenshot\n          if (i === 0) {\n            analysis.pageStructure = screenshotAnalysis.pageStructure;\n          }\n        }\n      }\n\n      // Analyze recorded actions for patterns\n      if (actions && actions.length > 0) {\n        console.log(`├ó┼í┬í Analyzing ${actions.length} recorded actions`);\n        analysis.actionPatterns = this.analyzeActionPatterns(actions);\n        analysis.navigationFlow = this.extractNavigationFlow(actions);\n      }\n\n      // Remove duplicates and filter by confidence\n      analysis.identifiedElements = this.deduplicateElements(analysis.identifiedElements);\n      analysis.dataFields = this.deduplicateDataFields(analysis.dataFields);\n\n      // Calculate overall confidence\n      analysis.confidence = this.calculateOverallConfidence(analysis);\n\n      // Generate recommendations\n      analysis.recommendations = this.generateRecommendations(analysis, actions);\n\n      console.log('├ó┼ôΓÇª Visual analysis completed');\n      console.log(`├░┼╕ΓÇ£┼á Found ${analysis.identifiedElements.length} elements, ${analysis.dataFields.length} data fields`);\n\n      return analysis;\n\n    } catch (error) {\n      console.error('├ó┬¥┼Æ Visual analysis failed:', error);\n      throw new Error(`Visual analysis failed: ${error.message}`);\n    }\n  }\n\n  async analyzeScreenshot(screenshot, index) {\n    try {\n      // Generate cache key based on screenshot data\n      const cacheKey = this.generateCacheKey(screenshot.data);\n      \n      if (this.analysisCache.has(cacheKey)) {\n        console.log(`├░┼╕ΓÇ£ΓÇ╣ Using cached analysis for screenshot ${index + 1}`);\n        return this.analysisCache.get(cacheKey);\n      }\n\n      const prompt = `\n        Analyze this web page screenshot and identify interactive elements and data structures.\n        \n        CONTEXT:\n        - URL: ${screenshot.url || 'unknown'}\n        - Viewport: ${screenshot.viewport?.width}x${screenshot.viewport?.height}\n        - Timestamp: ${screenshot.timestamp}\n        \n        ANALYSIS TASKS:\n        1. Identify ALL interactive elements (buttons, links, inputs, dropdowns, checkboxes, etc.)\n        2. Find data-rich sections (products, articles, listings, tables, cards, etc.)\n        3. Detect navigation elements (menus, breadcrumbs, pagination, etc.)\n        4. Identify form elements and their purposes\n        5. Spot repeating patterns that might contain structured data\n\n        For EACH element found, provide:\n        - Element type (button, input, link, text, image, etc.)\n        - Likely purpose/function\n        - Estimated CSS selector (be specific but robust)\n        - Confidence level (0.0-1.0)\n        - Position description (top, center, sidebar, etc.)\n        - Visual characteristics (color, size, styling hints)\n\n        For DATA FIELDS, identify:\n        - Field name/purpose (title, price, description, etc.)\n        - Data type (text, number, date, url, etc.)  \n        - Estimated selector\n        - Whether it appears multiple times (indicating a list/collection)\n        - Confidence level\n\n        Return ONLY valid JSON in this exact structure:\n        {\n          \"elements\": [\n            {\n              \"type\": \"button|input|link|dropdown|checkbox|etc\",\n              \"purpose\": \"search|submit|navigation|filter|etc\",\n              \"selector\": \"specific CSS selector\",\n              \"confidence\": 0.0-1.0,\n              \"position\": \"header|main|sidebar|footer|etc\",\n              \"characteristics\": \"visual description\",\n              \"interactionType\": \"click|type|select|etc\"\n            }\n          ],\n          \"dataFields\": [\n            {\n              \"name\": \"descriptive_field_name\", \n              \"type\": \"text|number|price|date|url|image|etc\",\n              \"selector\": \"CSS selector\",\n              \"confidence\": 0.0-1.0,\n              \"multiple\": true/false,\n              \"context\": \"product|article|listing|etc\",\n              \"sampleValue\": \"example of expected content\"\n            }\n          ],\n          \"pageStructure\": {\n            \"pageType\": \"ecommerce|blog|search|form|listing|news|etc\",\n            \"mainContent\": \"CSS selector for main content area\",\n            \"hasNavigation\": true/false,\n            \"hasPagination\": true/false,\n            \"hasSearch\": true/false,\n            \"layoutType\": \"grid|list|single|complex\"\n          }\n        }\n      `;\n\n      // Process image for optimal AI analysis\n      const imageBuffer = await this.processImageForAnalysis(screenshot.data);\n      \n      console.log('Sending screenshot ' + (index + 1) + ' to GPT-4V for analysis');\n      const analysis = await this.aiService.analyzeWithGPT4V(imageBuffer, prompt);\n      \n      let parsedAnalysis;\n      try {\n        // Clean the response to extract JSON\n        const cleanedAnalysis = this.cleanAIResponse(analysis);\n        parsedAnalysis = JSON.parse(cleanedAnalysis);\n      } catch (parseError) {\n        console.warn('JSON parsing failed for screenshot ' + (index + 1) + ', using fallback parser');\n        parsedAnalysis = this.parseUnstructuredAnalysis(analysis);\n      }\n\n      // Validate and enhance the analysis\n      parsedAnalysis = this.validateAndEnhanceAnalysis(parsedAnalysis, screenshot);\n\n      // Cache the result\n      this.analysisCache.set(cacheKey, parsedAnalysis);\n\n      return parsedAnalysis;\n\n    } catch (error) {\n      console.error(`├ó┬¥┼Æ Screenshot analysis failed for index ${index}:`, error);\n      \n      // Return empty structure on failure\n      return {\n        elements: [],\n        dataFields: [],\n        pageStructure: {\n          pageType: 'unknown',\n          mainContent: 'body',\n          hasNavigation: false,\n          hasPagination: false,\n          hasSearch: false,\n          layoutType: 'unknown'\n        }\n      };\n    }\n  }\n\n  async processImageForAnalysis(base64Data) {\n    try {\n      // Convert base64 to buffer\n      const imageData = base64Data.replace(/^data:image\\/[a-z]+;base64,/, '');\n      const inputBuffer = Buffer.from(imageData, 'base64');\n\n      // Optimize image for GPT-4V (resize, compress, enhance)\n      const optimizedBuffer = await sharp(inputBuffer)\n        .resize(1024, 768, { \n          fit: 'inside',\n          withoutEnlargement: true \n        })\n        .jpeg({ \n          quality: 85,\n          progressive: true \n        })\n        .sharpen()\n        .toBuffer();\n\n      return optimizedBuffer;\n\n    } catch (error) {\n      console.error('Image processing failed:', error);\n      // Return original buffer if processing fails\n      const imageData = base64Data.replace(/^data:image\\/[a-z]+;base64,/, '');\n      return Buffer.from(imageData, 'base64');\n    }\n  }\n\n  cleanAIResponse(response) {\n    // Remove markdown code blocks and extra text\n    let cleaned = response.replace(/```json\\s*/gi, '').replace(/```\\s*/g, '');\n    \n    // Find the JSON object boundaries\n    const jsonStart = cleaned.indexOf('{');\n    const jsonEnd = cleaned.lastIndexOf('}') + 1;\n    \n    if (jsonStart !== -1 && jsonEnd > jsonStart) {\n      cleaned = cleaned.substring(jsonStart, jsonEnd);\n    }\n    \n    return cleaned;\n  }\n\n  parseUnstructuredAnalysis(text) {\n    console.log('├░┼╕ΓÇ£┬¥ Parsing unstructured analysis text');\n    \n    const elements = [];\n    const dataFields = [];\n    const lines = text.split('\\n');\n    \n    let currentSection = '';\n    let currentElement = {};\n    \n    for (const line of lines) {\n      const trimmedLine = line.trim().toLowerCase();\n      \n      // Section detection\n      if (trimmedLine.includes('elements') || trimmedLine.includes('interactive')) {\n        currentSection = 'elements';\n        continue;\n      } else if (trimmedLine.includes('data') || trimmedLine.includes('fields')) {\n        currentSection = 'dataFields';\n        continue;\n      }\n      \n      // Element extraction patterns\n      if (currentSection === 'elements') {\n        const typeMatch = line.match(/type[:\\s]+(\\w+)/i);\n        const purposeMatch = line.match(/purpose[:\\s]+([^,\\n]+)/i);\n        const selectorMatch = line.match(/selector[:\\s]+([^,\\n]+)/i);\n        const confidenceMatch = line.match(/confidence[:\\s]+([0-9.]+)/i);\n        \n        if (typeMatch || purposeMatch || selectorMatch) {\n          elements.push({\n            type: typeMatch ? typeMatch[1] : 'unknown',\n            purpose: purposeMatch ? purposeMatch[1].trim() : 'unknown',\n            selector: selectorMatch ? selectorMatch[1].trim() : '',\n            confidence: confidenceMatch ? parseFloat(confidenceMatch[1]) : 0.5,\n            position: 'unknown',\n            characteristics: 'parsed from text',\n            interactionType: 'click'\n          });\n        }\n      }\n      \n      // Data field extraction patterns\n      if (currentSection === 'dataFields') {\n        const nameMatch = line.match(/name[:\\s]+([^,\\n]+)/i);\n        const typeMatch = line.match(/type[:\\s]+([^,\\n]+)/i);\n        const selectorMatch = line.match(/selector[:\\s]+([^,\\n]+)/i);\n        \n        if (nameMatch || selectorMatch) {\n          dataFields.push({\n            name: nameMatch ? nameMatch[1].trim() : 'unknown_field',\n            type: typeMatch ? typeMatch[1].trim() : 'text',\n            selector: selectorMatch ? selectorMatch[1].trim() : '',\n            confidence: 0.6,\n            multiple: false,\n            context: 'unknown',\n            sampleValue: ''\n          });\n        }\n      }\n    }\n\n    return {\n      elements,\n      dataFields,\n      pageStructure: {\n        pageType: 'unknown',\n        mainContent: 'main, .main, #main, .content, #content',\n        hasNavigation: true,\n        hasPagination: false,\n        hasSearch: false,\n        layoutType: 'unknown'\n      }\n    };\n  }\n\n  validateAndEnhanceAnalysis(analysis, screenshot) {\n    // Ensure required structure\n    analysis.elements = analysis.elements || [];\n    analysis.dataFields = analysis.dataFields || [];\n    analysis.pageStructure = analysis.pageStructure || {};\n\n    // Enhance elements with additional properties\n    analysis.elements = analysis.elements.map(element => ({\n      type: element.type || 'unknown',\n      purpose: element.purpose || 'unknown',\n      selector: this.enhanceSelector(element.selector),\n      confidence: Math.max(0, Math.min(1, element.confidence || 0.5)),\n      position: element.position || 'unknown',\n      characteristics: element.characteristics || '',\n      interactionType: element.interactionType || this.inferInteractionType(element.type),\n      url: screenshot.url,\n      timestamp: screenshot.timestamp\n    }));\n\n    // Enhance data fields\n    analysis.dataFields = analysis.dataFields.map(field => ({\n      name: field.name || 'unknown_field',\n      type: field.type || 'text',\n      selector: this.enhanceSelector(field.selector),\n      confidence: Math.max(0, Math.min(1, field.confidence || 0.5)),\n      multiple: field.multiple || false,\n      context: field.context || 'unknown',\n      sampleValue: field.sampleValue || '',\n      url: screenshot.url,\n      timestamp: screenshot.timestamp\n    }));\n\n    // Enhance page structure\n    analysis.pageStructure = {\n      pageType: analysis.pageStructure.pageType || 'unknown',\n      mainContent: analysis.pageStructure.mainContent || 'main',\n      hasNavigation: analysis.pageStructure.hasNavigation !== false,\n      hasPagination: analysis.pageStructure.hasPagination || false,\n      hasSearch: analysis.pageStructure.hasSearch || false,\n      layoutType: analysis.pageStructure.layoutType || 'unknown'\n    };\n\n    return analysis;\n  }\n\n  enhanceSelector(selector) {\n    if (!selector || selector.trim() === '') {\n      return '';\n    }\n\n    // Clean and validate selector\n    const cleaned = selector.trim().replace(/['\"]/g, '');\n    \n    // Add fallback selectors for robustness\n    if (cleaned.includes('#')) {\n      // ID-based selector - add class fallback\n      return cleaned;\n    } else if (cleaned.includes('.')) {\n      // Class-based selector - good as is\n      return cleaned;\n    } else {\n      // Tag-based selector - enhance with attributes\n      return cleaned;\n    }\n  }\n\n  inferInteractionType(elementType) {\n    const interactionMap = {\n      'button': 'click',\n      'link': 'click', \n      'input': 'type',\n      'textarea': 'type',\n      'select': 'select',\n      'dropdown': 'select',\n      'checkbox': 'click',\n      'radio': 'click',\n      'submit': 'click'\n    };\n\n    return interactionMap[elementType?.toLowerCase()] || 'click';\n  }\n\n  analyzeActionPatterns(actions) {\n    console.log('├ó┼í┬í Analyzing action patterns');\n    \n    const patterns = [];\n    \n    // Group actions by type\n    const actionsByType = actions.reduce((acc, action) => {\n      if (!acc[action.type]) acc[action.type] = [];\n      acc[action.type].push(action);\n      return acc;\n    }, {});\n\n    // Analyze click patterns\n    if (actionsByType.click) {\n      const clickPatterns = this.analyzeClickPatterns(actionsByType.click);\n      patterns.push(...clickPatterns);\n    }\n\n    // Analyze input patterns  \n    if (actionsByType.input) {\n      const inputPatterns = this.analyzeInputPatterns(actionsByType.input);\n      patterns.push(...inputPatterns);\n    }\n\n    // Analyze scroll patterns\n    if (actionsByType.scroll) {\n      const scrollPattern = this.analyzeScrollPattern(actionsByType.scroll);\n      if (scrollPattern) patterns.push(scrollPattern);\n    }\n\n    // Analyze temporal patterns\n    const temporalPatterns = this.analyzeTemporalPatterns(actions);\n    patterns.push(...temporalPatterns);\n\n    return patterns;\n  }\n\n  analyzeClickPatterns(clickActions) {\n    const patterns = [];\n    \n    // Group by similar selectors\n    const selectorGroups = {};\n    clickActions.forEach(action => {\n      const baseSelector = this.getBaseSelectorPattern(action.target);\n      if (!selectorGroups[baseSelector]) {\n        selectorGroups[baseSelector] = [];\n      }\n      selectorGroups[baseSelector].push(action);\n    });\n\n    // Identify repetitive clicking patterns\n    Object.entries(selectorGroups).forEach(([baseSelector, actions]) => {\n      if (actions.length > 1) {\n        patterns.push({\n          type: 'repetitive_clicks',\n          pattern: baseSelector,\n          occurrences: actions.length,\n          averageInterval: this.calculateAverageInterval(actions),\n          confidence: Math.min(0.9, actions.length / 10),\n          description: `Repetitive clicking on similar elements (${baseSelector})`\n        });\n      }\n    });\n\n    return patterns;\n  }\n\n  analyzeInputPatterns(inputActions) {\n    const patterns = [];\n    \n    // Group by form or input type\n    const inputGroups = {};\n    inputActions.forEach(action => {\n      const key = `${action.inputType || 'text'}_${this.getBaseSelectorPattern(action.target)}`;\n      if (!inputGroups[key]) {\n        inputGroups[key] = [];\n      }\n      inputGroups[key].push(action);\n    });\n\n    Object.entries(inputGroups).forEach(([key, actions]) => {\n      const [inputType, selector] = key.split('_');\n      \n      patterns.push({\n        type: 'input_pattern',\n        inputType: inputType,\n        selector: selector,\n        sampleValues: actions.map(a => a.value).slice(0, 3),\n        occurrences: actions.length,\n        confidence: 0.8,\n        description: `Input pattern for ${inputType} fields`\n      });\n    });\n\n    return patterns;\n  }\n\n  analyzeScrollPattern(scrollActions) {\n    if (scrollActions.length < 2) return null;\n\n    const totalScroll = scrollActions[scrollActions.length - 1].position.y - scrollActions[0].position.y;\n    const averageScrollDistance = totalScroll / scrollActions.length;\n\n    return {\n      type: 'scroll_pattern',\n      totalDistance: totalScroll,\n      averageDistance: averageScrollDistance,\n      scrollCount: scrollActions.length,\n      confidence: 0.7,\n      description: `Scrolling pattern with ${scrollActions.length} scroll events`\n    };\n  }\n\n  analyzeTemporalPatterns(actions) {\n    const patterns = [];\n    \n    // Find actions with consistent timing\n    for (let i = 1; i < actions.length; i++) {\n      const timeDiff = actions[i].timestamp - actions[i-1].timestamp;\n      \n      if (timeDiff > 5000 && timeDiff < 60000) { // Between 5s and 1min\n        patterns.push({\n          type: 'wait_pattern',\n          duration: timeDiff,\n          beforeAction: actions[i-1].type,\n          afterAction: actions[i].type,\n          confidence: 0.6,\n          description: `Wait pattern of ${Math.round(timeDiff/1000)}s between actions`\n        });\n      }\n    }\n\n    return patterns;\n  }\n\n  getBaseSelectorPattern(selector) {\n    // Extract the base pattern from a selector\n    return selector\n      .replace(/:nth-child\\(\\d+\\)/g, ':nth-child(n)')\n      .replace(/:nth-of-type\\(\\d+\\)/g, ':nth-of-type(n)')\n      .replace(/\\[\\w+=\"[^\"]*\"\\]/g, '[attr]')\n      .split(' > ').slice(0, 3).join(' > '); // Limit depth\n  }\n\n  calculateAverageInterval(actions) {\n    if (actions.length < 2) return 0;\n    \n    let totalInterval = 0;\n    for (let i = 1; i < actions.length; i++) {\n      totalInterval += actions[i].timestamp - actions[i-1].timestamp;\n    }\n    \n    return totalInterval / (actions.length - 1);\n  }\n\n  extractNavigationFlow(actions) {\n    return actions\n      .filter(action => action.type === 'navigation')\n      .map(nav => ({\n        from: nav.from,\n        to: nav.to,\n        timestamp: nav.timestamp,\n        trigger: nav.trigger || 'unknown'\n      }));\n  }\n\n  deduplicateElements(elements) {\n    const seen = new Set();\n    return elements.filter(element => {\n      const key = `${element.type}_${element.selector}_${element.purpose}`;\n      if (seen.has(key)) return false;\n      seen.add(key);\n      return element.confidence >= this.confidenceThreshold;\n    });\n  }\n\n  deduplicateDataFields(dataFields) {\n    const seen = new Set();\n    return dataFields.filter(field => {\n      const key = `${field.name}_${field.selector}`;\n      if (seen.has(key)) return false;\n      seen.add(key);\n      return field.confidence >= this.confidenceThreshold;\n    });\n  }\n\n  calculateOverallConfidence(analysis) {\n    const allConfidences = [\n      ...analysis.identifiedElements.map(e => e.confidence),\n      ...analysis.dataFields.map(d => d.confidence)\n    ];\n\n    if (allConfidences.length === 0) return 0;\n\n    const averageConfidence = allConfidences.reduce((sum, conf) => sum + conf, 0) / allConfidences.length;\n    const elementCount = analysis.identifiedElements.length + analysis.dataFields.length;\n    const countBonus = Math.min(0.2, elementCount * 0.02); // Bonus for finding more elements\n\n    return Math.min(0.95, averageConfidence + countBonus);\n  }\n\n  generateRecommendations(analysis, actions) {\n    const recommendations = [];\n\n    // Recommend based on confidence levels\n    const lowConfidenceElements = analysis.identifiedElements.filter(e => e.confidence < 0.7);\n    if (lowConfidenceElements.length > 0) {\n      recommendations.push({\n        type: 'selector_improvement',\n        message: `${lowConfidenceElements.length} elements have low confidence. Consider manual selector verification.`,\n        priority: 'medium',\n        elements: lowConfidenceElements.map(e => e.selector)\n      });\n    }\n\n    // Recommend based on action patterns\n    const repetitivePatterns = analysis.actionPatterns.filter(p => p.type === 'repetitive_clicks');\n    if (repetitivePatterns.length > 0) {\n      recommendations.push({\n        type: 'pagination_handling',\n        message: 'Detected repetitive clicking patterns. Consider implementing pagination or \"load more\" handling.',\n        priority: 'high',\n        patterns: repetitivePatterns\n      });\n    }\n\n    // Recommend based on page structure\n    if (analysis.pageStructure.hasPagination) {\n      recommendations.push({\n        type: 'pagination_scraping',\n        message: 'Page has pagination. Implement pagination handling to scrape all pages.',\n        priority: 'high'\n      });\n    }\n\n    if (analysis.pageStructure.pageType === 'ecommerce') {\n      recommendations.push({\n        type: 'ecommerce_optimization',\n        message: 'E-commerce site detected. Consider adding product-specific extractors (price, reviews, availability).',\n        priority: 'medium'\n      });\n    }\n\n    // Recommend based on data fields\n    const multipleDataFields = analysis.dataFields.filter(d => d.multiple);\n    if (multipleDataFields.length > 0) {\n      recommendations.push({\n        type: 'batch_extraction',\n        message: `Found ${multipleDataFields.length} repeating data patterns. Optimize for batch extraction.`,\n        priority: 'medium',\n        fields: multipleDataFields.map(d => d.name)\n      });\n    }\n\n    return recommendations;\n  }\n\n  generateCacheKey(imageData) {\n    // Generate a simple hash of the first 1000 characters for caching\n    const sample = imageData.substring(0, 1000);\n    let hash = 0;\n    for (let i = 0; i < sample.length; i++) {\n      const char = sample.charCodeAt(i);\n      hash = ((hash << 5) - hash) + char;\n      hash = hash & hash; // Convert to 32-bit integer\n    }\n    return hash.toString(36);\n  }\n}\n\nmodule.exports = { VisualAnalysisEngine };","usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\test-runner-local.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\tests\\__mocks__\\source-map-support.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\tests\\integration\\security.test.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\tests\\mocks\\ioredis-mock.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\tests\\mocks\\redis-mock.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\tests\\setup-integration.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\tests\\setup.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\tests\\unit\\auth-service.test.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\tests\\unit\\compliance-manager.test.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\utils\\env-detector.js","messages":[],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]},{"filePath":"C:\\Users\\Leon\\Documents\\GitHub\\APL-AI-Scraper-2.0\\utils\\health-checkers.js","messages":[{"ruleId":"no-unused-vars","severity":1,"message":"'data' is assigned a value but never used.","line":18,"column":13,"nodeType":"Identifier","messageId":"unusedVar","endLine":18,"endColumn":17}],"suppressedMessages":[],"errorCount":0,"fatalErrorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":0,"source":"∩╗┐const { createClient } = require('@supabase/supabase-js');\nconst Redis = require('ioredis');\n\nasync function checkDatabaseHealth() {\n  const start = Date.now();\n  try {\n    let supabase;\n    try {\n      const url = process.env.SUPABASE_URL;\n      if (!url || url === 'your_supabase_url_here' || url.trim() === '') {\n        supabase = require('../core/supabase').supabase;\n      } else {\n        supabase = createClient(url, process.env.SUPABASE_SERVICE_KEY);\n      }\n    } catch (e) {\n      supabase = require('../core/supabase').supabase;\n    }\n    const { data, error } = await supabase.from('health_check').select('*').limit(1);\n    return {\n      status: error ? 'unhealthy' : 'healthy',\n      responseTime: Date.now() - start,\n      error: error?.message\n    };\n  } catch (error) {\n    return { status: 'unhealthy', error: error.message };\n  }\n}\n\nasync function checkRedisHealth() {\n  try {\n    const redis = new Redis(process.env.REDIS_URL || 'redis://localhost:6379');\n    const start = Date.now();\n    await redis.ping();\n    const responseTime = Date.now() - start;\n    await redis.quit();\n    return { status: 'healthy', responseTime };\n  } catch (error) {\n    return { status: 'unhealthy', error: error.message };\n  }\n}\n\nasync function checkSecurityHealth() {\n  try {\n    const checks = {\n      https_enforced: process.env.NODE_ENV === 'production',\n      cors_configured: !!process.env.ALLOWED_ORIGINS,\n      rate_limiting_enabled: !!process.env.REDIS_URL,\n      encryption_configured: !!process.env.ENCRYPTION_KEY\n    };\n\n    const allHealthy = Object.values(checks).every(Boolean);\n    return { status: allHealthy ? 'healthy' : 'degraded', checks };\n  } catch (error) {\n    return { status: 'unhealthy', error: error.message };\n  }\n}\n\nmodule.exports = { checkDatabaseHealth, checkRedisHealth, checkSecurityHealth };\n","usedDeprecatedRules":[{"ruleId":"indent","replacedBy":[]},{"ruleId":"quotes","replacedBy":[]},{"ruleId":"semi","replacedBy":[]},{"ruleId":"no-extra-semi","replacedBy":[]},{"ruleId":"no-mixed-spaces-and-tabs","replacedBy":[]}]}]
